{"meta":{"version":1,"warehouse":"5.0.1"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"themes/next/source/images/algolia_logo.svg","path":"images/algolia_logo.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/avatar.gif","path":"images/avatar.gif","modified":0,"renderable":1},{"_id":"themes/next/source/images/apple-touch-icon-mw.png","path":"images/apple-touch-icon-mw.png","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","path":"images/cc-by-nc-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","path":"images/cc-by-nc-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nc.svg","path":"images/cc-by-nc.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-nd.svg","path":"images/cc-by-nd.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by-sa.svg","path":"images/cc-by-sa.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-by.svg","path":"images/cc-by.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/logo_mw.svg","path":"images/logo_mw.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/cc-zero.svg","path":"images/cc-zero.svg","modified":0,"renderable":1},{"_id":"themes/next/source/images/mw-icon-96x96.png","path":"images/mw-icon-96x96.png","modified":0,"renderable":1},{"_id":"themes/next/source/js/algolia-search.js","path":"js/algolia-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/images/mw-icon.ico","path":"images/mw-icon.ico","modified":0,"renderable":1},{"_id":"themes/next/source/js/bookmark.js","path":"js/bookmark.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/motion.js","path":"js/motion.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/local-search.js","path":"js/local-search.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/next-boot.js","path":"js/next-boot.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/utils.js","path":"js/utils.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/anime.min.js","path":"lib/anime.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/css/main.styl","path":"css/main.styl","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/pisces.js","path":"js/schemes/pisces.js","modified":0,"renderable":1},{"_id":"themes/next/source/js/schemes/muse.js","path":"js/schemes/muse.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/CHANGELOG.md","path":"lib/pjax/CHANGELOG.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/LICENSE","path":"lib/pjax/LICENSE","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/README.md","path":"lib/pjax/README.md","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/index.d.ts","path":"lib/pjax/index.d.ts","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/index.js","path":"lib/pjax/index.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/package.json","path":"lib/pjax/package.json","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/pjax.js","path":"lib/pjax/pjax.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/pjax.min.js","path":"lib/pjax/pjax.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.min.js","path":"lib/velocity/velocity.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","path":"lib/velocity/velocity.ui.min.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","path":"lib/font-awesome/css/all.min.css","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","path":"lib/font-awesome/webfonts/fa-brands-400.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","path":"lib/font-awesome/webfonts/fa-solid-900.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","path":"lib/font-awesome/webfonts/fa-regular-400.woff2","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/example/example.js","path":"lib/pjax/example/example.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/example/forms.html","path":"lib/pjax/example/forms.html","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/abort-request.js","path":"lib/pjax/lib/abort-request.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/example/index.html","path":"lib/pjax/example/index.html","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/eval-script.js","path":"lib/pjax/lib/eval-script.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/example/page2.html","path":"lib/pjax/example/page2.html","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/example/page3.html","path":"lib/pjax/example/page3.html","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/execute-scripts.js","path":"lib/pjax/lib/execute-scripts.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/foreach-els.js","path":"lib/pjax/lib/foreach-els.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/foreach-selectors.js","path":"lib/pjax/lib/foreach-selectors.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/is-supported.js","path":"lib/pjax/lib/is-supported.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/send-request.js","path":"lib/pjax/lib/send-request.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/switches-selectors.js","path":"lib/pjax/lib/switches-selectors.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/switches.js","path":"lib/pjax/lib/switches.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/parse-options.js","path":"lib/pjax/lib/parse-options.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/uniqueid.js","path":"lib/pjax/lib/uniqueid.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/setup.js","path":"lib/pjax/tests/setup.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/test.ts","path":"lib/pjax/tests/test.ts","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/events/off.js","path":"lib/pjax/lib/events/off.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/events/trigger.js","path":"lib/pjax/lib/events/trigger.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/events/on.js","path":"lib/pjax/lib/events/on.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/proto/attach-form.js","path":"lib/pjax/lib/proto/attach-form.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/proto/attach-link.js","path":"lib/pjax/lib/proto/attach-link.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/proto/handle-response.js","path":"lib/pjax/lib/proto/handle-response.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/proto/log.js","path":"lib/pjax/lib/proto/log.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/proto/parse-element.js","path":"lib/pjax/lib/proto/parse-element.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/util/clone.js","path":"lib/pjax/lib/util/clone.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/util/contains.js","path":"lib/pjax/lib/util/contains.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/util/extend.js","path":"lib/pjax/lib/util/extend.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/util/update-query-string.js","path":"lib/pjax/lib/util/update-query-string.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/lib/util/noop.js","path":"lib/pjax/lib/util/noop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/abort-request.js","path":"lib/pjax/tests/lib/abort-request.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/events.js","path":"lib/pjax/tests/lib/events.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/eval-scripts.js","path":"lib/pjax/tests/lib/eval-scripts.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/execute-scripts.js","path":"lib/pjax/tests/lib/execute-scripts.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/foreach-els.js","path":"lib/pjax/tests/lib/foreach-els.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/is-supported.js","path":"lib/pjax/tests/lib/is-supported.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/foreach-selectors.js","path":"lib/pjax/tests/lib/foreach-selectors.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/parse-options.js","path":"lib/pjax/tests/lib/parse-options.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/send-request.js","path":"lib/pjax/tests/lib/send-request.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/switch-selectors.js","path":"lib/pjax/tests/lib/switch-selectors.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/uniqueid.js","path":"lib/pjax/tests/lib/uniqueid.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/switches.js","path":"lib/pjax/tests/lib/switches.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/util/clone.js","path":"lib/pjax/tests/lib/util/clone.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/util/contains.js","path":"lib/pjax/tests/lib/util/contains.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/util/extend.js","path":"lib/pjax/tests/lib/util/extend.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/util/update-query-string.js","path":"lib/pjax/tests/lib/util/update-query-string.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/util/noop.js","path":"lib/pjax/tests/lib/util/noop.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/proto/attach-form.js","path":"lib/pjax/tests/lib/proto/attach-form.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/proto/attach-link.js","path":"lib/pjax/tests/lib/proto/attach-link.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/proto/handle-response.js","path":"lib/pjax/tests/lib/proto/handle-response.js","modified":0,"renderable":1},{"_id":"themes/next/source/lib/pjax/tests/lib/proto/parse-element.js","path":"lib/pjax/tests/lib/proto/parse-element.js","modified":0,"renderable":1}],"Cache":[{"_id":"source/_data/styles.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1742452043426},{"_id":"source/404.md","hash":"6aa9809e977871b0d374038dbf4ae48ff1e46c8c","modified":1742120646418},{"_id":"source/_data/footer.swig","hash":"7220a4e13b4c3b60c16f02a1267a495e43e6487a","modified":1742132280331},{"_id":"source/CNAME","hash":"b4bbb1bb35f75f6249c323af383e3e32e43788a4","modified":1742293163259},{"_id":"source/_posts/深度学习/深度学习python库.md","hash":"6d39e51633381f22bc47a6b78de1ae5cda90e2be","modified":1742453870693},{"_id":"source/categories/index.md","hash":"e16e67ac76db6ba98428d251f057dddd4638c84c","modified":1742293364614},{"_id":"source/about/index.md","hash":"a152b4cad2f09754cd28e829cb21dceed3b4b423","modified":1742293533208},{"_id":"source/_posts/python/python基础.md","hash":"b32e1db3cb4ec85a79ace88c36d2ea31e7197564","modified":1742445569411},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读.md","hash":"c1841f255c2b572b5bfb7c31d1581a58ba257419","modified":1742453277330},{"_id":"source/_posts/深度学习/深度学习python库/Pastedimage20250319161350.png","hash":"e6126b90bb3185326441ebc2ef7931d41fb53a5d","modified":1742372030816},{"_id":"source/tags/index.md","hash":"0c67ba36fcf4bfa1114ef7d76ee05deda3b9083d","modified":1742293519203},{"_id":"source/_posts/深度学习/深度学习python库/Snipaste_2025-03-20_14-56-59.jpg","hash":"60461b4219b9f7381afb32a29389a968fcfb2e8f","modified":1742453837981},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读/06ec95ff3c17534440c17f2c5081b49.png","hash":"a391d6ae0fe48dec5ed076cee5e7c45883fc82f4","modified":1742442507423},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读/5a6c5073a5cd2f4a2e6f7ca2e39dc5f.png","hash":"e45bc31449d6c732c18f3da933d4e99b2e66428c","modified":1742442600125},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读/Pastedimage20250319232552.png","hash":"4aaf5ced7c0ff43a447e910bd55afafeec5d3f0b","modified":1742397952045},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读/Pastedimage20250319234616.png","hash":"79782b5dd1386f1ea3b26403342af7d038f84d7a","modified":1742399176903},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读/0b0fcf44d3b47e8a5e9e12faccd42bc.png","hash":"7576a9d367eb4092b23dcf617653f612b48b6350","modified":1742442610266},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读/607dc97725be3eecbb191ed4887eadb.png","hash":"5b363b29502702a217ca028265822eef3455faa0","modified":1742442621056},{"_id":"themes/next/.editorconfig","hash":"731c650ddad6eb0fc7c3d4a91cad1698fe7ad311","modified":1742096448633},{"_id":"themes/next/.eslintrc.json","hash":"d3c11de434171d55d70daadd3914bc33544b74b8","modified":1742096448633},{"_id":"themes/next/.gitignore","hash":"83418530da80e6a78501e1d62a89c3bf5cbaec3d","modified":1742096448640},{"_id":"themes/next/.gitattributes","hash":"3e00e1fb043438cd820d94ee3dc9ffb6718996f3","modified":1742096448633},{"_id":"themes/next/.travis.yml","hash":"379f31a140ce41e441442add6f673bf397d863ea","modified":1742096448642},{"_id":"themes/next/README.md","hash":"7d56751b580d042559b2acf904fca4b42bcb30a7","modified":1742096448642},{"_id":"themes/next/.stylintrc","hash":"6259e2a0b65d46865ab89564b88fc67638668295","modified":1742096448640},{"_id":"themes/next/_config.yml","hash":"5afb3038d53b437a4d965dbb9f942ef63bea341d","modified":1742453179615},{"_id":"themes/next/gulpfile.js","hash":"0c76a1ac610ee8cbe8e2cc9cca1c925ffd0edf98","modified":1742096448655},{"_id":"themes/next/crowdin.yml","hash":"4a53f5985e545c635cb56b2a57ed290cb8cf8942","modified":1742096448644},{"_id":"themes/next/LICENSE.md","hash":"0a9c7399f102b4eb0a6950dd31264be421557c7d","modified":1742096448642},{"_id":"themes/next/package.json","hash":"b099e7cea4406e209130410d13de87988ba37b2a","modified":1742096449474},{"_id":"themes/next/.github/CODE_OF_CONDUCT.md","hash":"778b7e052993ed59f21ed266ba7119ee2e5253fb","modified":1742096448635},{"_id":"themes/next/.github/CONTRIBUTING.md","hash":"5ddde54fb50d11dc08cec899a3588addb56aa386","modified":1742096448635},{"_id":"themes/next/.github/issue_label_bot.yaml","hash":"533fbe6b2f87d7e7ec6949063bb7ea7eb4fbe52d","modified":1742096448638},{"_id":"themes/next/.github/issue-close-app.yml","hash":"b14756e65546eb9ecc9d4393f0c9a84a3dac1824","modified":1742096448638},{"_id":"themes/next/.github/PULL_REQUEST_TEMPLATE.md","hash":"d2f8e6b65783e31787feb05d2ccea86151f53f35","modified":1742096448637},{"_id":"themes/next/.github/config.yml","hash":"df3d970700e6b409edc3d23be8d553db78d5ba3f","modified":1742096448637},{"_id":"themes/next/.github/lock.yml","hash":"3ce3d0a26030a1cd52b273cc6a6d444d7c8d85c2","modified":1742096448638},{"_id":"themes/next/.github/release-drafter.yml","hash":"09c3352b2d643acdc6839601ceb38abc38ab97c5","modified":1742096448639},{"_id":"themes/next/docs/AGPL3.md","hash":"f463f95b169d64983f59fa6f3e4b6760290a0e6b","modified":1742096448644},{"_id":"themes/next/.github/mergeable.yml","hash":"1c1cb77a62df1e3654b151c2da34b4a10d351170","modified":1742096448639},{"_id":"themes/next/.github/support.yml","hash":"7ce2722d6904c31a086444c422dc49b6aa310651","modified":1742096448640},{"_id":"themes/next/.github/stale.yml","hash":"590b65aca710e0fba75d3cf5361a64d13b6b0f63","modified":1742096448639},{"_id":"themes/next/docs/ALGOLIA-SEARCH.md","hash":"60c7e9ef0c578deebad43e9395c958fa61096baf","modified":1742096448645},{"_id":"themes/next/docs/AUTHORS.md","hash":"cde7cc095ac31b421a573042cf61060f90d9ad0d","modified":1742096448645},{"_id":"themes/next/docs/DATA-FILES.md","hash":"980fb8d37701f7fd96b30bb911519de3bbb473d1","modified":1742096448645},{"_id":"themes/next/docs/INSTALLATION.md","hash":"07ea00bee149a1bdc9073e903ee6b411e9f2f818","modified":1742096448645},{"_id":"themes/next/docs/LICENSE.txt","hash":"ae5ad07e4f4106bad55535dba042221539e6c7f9","modified":1742096448646},{"_id":"themes/next/languages/ar.yml","hash":"abcf220bd615cec0dd50e4d98da56580169d77e1","modified":1742096448656},{"_id":"themes/next/docs/MATH.md","hash":"f56946053ade0915ff7efa74d43c38b8dd9e63bb","modified":1742096448648},{"_id":"themes/next/docs/UPDATE-FROM-5.1.X.md","hash":"1e86d32063b490d204baa9d45d8d3cb22c24a37d","modified":1742096448648},{"_id":"themes/next/docs/LEANCLOUD-COUNTER-SECURITY.md","hash":"6cc663db5e99fd86bb993c10d446ad26ada88e58","modified":1742096448646},{"_id":"themes/next/languages/en.yml","hash":"dbb64776f9c001c54d0058256c415a9a0724ed5d","modified":1742096448657},{"_id":"themes/next/languages/default.yml","hash":"ea5e6aee4cb14510793ac4593a3bddffe23e530c","modified":1742096448656},{"_id":"themes/next/languages/de.yml","hash":"15078b7ede1b084e8a6a15d271f0db9c325bd698","modified":1742096448656},{"_id":"themes/next/languages/es.yml","hash":"f064c793d56a5e0f20cda93b6f0e355044efc7d8","modified":1742096448657},{"_id":"themes/next/languages/fr.yml","hash":"3e2f89d4bb4441d33ecc7b5a4ee114f627603391","modified":1742096448658},{"_id":"themes/next/languages/fa.yml","hash":"6c0a7d5bcc26eb45a9f3e02f13117c668e77fffd","modified":1742096448657},{"_id":"themes/next/languages/hu.yml","hash":"0ea89ffaefd02a10494995f05a2a59d5e5679a28","modified":1742096448658},{"_id":"themes/next/languages/id.yml","hash":"7599bb0ecf278beb8fde3d17bfc148a3241aef82","modified":1742096448659},{"_id":"themes/next/languages/ja.yml","hash":"bf279d0eb1911806d01a12f27261fbc76a3bb3f9","modified":1742096448659},{"_id":"themes/next/languages/nl.yml","hash":"9749cf90b250e631dd550a4f32ada3bb20f66dd0","modified":1742096448660},{"_id":"themes/next/languages/it.yml","hash":"46222f468e66789e9ba13095809eb5e5b63edf30","modified":1742096448659},{"_id":"themes/next/languages/pt.yml","hash":"f6606dd0b916a465c233f24bd9a70adce34dc8d6","modified":1742096448662},{"_id":"themes/next/languages/ko.yml","hash":"af4be6cb394abd4e2e9a728418897d2ed4cc5315","modified":1742096448660},{"_id":"themes/next/languages/ru.yml","hash":"012abc694cf9de281a0610f95f79c594f0a16562","modified":1742096448662},{"_id":"themes/next/languages/tr.yml","hash":"c4e9ab7e047ae13a19f147c6bec163c3ba2c6898","modified":1742096448662},{"_id":"themes/next/languages/pt-BR.yml","hash":"69aa3bef5710b61dc9a0f3b3a8f52f88c4d08c00","modified":1742096448660},{"_id":"themes/next/languages/zh-CN.yml","hash":"81d73e21402dad729053a3041390435f43136a68","modified":1742096448663},{"_id":"themes/next/languages/vi.yml","hash":"6a578cc28773bd764f4418110500478f185d6efa","modified":1742096448662},{"_id":"themes/next/languages/uk.yml","hash":"69ef00b1b8225920fcefff6a6b6f2f3aad00b4ce","modified":1742096448662},{"_id":"themes/next/languages/zh-HK.yml","hash":"92ccee40c234626bf0142152949811ebe39fcef2","modified":1742096448663},{"_id":"themes/next/layout/_layout.swig","hash":"9554bd0f5c5a0438aa7b64065be5561c374d260e","modified":1742096448665},{"_id":"themes/next/layout/index.swig","hash":"bf7730fb66386184985a2ea3e4dfe0b843618bf6","modified":1742452117079},{"_id":"themes/next/layout/archive.swig","hash":"d9bca77f6dcfef71e300a294f731bead11ce199f","modified":1742096449472},{"_id":"themes/next/layout/index_c.swig","hash":"8dfd96fb6f833dd5d037de800813105654e8e8e6","modified":1742449351614},{"_id":"themes/next/layout/page.swig","hash":"357d916694d4c9a0fd1140fa56d3d17e067d8b52","modified":1742096449473},{"_id":"themes/next/layout/category.swig","hash":"c546b017a956faaa5f5643c7c8a363af7ac9d6b9","modified":1742096449473},{"_id":"themes/next/layout/tag.swig","hash":"d44ff8755727f6532e86fc9fc8dc631200ffe161","modified":1742096449474},{"_id":"themes/next/languages/zh-TW.yml","hash":"cf0740648725983fb88409d6501876f8b79db41d","modified":1742096448663},{"_id":"themes/next/.github/ISSUE_TEMPLATE/bug-report.md","hash":"e67146befddec3a0dc47dc80d1109070c71d5d04","modified":1742096448636},{"_id":"themes/next/layout/post.swig","hash":"5f0b5ba2e0a5b763be5e7e96611865e33bba24d7","modified":1742096449474},{"_id":"themes/next/.github/ISSUE_TEMPLATE/other.md","hash":"d5aa1a3323639a36bcd9a401484b67537043cd3c","modified":1742096448637},{"_id":"themes/next/scripts/renderer.js","hash":"e3658eea97b1183ee2e9f676231e53f7994741f6","modified":1742096449485},{"_id":"themes/next/docs/ru/INSTALLATION.md","hash":"a9cfe5ac9ef727a8650b2b6584482751a26b1460","modified":1742096448649},{"_id":"themes/next/.github/ISSUE_TEMPLATE/feature-request.md","hash":"6beeca0f45a429cd932b6e648617f548ff64c27c","modified":1742096448636},{"_id":"themes/next/.github/ISSUE_TEMPLATE/question.md","hash":"59275aa0582f793fee7be67904dcf52ad33a7181","modified":1742096448637},{"_id":"themes/next/docs/ru/DATA-FILES.md","hash":"54e6a067ed95268eab6be2ba040a7e9b1907928e","modified":1742096448648},{"_id":"themes/next/docs/ru/README.md","hash":"1e5ddb26ad6f931f8c06ce2120f257ff38b74fdf","modified":1742096448649},{"_id":"themes/next/docs/zh-CN/ALGOLIA-SEARCH.md","hash":"3202be9a8d31986caac640e7a4c7ce22e99917eb","modified":1742096448650},{"_id":"themes/next/docs/ru/UPDATE-FROM-5.1.X.md","hash":"cb8e39c377fc4a14aaf133b4d1338a48560e9e65","modified":1742096448650},{"_id":"themes/next/docs/zh-CN/CODE_OF_CONDUCT.md","hash":"7e6f227f2aaf30f400d4c065650a4e3d0d61b9e1","modified":1742096448650},{"_id":"themes/next/docs/zh-CN/INSTALLATION.md","hash":"716111dd36d276f463c707dfcc9937fea2a1cf7a","modified":1742096448652},{"_id":"themes/next/docs/zh-CN/DATA-FILES.md","hash":"2d868cd271d78b08775e28c5b976de8836da4455","modified":1742096448652},{"_id":"themes/next/docs/zh-CN/CONTRIBUTING.md","hash":"611f2930c2b281b80543531b1bf33d082531456a","modified":1742096448651},{"_id":"themes/next/docs/zh-CN/LEANCLOUD-COUNTER-SECURITY.md","hash":"50ab381c27611d5bf97bb3907b5ca9998f28187d","modified":1742096448653},{"_id":"themes/next/docs/zh-CN/MATH.md","hash":"0d46f9f50cf2e4183970adce705d1041155b0d37","modified":1742096448653},{"_id":"themes/next/docs/zh-CN/UPDATE-FROM-5.1.X.md","hash":"b3201934b966bc731eaf8a4dad4ba4bdcd300c10","modified":1742096448655},{"_id":"themes/next/layout/_partials/comments.swig","hash":"142efb4c6b73d8f736f6784804b40d5871333172","modified":1742096448666},{"_id":"themes/next/docs/zh-CN/README.md","hash":"8f7c0d0b766024152591d4ccfac715c8e18b37f3","modified":1742096448653},{"_id":"themes/next/layout/_partials/footer.swig","hash":"571b0f997610f40bd16374104cbe95235bd34ab3","modified":1742135156982},{"_id":"themes/next/layout/_partials/languages.swig","hash":"c3ea82604a5853fb44c5f4e4663cbe912aa5dcf8","modified":1742096448671},{"_id":"themes/next/layout/_scripts/index.swig","hash":"1822eaf55bbb4bec88871c324fc18ad95580ccb4","modified":1742096449449},{"_id":"themes/next/layout/_partials/pagination.swig","hash":"2de77d533c91532a8a4052000244d0c1693370df","modified":1742096449370},{"_id":"themes/next/layout/_partials/widgets.swig","hash":"5392dcbb504266f0f61d5b8219914068ef9cdc25","modified":1742096449448},{"_id":"themes/next/layout/_macro/post-collapse.swig","hash":"30ade8c806d7826cc50a4a3e46a9e6213fddf333","modified":1742096448665},{"_id":"themes/next/layout/_scripts/noscript.swig","hash":"7b9e0f776a5be6c3f95bc7f394e1424ba02ba93b","modified":1742096449450},{"_id":"themes/next/layout/_scripts/pjax.swig","hash":"ccff5a773644d33ff22f6b45b6734f52b048f22b","modified":1742096449450},{"_id":"themes/next/layout/_macro/post.swig","hash":"c3fd56bac90ce45a0c79ddfe68beb223ad0d72b4","modified":1742096448666},{"_id":"themes/next/layout/_macro/sidebar.swig","hash":"5bffdb1448caca7db7b1f84e1693e6657a106d50","modified":1742096448666},{"_id":"themes/next/layout/_scripts/three.swig","hash":"6b092c6d882b2dfa5273e1b3f60b244cb7c29fcd","modified":1742096449452},{"_id":"themes/next/layout/_scripts/vendors.swig","hash":"244ca2d74ee0d497c87572c6a26b43c62a952673","modified":1742096449453},{"_id":"themes/next/layout/_third-party/index.swig","hash":"c6b63cbc80938e6e09578b8c67e01adf13a9e3bd","modified":1742096449465},{"_id":"themes/next/layout/_third-party/baidu-push.swig","hash":"28b0a7e843ec4365db1963646659a153753cd746","modified":1742096449455},{"_id":"themes/next/source/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1742096449528},{"_id":"themes/next/layout/_third-party/rating.swig","hash":"269102fc5e46bd1ce75abdcce161f0570ae70e2f","modified":1742096449467},{"_id":"themes/next/layout/_third-party/quicklink.swig","hash":"5ae5adcd6f63ed98b2071e4f7e5e38c4d7d24e1b","modified":1742096449466},{"_id":"themes/next/source/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1742096449531},{"_id":"themes/next/source/images/avatar.gif","hash":"764ff4a365c70484ea8581f582d0997ca28e247e","modified":1681399569928},{"_id":"themes/next/source/images/apple-touch-icon-mw.png","hash":"9462336d3638b5c2002b1d7a4358fc3e1fe76f35","modified":1742084144000},{"_id":"themes/next/source/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1742096449530},{"_id":"themes/next/source/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1742096449531},{"_id":"themes/next/source/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1742096449531},{"_id":"themes/next/source/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1742096449531},{"_id":"themes/next/source/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1742096449532},{"_id":"themes/next/source/images/mw-icon-96x96.png","hash":"9d436438e15be6f8be2c99a31b9bc284daaf2a14","modified":1742084144000},{"_id":"themes/next/source/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1742096449532},{"_id":"themes/next/source/images/logo_mw.svg","hash":"a81d429d44b6cd997ae970dee3d348cf6e778fc9","modified":1742084144000},{"_id":"themes/next/source/js/algolia-search.js","hash":"6a813410e33824d7acc65a369a2983912bb3420c","modified":1742096449534},{"_id":"themes/next/source/js/bookmark.js","hash":"9f05fd3672789311dc0cf5b37e40dc654cb04a2a","modified":1742096449534},{"_id":"themes/next/source/images/mw-icon.ico","hash":"4de7d6e3c4d97d7345cabae665d7aa89a2035a00","modified":1742084144000},{"_id":"themes/next/source/js/motion.js","hash":"d5aa1a08cdf3c8d1d8d550fb1801274cc41e5874","modified":1742096449535},{"_id":"themes/next/source/js/local-search.js","hash":"cfa6a0f3f9c2bc759ee507668a21f4e8f250f42a","modified":1742096449535},{"_id":"themes/next/source/js/next-boot.js","hash":"250d8dcd6322e69e3fbadd0f3e37081c97b47c52","modified":1742096449536},{"_id":"themes/next/source/js/utils.js","hash":"26a82e46fdcadc7c3c2c56a7267284b61a26f7f3","modified":1742096449537},{"_id":"themes/next/source/lib/anime.min.js","hash":"960be51132134acd65c2017cc8a5d69cb419a0cd","modified":1742096449537},{"_id":"themes/next/scripts/events/index.js","hash":"5c355f10fe8c948a7f7cd28bd8120adb7595ebde","modified":1742096449475},{"_id":"themes/next/source/css/_colors.styl","hash":"11aef31a8e76f0f332a274a8bfd4537b73d4f88f","modified":1742096449492},{"_id":"themes/next/source/css/_mixins.styl","hash":"072a3fa473c19b20ccd7536a656cda044dbdae0a","modified":1742096449518},{"_id":"themes/next/scripts/helpers/font.js","hash":"8fb1c0fc745df28e20b96222974402aab6d13a79","modified":1742096449485},{"_id":"themes/next/scripts/helpers/engine.js","hash":"eb6b8bbc1dce4846cd5e0fac0452dbff56d07b5d","modified":1742096449484},{"_id":"themes/next/scripts/helpers/next-config.js","hash":"b8d7ddfa4baa9b8d6b9066a634aa81c6243beec9","modified":1742096449485},{"_id":"themes/next/source/css/main.styl","hash":"815ef30987d02f3d76dbe4b5ee3a72135a152678","modified":1742096449528},{"_id":"themes/next/scripts/helpers/next-url.js","hash":"4044129368d0e2811859a9661cad8ab47118bc32","modified":1742096449485},{"_id":"themes/next/scripts/tags/button.js","hash":"bb0e8abbc0a6d5b3a1a75a23976f2ac3075aab31","modified":1742096449486},{"_id":"themes/next/scripts/filters/default-injects.js","hash":"ad321db012cea520066deb0639335e9bc0dcc343","modified":1742096449482},{"_id":"themes/next/scripts/filters/locals.js","hash":"a5e7d05d3bd2ae6dcffad5a8ea0f72c6e55dbd02","modified":1742096449482},{"_id":"themes/next/scripts/tags/caniuse.js","hash":"840536754121e0da5968f5ad235f29200fc5d769","modified":1742096449486},{"_id":"themes/next/scripts/filters/front-matter.js","hash":"305d03c1e45782988809298c3e3b3c5d5ee438aa","modified":1742096449482},{"_id":"themes/next/scripts/filters/minify.js","hash":"21196a48cb127bf476ce598f25f24e8a53ef50c2","modified":1742096449483},{"_id":"themes/next/scripts/tags/center-quote.js","hash":"e2d0184bc4a557e1017395b80ff46880078d8537","modified":1742096449486},{"_id":"themes/next/scripts/filters/post.js","hash":"57f2d817578dd97e206942604365e936a49854de","modified":1742096449484},{"_id":"themes/next/scripts/tags/group-pictures.js","hash":"93ccd3f99d3cb42674f29183c756df63acb5d7f8","modified":1742096449488},{"_id":"themes/next/scripts/tags/label.js","hash":"fc83f4e1be2c34e81cb79938f4f99973eba1ea60","modified":1742096449488},{"_id":"themes/next/scripts/tags/mermaid.js","hash":"81134494ff0134c0dae1b3815caf6606fccd4e46","modified":1742096449488},{"_id":"themes/next/scripts/tags/note.js","hash":"1fdf4f95810fdb983bfd5ad4c4f13fedd4ea2f8d","modified":1742096449489},{"_id":"themes/next/scripts/tags/pdf.js","hash":"37b53661ad00a01a2ca7d2e4a5ad3a926073f8e2","modified":1742096449489},{"_id":"themes/next/scripts/tags/tabs.js","hash":"c70a4a66fd0c28c98ccb6c5d5f398972e5574d28","modified":1742096449490},{"_id":"themes/next/layout/_partials/header/index.swig","hash":"0dd316f153c492c0a03bd0273d50fa322bc81f11","modified":1742096448670},{"_id":"themes/next/layout/_partials/header/sub-menu.swig","hash":"bed6cc2b48cf2655036ba39c9bae73a295228a4d","modified":1742096448671},{"_id":"themes/next/scripts/tags/video.js","hash":"944293fec96e568d9b09bc1280d5dbc9ee1bbd17","modified":1742096449490},{"_id":"themes/next/layout/_partials/header/menu.swig","hash":"90d3eaba6fbe69bee465ddd67c467fd2c0239dc4","modified":1742096448670},{"_id":"themes/next/layout/_partials/header/brand.swig","hash":"91056a6c98cca63ff8cc6956e531ee3faf4b8ad9","modified":1742096448669},{"_id":"themes/next/layout/_partials/header/menu-item.swig","hash":"4baa86ca631168fc6388d27f4b1b501b40c877a8","modified":1742096448670},{"_id":"themes/next/layout/_partials/page/breadcrumb.swig","hash":"91c0addb33006619faa4c32e5d66874e25f1e9b3","modified":1742096448671},{"_id":"themes/next/layout/_partials/head/head.swig","hash":"90cce9f407e9490756ba99580e3eb09f55b05eaa","modified":1742096448669},{"_id":"themes/next/layout/_partials/head/head-unique.swig","hash":"7d638e413f2548fc990c4a467dd03de6c81fc960","modified":1742096448667},{"_id":"themes/next/layout/_partials/page/page-header.swig","hash":"8d4e3dd0d3631ce0b21bc15c259f6ac886de631d","modified":1742096448672},{"_id":"themes/next/layout/_partials/post/post-copyright.swig","hash":"f2eb455c8bf13533427254f0c9b4b17b2498168b","modified":1742096449372},{"_id":"themes/next/layout/_partials/post/post-followme.swig","hash":"d8f785c062c6b0763a778bd4a252e6f5fee0e432","modified":1742096449373},{"_id":"themes/next/layout/_scripts/schemes/mist.swig","hash":"0b44f400ec00d2b5add5ee96c11d22465c432376","modified":1742096449451},{"_id":"themes/next/layout/_partials/post/post-footer.swig","hash":"ce712c110b5ce8aacba7a86b0558ff89700675c9","modified":1742096449444},{"_id":"themes/next/layout/_partials/post/post-related.swig","hash":"bc7b047a6246df07767373644b1637d91c3a88b1","modified":1742096449445},{"_id":"themes/next/layout/_partials/post/post-reward.swig","hash":"f349a226e5370075bb6924e60da8b0170c7cfcc1","modified":1742096449445},{"_id":"themes/next/layout/_scripts/pages/schedule.swig","hash":"34c05e9d73b0f081db70990c296b6d6a0f8ea2ca","modified":1742096449450},{"_id":"themes/next/layout/_scripts/schemes/gemini.swig","hash":"34495d408e8467555afee489500b8aad98c52079","modified":1742096449451},{"_id":"themes/next/layout/_partials/sidebar/site-overview.swig","hash":"7b2ef5db9615267a24b884388925de1e9b447c1f","modified":1742096449448},{"_id":"themes/next/layout/_scripts/schemes/muse.swig","hash":"0b44f400ec00d2b5add5ee96c11d22465c432376","modified":1742096449451},{"_id":"themes/next/layout/_scripts/schemes/pisces.swig","hash":"34495d408e8467555afee489500b8aad98c52079","modified":1742096449452},{"_id":"themes/next/layout/_third-party/analytics/baidu-analytics.swig","hash":"84adaadd83ce447fa9da2cff19006334c9fcbff9","modified":1742096449454},{"_id":"themes/next/layout/_third-party/analytics/google-analytics.swig","hash":"b8819bd056f8a580c5556d4415836a906ed5d7a4","modified":1742096449454},{"_id":"themes/next/layout/_third-party/analytics/growingio.swig","hash":"91c2cb900c76224c5814eeb842d1d5f517f9bf05","modified":1742096449455},{"_id":"themes/next/layout/_third-party/analytics/index.swig","hash":"85b60e222712ca3b2c4dc2039de2dc36b8d82940","modified":1742096449455},{"_id":"themes/next/layout/_partials/search/index.swig","hash":"a6c761d5193cb6f22e9422dbbcf209e05471b0ed","modified":1742096449447},{"_id":"themes/next/layout/_third-party/chat/tidio.swig","hash":"fb94ee487d75e484e59b7fba96e989f699ff8a83","modified":1742096449462},{"_id":"themes/next/layout/_third-party/chat/chatra.swig","hash":"2642e8aef5afbe23a2a76efdc955dab2ee04ed48","modified":1742096449455},{"_id":"themes/next/layout/_partials/search/algolia-search.swig","hash":"98fd1f5df044f4534e1d4ca9ab092ba5761739a9","modified":1742096449447},{"_id":"themes/next/layout/_third-party/comments/changyan.swig","hash":"9298e6d6c4a62a0862fc0f4060ed99779d7b68cb","modified":1742096449462},{"_id":"themes/next/layout/_partials/search/localsearch.swig","hash":"128f7d679bb4d53b29203d598d217f029a66dee7","modified":1742096449447},{"_id":"themes/next/layout/_third-party/math/index.swig","hash":"59df21fcfe9d0ada8cee3188cb1075529c1c3eb8","modified":1742096449465},{"_id":"themes/next/layout/_third-party/comments/disqus.swig","hash":"1b29b99fa921f12c25d3dc95facdf84ef7bb1b5c","modified":1742096449463},{"_id":"themes/next/layout/_third-party/comments/disqusjs.swig","hash":"a42f97eda3748583bac2253c47fe5dfa54f07b8f","modified":1742096449463},{"_id":"themes/next/layout/_third-party/comments/gitalk.swig","hash":"606ad14a29320157df9b8f33738282c51bb393d9","modified":1742096449463},{"_id":"themes/next/layout/_third-party/comments/valine.swig","hash":"ae2707d6e47582bb470c075649ec7bad86a6d5a9","modified":1742096449464},{"_id":"themes/next/layout/_third-party/math/katex.swig","hash":"276f523e414d4aa7f350a8f2fd3df8a3d8ea9656","modified":1742096449466},{"_id":"themes/next/layout/_third-party/search/algolia-search.swig","hash":"fd726aad77a57b288f07d6998ec29291c67c7cbb","modified":1742096449468},{"_id":"themes/next/layout/_third-party/comments/livere.swig","hash":"3d91899ca079e84d95087b882526d291e6f53918","modified":1742096449463},{"_id":"themes/next/layout/_third-party/math/mathjax.swig","hash":"1f34b2d3c753a3589ab6c462880bd4eb7df09914","modified":1742096449466},{"_id":"themes/next/layout/_third-party/search/localsearch.swig","hash":"58296a5c1883f26464c2a5ccf734c19f5fbf395a","modified":1742096449468},{"_id":"themes/next/layout/_third-party/search/swiftype.swig","hash":"aa6ab95b8b76611694613defb4bf25003d1b927f","modified":1742096449468},{"_id":"themes/next/layout/_third-party/statistics/busuanzi-counter.swig","hash":"d2f0e4c598410ec33785abe302c7ea7492bb791a","modified":1742096449469},{"_id":"themes/next/layout/_third-party/statistics/firestore.swig","hash":"01d94354d07e72cad47100482068b6be69fcc033","modified":1742096449469},{"_id":"themes/next/layout/_third-party/statistics/cnzz-analytics.swig","hash":"53a0760c75d5aaabb3ce8e8aa8e003510d59807f","modified":1742096449469},{"_id":"themes/next/layout/_third-party/statistics/lean-analytics.swig","hash":"c171ea94e9afbba97f06856904264da331559463","modified":1742096449471},{"_id":"themes/next/layout/_third-party/statistics/index.swig","hash":"964cd6bac668cf6d211a2624fbef3948cfdece55","modified":1742096449470},{"_id":"themes/next/source/js/schemes/pisces.js","hash":"b85a6e2af1387fe64b51e7cd3e2da8616e6f5a3f","modified":1742096449536},{"_id":"themes/next/source/js/schemes/muse.js","hash":"a18559a9c332199efad0100cf84bb0c23fc0f17a","modified":1742096449536},{"_id":"themes/next/layout/_third-party/tags/pdf.swig","hash":"5a223b60406cee7438cfe3a5e41d1284425aa7a5","modified":1742096449472},{"_id":"themes/next/source/lib/pjax/.editorconfig","hash":"8c81c4efc1ebde69b4c084c370c29071af62ed2f","modified":1742118672182},{"_id":"themes/next/layout/_third-party/tags/mermaid.swig","hash":"619338ddacf01e3df812e66a997e778f672f4726","modified":1742096449471},{"_id":"themes/next/source/lib/pjax/.eslintrc.json","hash":"37eb8287500c9f2d4e1b887bec1c7636791a31dc","modified":1742118672184},{"_id":"themes/next/source/lib/pjax/.eslintignore","hash":"72a528c49453bc417123c12a5482f9a3dec8e1a5","modified":1742118672184},{"_id":"themes/next/source/lib/pjax/.gitignore","hash":"37a0d5efc8a1eaa132681e1c2bf9f68333c2fbff","modified":1742118672184},{"_id":"themes/next/source/lib/pjax/.prettierignore","hash":"72a528c49453bc417123c12a5482f9a3dec8e1a5","modified":1742118672185},{"_id":"themes/next/source/lib/pjax/CHANGELOG.md","hash":"c6bec1421c73c2b33eb0f2974566351968eac342","modified":1742118672186},{"_id":"themes/next/source/lib/pjax/.travis.yml","hash":"a8afaf403ef826c018803df13ed5a36943320983","modified":1742118672186},{"_id":"themes/next/source/lib/pjax/LICENSE","hash":"2c7168814d9d35ea9500809b0904962f511eb4a8","modified":1742118672186},{"_id":"themes/next/source/lib/pjax/package.json","hash":"c3670a3576aa4bc6d0818edda3723e034b647e52","modified":1742118672198},{"_id":"themes/next/source/lib/pjax/index.js","hash":"e6fc3fb4501520bae1505c5d95214d3318810c2d","modified":1742118672189},{"_id":"themes/next/source/lib/pjax/README.md","hash":"f3f023db70ec08d0e8602e091c9aee621978a23d","modified":1742118672187},{"_id":"themes/next/source/lib/pjax/pjax.js","hash":"39b3063839a090bf3224c0c4776ffcedf2faf327","modified":1742118672199},{"_id":"themes/next/source/lib/pjax/index.d.ts","hash":"c452cd6e990eeeea10f7dbf17fefbc6845585bf6","modified":1742118672189},{"_id":"themes/next/source/lib/pjax/pjax.min.js","hash":"68d0c6ad28f042c78fc18a6e9d782a7047c01905","modified":1742118672199},{"_id":"themes/next/scripts/events/lib/config.js","hash":"aefe3b38a22bc155d485e39187f23e4f2ee5680a","modified":1742096449476},{"_id":"themes/next/source/lib/velocity/velocity.min.js","hash":"bf172816a9c57f9040e3d19c24e181a142daf92b","modified":1742096449542},{"_id":"themes/next/source/lib/velocity/velocity.ui.min.js","hash":"dde584994ac13dc601836e86f4cf490e418d9723","modified":1742096449542},{"_id":"themes/next/scripts/events/lib/injects-point.js","hash":"08496b71c9939718e7955704d219e44d7109247b","modified":1742096449477},{"_id":"themes/next/scripts/events/lib/injects.js","hash":"e73f697bb160b223fdde783237148be5f41c1d78","modified":1742096449477},{"_id":"themes/next/source/css/_variables/Gemini.styl","hash":"583ff1e7a2ca889f1f54eb0ca793894466823c7c","modified":1742096449527},{"_id":"themes/next/source/css/_variables/Mist.styl","hash":"5980abbbbeacd8541121f436fa414d24ad5e97c2","modified":1742096449527},{"_id":"themes/next/scripts/filters/comment/changyan.js","hash":"2f22f48f7370470cef78561a47c2a47c78035385","modified":1742096449479},{"_id":"themes/next/source/css/_variables/Muse.styl","hash":"c22b58af3327236ec54d5706501aa5a20e15012e","modified":1742096449527},{"_id":"themes/next/source/css/_variables/Pisces.styl","hash":"4e33774b1fe6d0a51f3a428c54c5e600e83bf154","modified":1742096449528},{"_id":"themes/next/source/css/_variables/base.styl","hash":"ad680efdfb2f86546182bf3f59886efbcf3c1b2d","modified":1742096449528},{"_id":"themes/next/scripts/filters/comment/default-config.js","hash":"0c3bea89d64bc12c1bbe6f208a83773c6fb5375a","modified":1742096449479},{"_id":"themes/next/scripts/filters/comment/common.js","hash":"713056d33dbcd8e9748205c5680b456c21174f4e","modified":1742096449479},{"_id":"themes/next/scripts/filters/comment/gitalk.js","hash":"323a47df6ded894944a2647db44556d6163e67c4","modified":1742096449480},{"_id":"themes/next/scripts/filters/comment/disqus.js","hash":"3a80559df0b670ccb065ea9d3bb587d0b61be3a4","modified":1742096449480},{"_id":"themes/next/scripts/filters/comment/valine.js","hash":"851359f5ff90f733a9bd7fe677edbee8b8ac714c","modified":1742096449482},{"_id":"themes/next/scripts/filters/comment/disqusjs.js","hash":"67cf90d9a2428c14eb113a64bdd213c22a019aef","modified":1742096449480},{"_id":"themes/next/source/lib/font-awesome/css/all.min.css","hash":"82e34d28f8a1169b20b60101d5bb0446deba3514","modified":1742096449539},{"_id":"themes/next/scripts/filters/comment/livere.js","hash":"a4f3153ac76a7ffdf6cc70f52f1b2cc218ed393e","modified":1742096449482},{"_id":"themes/next/source/lib/pjax/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1742118672174},{"_id":"themes/next/source/lib/pjax/.git/config","hash":"010b0a17841ab64c111503876d0d06bafdd7ef62","modified":1742118672181},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1742096449541},{"_id":"themes/next/source/lib/pjax/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1742118669795},{"_id":"themes/next/source/lib/pjax/.git/index","hash":"2391d7e3e5ac7fba6ab53e66d1b64cfad786494b","modified":1742118672209},{"_id":"themes/next/source/lib/pjax/example/example.js","hash":"4d9fea820f1a91590b0d49099e8a79d3984ba9bf","modified":1742118672187},{"_id":"themes/next/source/lib/pjax/example/forms.html","hash":"35769b826750d64ad649f93ee2272961d65a61c7","modified":1742118672188},{"_id":"themes/next/source/lib/pjax/lib/abort-request.js","hash":"a6eae74d5b80dccde2ee4485ee427aca39dbe66b","modified":1742118672190},{"_id":"themes/next/source/lib/pjax/.git/packed-refs","hash":"edbdf218d3ec5dca511b8e29e88aebcf725d68ad","modified":1742118672169},{"_id":"themes/next/source/lib/pjax/example/index.html","hash":"1cc0a16040597669b26e8a7168a620520c98f4a1","modified":1742118672188},{"_id":"themes/next/source/lib/pjax/lib/eval-script.js","hash":"773d5fddaad11e76df9354f3cf8078f26ad760ff","modified":1742118672190},{"_id":"themes/next/source/lib/pjax/example/page2.html","hash":"5396196c9179746716ef02833504b1172332243f","modified":1742118672188},{"_id":"themes/next/source/lib/pjax/lib/execute-scripts.js","hash":"b16da782b37eb0538ac267f4ec4d478e79e96fa6","modified":1742118672192},{"_id":"themes/next/source/lib/pjax/lib/is-supported.js","hash":"dea4eb52b70fd285ba3df161b51496d30677d9e9","modified":1742118672193},{"_id":"themes/next/source/lib/pjax/example/page3.html","hash":"703e2d34011c3d8918ba811ead26f1734d4d74fb","modified":1742118672189},{"_id":"themes/next/source/lib/pjax/lib/foreach-els.js","hash":"95df8390d94be89f23f50e2acbde8e08a6264096","modified":1742118672192},{"_id":"themes/next/source/lib/pjax/lib/foreach-selectors.js","hash":"e79ce0bccb20e9245605ae7b2d7dc35fa4213c45","modified":1742118672192},{"_id":"themes/next/source/lib/pjax/lib/send-request.js","hash":"ca86271d363eecd1abc854bf03491130101b427d","modified":1742118672195},{"_id":"themes/next/source/lib/pjax/lib/switches-selectors.js","hash":"1348fec349076f4ccf72bb6f01999988764bdafd","modified":1742118672196},{"_id":"themes/next/source/lib/pjax/tests/setup.js","hash":"de561f5d8212d10ae79188b294c94264e6f96386","modified":1742118672208},{"_id":"themes/next/source/lib/pjax/lib/switches.js","hash":"29ea82638d566d41dc6562873fb3d58310ca34d0","modified":1742118672196},{"_id":"themes/next/source/lib/pjax/lib/parse-options.js","hash":"0b29aa13306a0462d5c6ba0761acd75e65bb1691","modified":1742118672193},{"_id":"themes/next/source/css/_schemes/Gemini/index.styl","hash":"e342b8f8e11a3a6aa5a029912c9778c25bf5d135","modified":1742096449519},{"_id":"themes/next/source/lib/pjax/lib/uniqueid.js","hash":"643336dc475999234b7843b1e7b38c60a48041bc","modified":1742118672196},{"_id":"themes/next/source/lib/pjax/tests/test.ts","hash":"5fc54ab37b36965037b2476e62adbc684cdd5537","modified":1742118672208},{"_id":"themes/next/source/css/_schemes/Mist/_header.styl","hash":"b9e87d32da24264bda247c1526afe140c858b0ef","modified":1742096449520},{"_id":"themes/next/source/css/_schemes/Mist/_layout.styl","hash":"12b265f82840f27112ca2b1be497677f20f87545","modified":1742096449520},{"_id":"themes/next/source/css/_schemes/Mist/_menu.styl","hash":"716e8b0f056bf6393e6bc6969ac84598ab8e7a6f","modified":1742096449520},{"_id":"themes/next/source/css/_schemes/Mist/_posts-expand.styl","hash":"e1c29b81a32273a0dedd926cda199a71aea72624","modified":1742096449521},{"_id":"themes/next/source/css/_schemes/Mist/index.styl","hash":"c5142739e01e9f25c8b32b2209af85c787bb2b42","modified":1742096449521},{"_id":"themes/next/source/css/_schemes/Muse/_header.styl","hash":"8674bd88df076a1dfe4023ed6750ded1f5b00223","modified":1742096449521},{"_id":"themes/next/source/css/_schemes/Muse/_layout.styl","hash":"49c76bc723d3952abb613d9d68398ed7305da999","modified":1742096449521},{"_id":"themes/next/source/css/_schemes/Pisces/_header.styl","hash":"558794fced306339b98dc2b0ee7f0576802f1355","modified":1742096449524},{"_id":"themes/next/source/css/_schemes/Muse/_menu.styl","hash":"4b7f057dbb53efd7cbe7eac7835a793ab3cbb135","modified":1742096449523},{"_id":"themes/next/source/css/_schemes/Pisces/_layout.styl","hash":"5de34e1d8a290751641ae456c942410852d5e809","modified":1742096449525},{"_id":"themes/next/source/css/_schemes/Muse/_sidebar.styl","hash":"9898323ee5a7ac2a5d4f633c653112280beb2643","modified":1742096449523},{"_id":"themes/next/source/css/_schemes/Pisces/_menu.styl","hash":"0a9f0d9eb042595502d200fb8c65efb0e6c89aa9","modified":1742096449525},{"_id":"themes/next/source/css/_schemes/Pisces/_sidebar.styl","hash":"dc9318992ce2eb086ebaa2fe56b325e56d24098b","modified":1742096449525},{"_id":"themes/next/source/css/_schemes/Muse/index.styl","hash":"25c2a7930da14f023329df20f38df2728057fb4d","modified":1742096449524},{"_id":"themes/next/source/css/_schemes/Muse/_sub-menu.styl","hash":"2d3e05015796a790abd9d68957a5c698c0c9f9b6","modified":1742096449523},{"_id":"themes/next/source/css/_schemes/Pisces/index.styl","hash":"25c2a7930da14f023329df20f38df2728057fb4d","modified":1742096449526},{"_id":"themes/next/source/css/_common/components/components.styl","hash":"d17236df3b4d6def1e4e81133ef4729c390de3ac","modified":1742096449493},{"_id":"themes/next/source/css/_common/components/back-to-top-sidebar.styl","hash":"510a6f0ba7485dd54ce347cca890ab52c4957081","modified":1742096449492},{"_id":"themes/next/source/css/_common/components/back-to-top.styl","hash":"0534b329d279a6f255112b3305ff92c810f31724","modified":1742096449493},{"_id":"themes/next/source/css/_common/outline/mobile.styl","hash":"a2ee16cac29a82cfce26804c160286fcbee94161","modified":1742096449506},{"_id":"themes/next/source/css/_common/components/reading-progress.styl","hash":"c52648a7b09f9fe37858f5694fcc1ffc709ad147","modified":1742096449500},{"_id":"themes/next/source/css/_common/scaffolding/base.styl","hash":"5540c9259cb7895a5f10a289c7937e5470a7c134","modified":1742096449511},{"_id":"themes/next/source/css/_common/scaffolding/buttons.styl","hash":"45f4badac6ec45cf24355f6157aece1d4d3f1134","modified":1742096449511},{"_id":"themes/next/source/css/_common/outline/outline.styl","hash":"7a95c27762e1303bf06ee808c63f616cb192fcaf","modified":1742096449506},{"_id":"themes/next/source/css/_common/scaffolding/pagination.styl","hash":"b619f39e18398422e0ac4999d8f042a5eaebe9cd","modified":1742096449515},{"_id":"themes/next/source/css/_common/scaffolding/comments.styl","hash":"4b068d0d898f4e624937503f0e1428993050bd65","modified":1742096449512},{"_id":"themes/next/source/css/_schemes/Pisces/_sub-menu.styl","hash":"b69ac38b9da8c9c1b7de696fdeea7f9d7705213a","modified":1742096449526},{"_id":"themes/next/source/css/_common/scaffolding/tables.styl","hash":"f317d2e3886e94f5fbb8781c2e68edd19669ff58","modified":1742096449515},{"_id":"themes/next/source/css/_common/scaffolding/normalize.styl","hash":"6d740699fb6a7640647a8fd77c4ea4992d8d6437","modified":1742096449513},{"_id":"themes/next/source/css/_common/scaffolding/scaffolding.styl","hash":"43045d115f8fe95732c446aa45bf1c97609ff2a5","modified":1742096449515},{"_id":"themes/next/source/lib/pjax/.git/hooks/fsmonitor-watchman.sample","hash":"0ec0ec9ac11111433d17ea79e0ae8cec650dcfa4","modified":1742118669797},{"_id":"themes/next/source/lib/pjax/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1742118669797},{"_id":"themes/next/source/lib/pjax/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1742118669795},{"_id":"themes/next/source/css/_common/scaffolding/toggles.styl","hash":"20e0e3e3eba384930c022e21511214d244b4c9e7","modified":1742096449518},{"_id":"themes/next/source/lib/pjax/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1742118669797},{"_id":"themes/next/source/lib/pjax/.git/hooks/pre-push.sample","hash":"a599b773b930ca83dbc3a5c7c13059ac4a6eaedc","modified":1742118669856},{"_id":"themes/next/source/lib/pjax/.git/hooks/pre-commit.sample","hash":"8093d68e142db52dcab2215e770ba0bbe4cfbf24","modified":1742118669855},{"_id":"themes/next/source/lib/pjax/.git/hooks/pre-merge-commit.sample","hash":"04c64e58bc25c149482ed45dbd79e40effb89eb7","modified":1742118669856},{"_id":"themes/next/source/lib/pjax/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1742118669795},{"_id":"themes/next/source/lib/pjax/.git/hooks/pre-rebase.sample","hash":"288efdc0027db4cfd8b7c47c4aeddba09b6ded12","modified":1742118669857},{"_id":"themes/next/source/lib/pjax/.git/hooks/sendemail-validate.sample","hash":"74cf1d5415a5c03c110240f749491297d65c4c98","modified":1742118669858},{"_id":"themes/next/source/lib/pjax/.git/hooks/pre-receive.sample","hash":"705a17d259e7896f0082fe2e9f2c0c3b127be5ac","modified":1742118669857},{"_id":"themes/next/source/lib/pjax/.git/hooks/update.sample","hash":"730e6bd5225478bab6147b7a62a6e2ae21d40507","modified":1742118669858},{"_id":"themes/next/source/lib/pjax/.git/hooks/push-to-checkout.sample","hash":"508240328c8b55f8157c93c43bf5e291e5d2fbcb","modified":1742118669857},{"_id":"themes/next/source/lib/pjax/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1742118669860},{"_id":"themes/next/source/lib/pjax/.git/logs/HEAD","hash":"249808c632c87ec09dc5e6e23ca7ab86bde22b87","modified":1742118672178},{"_id":"themes/next/source/lib/pjax/lib/proto/attach-form.js","hash":"65db8791b8914ab29f2bad44202d2ee20cfaac1f","modified":1742118672194},{"_id":"themes/next/source/lib/pjax/lib/events/trigger.js","hash":"1e8b098cc0ff96a964fe90595bcf65b7d6ca1792","modified":1742118672191},{"_id":"themes/next/source/lib/pjax/lib/events/on.js","hash":"c8d6059a8117c61e64e10e65a977e6d82b866af8","modified":1742118672191},{"_id":"themes/next/source/lib/pjax/lib/events/off.js","hash":"edc5309f813bc323a0f470836d763aa54d996042","modified":1742118672191},{"_id":"themes/next/source/lib/pjax/lib/proto/attach-link.js","hash":"de0d6d91ecb8650e0cb49dc40470233c4bab57e9","modified":1742118672194},{"_id":"themes/next/source/lib/pjax/lib/proto/handle-response.js","hash":"43bff3061ccc910c86247735c6059ff489310a81","modified":1742118672194},{"_id":"themes/next/source/lib/pjax/.git/hooks/prepare-commit-msg.sample","hash":"2584806ba147152ae005cb675aa4f01d5d068456","modified":1742118669857},{"_id":"themes/next/source/lib/pjax/lib/proto/log.js","hash":"a61b7223e6af70a075bb7c6c5f6ebfa4adbc5f31","modified":1742118672195},{"_id":"themes/next/source/lib/pjax/lib/proto/parse-element.js","hash":"fa128f4b896c22c48c748b41cd6886ac7325a2d4","modified":1742118672195},{"_id":"themes/next/source/lib/pjax/lib/util/clone.js","hash":"862544f772345622927de6a4bcb27e4ef18aec3e","modified":1742118672197},{"_id":"themes/next/source/lib/pjax/lib/util/update-query-string.js","hash":"d505d78906844c180503c63767083d599bfbbfa8","modified":1742118672198},{"_id":"themes/next/source/lib/pjax/lib/util/noop.js","hash":"674eb07958489875f1d449b7864511f6bc70ee45","modified":1742118672198},{"_id":"themes/next/source/lib/pjax/lib/util/contains.js","hash":"77fd6688fb2c1818d9b35f17b6d3b6a70cc492f7","modified":1742118672197},{"_id":"themes/next/source/lib/pjax/lib/util/extend.js","hash":"bfd842a1c6b091b9a231652cc9aa4b59bcc4ea6f","modified":1742118672197},{"_id":"themes/next/source/lib/pjax/tests/lib/abort-request.js","hash":"edb7512db755a13b963510914152b53aaf490e40","modified":1742118672200},{"_id":"themes/next/source/lib/pjax/tests/lib/foreach-els.js","hash":"a94e48ceede2b4543e82f74ed0f2b2da3fb8357f","modified":1742118672201},{"_id":"themes/next/source/lib/pjax/tests/lib/execute-scripts.js","hash":"2419bf5111e717b720643dbf5f89e49f301b4ac0","modified":1742118672201},{"_id":"themes/next/source/lib/pjax/tests/lib/eval-scripts.js","hash":"3a11fd9978ccc4c5b5ca91699a55336633173d33","modified":1742118672200},{"_id":"themes/next/source/lib/pjax/tests/lib/foreach-selectors.js","hash":"2ca80242a94d2aab40fd0e25d91a6db4991802ce","modified":1742118672203},{"_id":"themes/next/source/lib/pjax/tests/lib/events.js","hash":"6c4e2b245605bf78d84fcd596034680f6e483dee","modified":1742118672201},{"_id":"themes/next/source/lib/pjax/tests/lib/is-supported.js","hash":"19f819a9508f4ce2ce11ad8f27979fa84d7a2a52","modified":1742118672203},{"_id":"themes/next/source/lib/pjax/tests/lib/parse-options.js","hash":"65baba099b1074521287a10f5ff3f4e99cf8f8ee","modified":1742118672203},{"_id":"themes/next/source/lib/pjax/tests/lib/switch-selectors.js","hash":"b3f170dd24a1d4a9aefbb5cdfbaad16dfafa6c80","modified":1742118672205},{"_id":"themes/next/source/lib/pjax/tests/lib/send-request.js","hash":"b8fd6112a0d908c0f4c64ec898578bedbd23ed70","modified":1742118672205},{"_id":"themes/next/source/css/_common/components/pages/breadcrumb.styl","hash":"236a039b0900f4267de566b46f62314ad967d30f","modified":1742096449493},{"_id":"themes/next/source/lib/pjax/tests/lib/uniqueid.js","hash":"e0bc8cf01dcc15eb72a856c4f0734f02790e3275","modified":1742118672206},{"_id":"themes/next/source/css/_common/components/pages/pages.styl","hash":"6cf78a379bb656cc0abb4ab80fcae60152ce41ad","modified":1742096449495},{"_id":"themes/next/source/css/_common/components/pages/categories.styl","hash":"18edddb2ffb3f85a68e4367f81e06c461e07bc25","modified":1742096449495},{"_id":"themes/next/source/css/_common/components/post/post-collapse.styl","hash":"a52f8cae599099231866298ed831fdf76c9b6717","modified":1742096449496},{"_id":"themes/next/source/css/_common/components/pages/tag-cloud.styl","hash":"97974c231b4659b8aa5e9321c4d54db5c816d0db","modified":1742096449495},{"_id":"themes/next/source/css/_common/components/post/post-eof.styl","hash":"70b3eb9d36543ab92796ac163544e9cf51b7c1e6","modified":1742096449496},{"_id":"themes/next/source/css/_common/components/post/post-copyright.styl","hash":"9af620eba5ccceea21a0e3bc69f6f1fa7637c2f3","modified":1742096449496},{"_id":"themes/next/source/lib/pjax/tests/lib/switches.js","hash":"0274886c7f0985da381c234a3ddca510e1a8cc0f","modified":1742118672205},{"_id":"themes/next/source/css/_common/components/post/post-expand.styl","hash":"97dec98d0403097d66822f1c90b50b2890c84698","modified":1742096449497},{"_id":"themes/next/source/css/_common/components/pages/schedule.styl","hash":"f6f05f02d50f742c84ee5122016c0563a8bb2cf9","modified":1742096449495},{"_id":"themes/next/source/css/_common/components/post/post-gallery.styl","hash":"0dfb97703a519d9438f64f9e41ab1dd37381f733","modified":1742096449498},{"_id":"themes/next/source/css/_common/components/post/post-followme.styl","hash":"57b9a179675f1536e017cba457b6ac575e397c4f","modified":1742096449497},{"_id":"themes/next/source/css/_common/components/post/post-header.styl","hash":"93ba8172c0d2c37d738e6dbd44fcd5a2e23b92f3","modified":1742096449498},{"_id":"themes/next/source/css/_common/components/post/post-nav.styl","hash":"2c24829d95c742eb9e8316ebf2fbe9f2c168b59a","modified":1742096449499},{"_id":"themes/next/source/css/_common/components/post/post-reward.styl","hash":"66fc406796b6efe6cea76550573b7a632112406a","modified":1742096449499},{"_id":"themes/next/source/css/_common/components/post/post-rtl.styl","hash":"09dda2667628d1f91b2e37d8fc6df1413f961b64","modified":1742096449499},{"_id":"themes/next/source/css/_common/components/post/post-tags.styl","hash":"5cc9e7394c927065c688cba5edd6e0a27587f1d8","modified":1742096449499},{"_id":"themes/next/source/css/_common/components/third-party/gitalk.styl","hash":"b87f4a06c0db893df4f756f24be182e1a4751f24","modified":1742096449502},{"_id":"themes/next/source/css/_common/components/post/post.styl","hash":"fcd64c23d17775b3635325f6758b648d932e79b5","modified":1742096449500},{"_id":"themes/next/source/css/_common/components/post/post-widgets.styl","hash":"b266d2ce5e2b117be01537889e839a69004dc0bb","modified":1742096449500},{"_id":"themes/next/source/css/_common/components/third-party/search.styl","hash":"bad99f4cccb93b3cefe990a2c85124e60698d32e","modified":1742096449503},{"_id":"themes/next/source/css/_common/components/third-party/third-party.styl","hash":"1f6b0d3ab227697ca115e57fd61122ea7950e19d","modified":1742096449503},{"_id":"themes/next/source/css/_common/outline/header/bookmark.styl","hash":"b4f4bae437d4f994af93cf142494ffcd86bae46b","modified":1742096449504},{"_id":"themes/next/source/css/_common/outline/header/github-banner.styl","hash":"b31c86d1a4f89837f9187bed646bda96b2cd286c","modified":1742096449504},{"_id":"themes/next/source/css/_common/outline/header/header.styl","hash":"300058ca12e81013e77ba01fe66ac210525768b6","modified":1742096449504},{"_id":"themes/next/source/css/_common/outline/header/menu.styl","hash":"7a3a56b10ab714c0e2ed240d0939deeecdcad167","modified":1742096449505},{"_id":"themes/next/source/css/_common/outline/header/headerband.styl","hash":"6d5f26646e2914474f295de8bf6dc327d4acd529","modified":1742096449505},{"_id":"themes/next/source/css/_common/outline/header/site-meta.styl","hash":"3d16ac0f4ccaeed868c246d4d49bde543d1f62cb","modified":1742096449505},{"_id":"themes/next/source/css/_common/outline/footer/footer.styl","hash":"7eeb22c5696f8e0c95161dc57703973cf81c8c12","modified":1742096449503},{"_id":"themes/next/source/css/_common/components/third-party/related-posts.styl","hash":"8ed7a9d5dfac592de703421b543978095129aa5b","modified":1742096449502},{"_id":"themes/next/source/css/_common/outline/header/site-nav.styl","hash":"b8c816fba0a9b4a35fbae03ba5b1b2da96ba2687","modified":1742096449506},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author-links.styl","hash":"49722d555a2edb18094bb2cb3d7336dd72051b93","modified":1742096449508},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-blogroll.styl","hash":"096f908c08ce553e482aadfd3e767a0145191093","modified":1742096449509},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-button.styl","hash":"525242ce9e912c4adfe5134347c67dbdb9e98e3d","modified":1742096449509},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-author.styl","hash":"357f825f0a649b2e28cba1481d4c9a0cb402e43a","modified":1742096449509},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-dimmer.styl","hash":"12f7eaf6b56624cbc411528562d6bb848ff97039","modified":1742096449509},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-nav.styl","hash":"b11b04737a1a0fea3bd9f0081d96ee6c015358d4","modified":1742096449510},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toc.styl","hash":"fa0a2ea57b7b4ce75b5d18c264af2d92ea3192f9","modified":1742096449510},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar-toggle.styl","hash":"098b4bdf49c7300490f959386d5d1185a32543f6","modified":1742096449510},{"_id":"themes/next/source/css/_common/scaffolding/highlight/copy-code.styl","hash":"4079e616fbf36112dec0674c1e0713d1d9769068","modified":1742096449512},{"_id":"themes/next/source/css/_common/scaffolding/highlight/diff.styl","hash":"83bd737f663a8461e66985af8ddbfc0a731fc939","modified":1742096449512},{"_id":"themes/next/source/css/_common/outline/sidebar/site-state.styl","hash":"67a1fcb33535122d41acd24f1f49cf02c89b88fa","modified":1742096449511},{"_id":"themes/next/source/css/_common/outline/sidebar/sidebar.styl","hash":"5d540f683018745a5ed1d6f635df28ea610c1244","modified":1742096449511},{"_id":"themes/next/source/css/_common/scaffolding/tags/blockquote-center.styl","hash":"ceacfa6218f6084c71a230b086e5d2708d29927e","modified":1742096449516},{"_id":"themes/next/source/css/_common/scaffolding/highlight/theme.styl","hash":"c911045b2ce9a66e38d9dd30c7ed078abbc10cbf","modified":1742096449513},{"_id":"themes/next/source/css/_common/scaffolding/tags/group-pictures.styl","hash":"aca7bb220fc14ef2a8f96282d2a95a96a9238d46","modified":1742096449517},{"_id":"themes/next/source/css/_common/scaffolding/tags/label.styl","hash":"8b7aafb911850c73074cdb6cc87abe4ac8c12e99","modified":1742096449517},{"_id":"themes/next/source/css/_common/scaffolding/highlight/highlight.styl","hash":"80488259271bcfe38031f4c2e902463daba9336b","modified":1742096449513},{"_id":"themes/next/source/css/_common/scaffolding/tags/pdf.styl","hash":"03a5bcecc0b12231462ef6ffe432fa77ee71beff","modified":1742096449518},{"_id":"themes/next/source/css/_common/scaffolding/tags/tabs.styl","hash":"3256e39f281f06751a1c0145d9806a0e56d68170","modified":1742096449518},{"_id":"themes/next/source/css/_common/scaffolding/tags/tags.styl","hash":"51d46fa3c7c6b691c61a2c2b0ac005c97cfbf72b","modified":1742096449518},{"_id":"themes/next/source/lib/pjax/.git/objects/pack/pack-c44afb8b14583aea8a946bf5944820752f2a752d.idx","hash":"db4f698e84be1b114c7d29712172449b3183c286","modified":1742118671946},{"_id":"themes/next/source/lib/pjax/tests/lib/util/contains.js","hash":"c8b155c78944368f37715f9631115769ea1f645b","modified":1742118672207},{"_id":"themes/next/source/css/_common/scaffolding/tags/note.styl","hash":"adaf0f580fccf4158169eeaf534a18005b39a760","modified":1742096449517},{"_id":"themes/next/source/lib/pjax/tests/lib/util/clone.js","hash":"306d6430ddcddf7c67c1d76ed784036713cd86a1","modified":1742118672206},{"_id":"themes/next/source/lib/pjax/.git/objects/pack/pack-c44afb8b14583aea8a946bf5944820752f2a752d.rev","hash":"15a069545fa0a13b00a5076d876d2c6f562e5106","modified":1742118671993},{"_id":"themes/next/source/lib/pjax/tests/lib/util/extend.js","hash":"f46afc9bf5c2ebe3766ffeb0ee638ac610e70332","modified":1742118672207},{"_id":"themes/next/source/lib/pjax/tests/lib/util/update-query-string.js","hash":"ffd39a45f445e2f2d20593415ef31280bff7dfdd","modified":1742118672208},{"_id":"themes/next/source/lib/pjax/tests/lib/util/noop.js","hash":"ed2fb792fe378b9b92cf7854a3e395fc6eec21bf","modified":1742118672207},{"_id":"themes/next/source/lib/pjax/tests/lib/proto/attach-link.js","hash":"9b1b047dc1d1d3d114540438511afcedb5b86528","modified":1742118672204},{"_id":"themes/next/source/lib/pjax/tests/lib/proto/handle-response.js","hash":"1dbbfa86a360c1109fc1153bea363cfd57d05afe","modified":1742118672204},{"_id":"themes/next/source/lib/pjax/.git/refs/heads/master","hash":"3c8414e83b32974579667c4c79e04af7d699edde","modified":1742118672177},{"_id":"themes/next/source/lib/pjax/tests/lib/proto/parse-element.js","hash":"942486a70ec1f582f456834df4f22effa2603a87","modified":1742118672205},{"_id":"themes/next/source/lib/pjax/.git/logs/refs/heads/master","hash":"249808c632c87ec09dc5e6e23ca7ab86bde22b87","modified":1742118672179},{"_id":"themes/next/source/lib/pjax/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1742118672171},{"_id":"themes/next/source/lib/pjax/.git/logs/refs/remotes/origin/HEAD","hash":"249808c632c87ec09dc5e6e23ca7ab86bde22b87","modified":1742118672173},{"_id":"themes/next/source/lib/pjax/tests/lib/proto/attach-form.js","hash":"93675412f1c9e4008ab5c64edcb9a08b549bb477","modified":1742118672204},{"_id":"themes/next/source/css/_common/components/third-party/math.styl","hash":"d83102771df652769e51ddfd041cf5f4ca1a041d","modified":1742096449502},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1742096449539},{"_id":"themes/next/source/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1742096449541},{"_id":"themes/next/source/lib/pjax/.git/objects/pack/pack-c44afb8b14583aea8a946bf5944820752f2a752d.pack","hash":"c1012ba4ce7f42ed478471cc6dd0c20153bfc19f","modified":1742118671945},{"_id":"public/search.xml","hash":"b45d0dbeeb8a2100781ceb954eefe4f6a92ae874","modified":1742453879827},{"_id":"public/about/index.html","hash":"4786c8058fdc782ff1b481bc63fd8c110b5f74c4","modified":1742453879827},{"_id":"public/tags/index.html","hash":"1e008fd832a802b8cc63c03cf7e60065f1b7e5aa","modified":1742453879827},{"_id":"public/categories/index.html","hash":"bd96ad49acb2474c879330f5483a18cd6a90a23c","modified":1742453879827},{"_id":"public/404.html","hash":"da2bf64f84cfef19bacda6d3cf17e0b2c3ff4f5f","modified":1742453879827},{"_id":"public/2025/03/20/李哥考研复试项目/线性回归练习代码解读/index.html","hash":"a24d8039c1dee493a8fb5bc7218b8371e408a9c5","modified":1742453879827},{"_id":"public/2025/03/20/深度学习/深度学习python库/index.html","hash":"fcdd8abb9d93dd93383643f0007accd5170ea111","modified":1742453879827},{"_id":"public/2025/03/17/python/python基础/index.html","hash":"b58446d3fe6c83273f1ff24ef170bb2dece793d9","modified":1742453879827},{"_id":"public/categories/python/index.html","hash":"c34e494efa219bf4cd908d43b6e3c85f1baa7ef7","modified":1742453879827},{"_id":"public/categories/李哥考研复试项目/index.html","hash":"81741dfc35842c7b11a497e076b79bee40df051e","modified":1742453879827},{"_id":"public/categories/深度学习/index.html","hash":"d48b8e587e0ca0566d6ac1dce8b277c137b20f38","modified":1742453879827},{"_id":"public/archives/index.html","hash":"8e7ebf68981c180009712df152fd0fa3c239a1be","modified":1742453879827},{"_id":"public/archives/2025/index.html","hash":"88e8a29521b0d0804d5edaefdb2a809e0b1de397","modified":1742453879827},{"_id":"public/index.html","hash":"f1f5518bf962e8eee96f4c0e9b2bf1f632636b83","modified":1742453879827},{"_id":"public/tags/python/index.html","hash":"26f238d86adc54f911ad24686dce4a122d70f4d8","modified":1742453879827},{"_id":"public/archives/2025/03/index.html","hash":"5cd9fed51b9ad913bbdce14b5d247c5373e0f964","modified":1742453879827},{"_id":"public/tags/深度学习/index.html","hash":"72f13d8d6f6de61963fe53b4f60c300fd55bc809","modified":1742453879827},{"_id":"public/tags/计算机考研复试/index.html","hash":"9b7501714101c51d0d65576d0ffc544d0ecf8f0e","modified":1742453879827},{"_id":"public/images/algolia_logo.svg","hash":"45eeea0b5fba833e21e38ea10ed5ab385ceb4f01","modified":1742453879827},{"_id":"public/CNAME","hash":"b4bbb1bb35f75f6249c323af383e3e32e43788a4","modified":1742453879827},{"_id":"public/images/apple-touch-icon-mw.png","hash":"9462336d3638b5c2002b1d7a4358fc3e1fe76f35","modified":1742453879827},{"_id":"public/images/avatar.gif","hash":"764ff4a365c70484ea8581f582d0997ca28e247e","modified":1742453879827},{"_id":"public/images/cc-by-nc-nd.svg","hash":"bc3588c9b2d7c68830524783120ff6cf957cf668","modified":1742453879827},{"_id":"public/images/cc-by-nc-sa.svg","hash":"6f55543d1fb9cbc436c101d24f802dec7b41efc3","modified":1742453879827},{"_id":"public/images/cc-by-nd.svg","hash":"42cd73da328077ccc92f859bb8f3cf621b3484f8","modified":1742453879827},{"_id":"public/images/cc-by-sa.svg","hash":"70c1535f43e54e5ff35ca81419e77e4c0c301398","modified":1742453879827},{"_id":"public/images/cc-by-nc.svg","hash":"6f076713fb9bf934aa2c1046bdf2cf2e37bc1eab","modified":1742453879827},{"_id":"public/images/cc-by.svg","hash":"e92a33c32d1dac8ed94849b2b4e6456e887efe70","modified":1742453879827},{"_id":"public/images/logo_mw.svg","hash":"a81d429d44b6cd997ae970dee3d348cf6e778fc9","modified":1742453879827},{"_id":"public/images/cc-zero.svg","hash":"9bfb52b2f63527a7049247bf00d44e6dc1170e7d","modified":1742453879827},{"_id":"public/images/mw-icon.ico","hash":"4de7d6e3c4d97d7345cabae665d7aa89a2035a00","modified":1742453879827},{"_id":"public/lib/pjax/LICENSE","hash":"2c7168814d9d35ea9500809b0904962f511eb4a8","modified":1742453879827},{"_id":"public/images/mw-icon-96x96.png","hash":"9d436438e15be6f8be2c99a31b9bc284daaf2a14","modified":1742453879827},{"_id":"public/lib/font-awesome/webfonts/fa-regular-400.woff2","hash":"260bb01acd44d88dcb7f501a238ab968f86bef9e","modified":1742453879827},{"_id":"public/lib/pjax/index.d.ts","hash":"c452cd6e990eeeea10f7dbf17fefbc6845585bf6","modified":1742453879827},{"_id":"public/js/algolia-search.js","hash":"498d233eb5c7af6940baf94c1a1c36fdf1dd2636","modified":1742453879827},{"_id":"public/js/local-search.js","hash":"35ccf100d8f9c0fd6bfbb7fa88c2a76c42a69110","modified":1742453879827},{"_id":"public/lib/anime.min.js","hash":"47cb482a8a488620a793d50ba8f6752324b46af3","modified":1742453879827},{"_id":"public/js/bookmark.js","hash":"9734ebcb9b83489686f5c2da67dc9e6157e988ad","modified":1742453879827},{"_id":"public/css/main.css","hash":"962b4d899be34690ce85bde7ec582d389f280da0","modified":1742453879827},{"_id":"public/lib/pjax/tests/test.ts","hash":"5fc54ab37b36965037b2476e62adbc684cdd5537","modified":1742453879827},{"_id":"public/2025/03/20/深度学习/深度学习python库/Pastedimage20250319161350.png","hash":"e6126b90bb3185326441ebc2ef7931d41fb53a5d","modified":1742453879827},{"_id":"public/2025/03/20/深度学习/深度学习python库/Snipaste_2025-03-20_14-56-59.jpg","hash":"60461b4219b9f7381afb32a29389a968fcfb2e8f","modified":1742453879827},{"_id":"public/2025/03/20/李哥考研复试项目/线性回归练习代码解读/Pastedimage20250319232552.png","hash":"4aaf5ced7c0ff43a447e910bd55afafeec5d3f0b","modified":1742453879827},{"_id":"public/2025/03/20/李哥考研复试项目/线性回归练习代码解读/5a6c5073a5cd2f4a2e6f7ca2e39dc5f.png","hash":"e45bc31449d6c732c18f3da933d4e99b2e66428c","modified":1742453879827},{"_id":"public/2025/03/20/李哥考研复试项目/线性回归练习代码解读/06ec95ff3c17534440c17f2c5081b49.png","hash":"a391d6ae0fe48dec5ed076cee5e7c45883fc82f4","modified":1742453879827},{"_id":"public/lib/font-awesome/webfonts/fa-brands-400.woff2","hash":"509988477da79c146cb93fb728405f18e923c2de","modified":1742453879827},{"_id":"public/lib/font-awesome/webfonts/fa-solid-900.woff2","hash":"75a88815c47a249eadb5f0edc1675957f860cca7","modified":1742453879827},{"_id":"public/js/motion.js","hash":"72df86f6dfa29cce22abeff9d814c9dddfcf13a9","modified":1742453879827},{"_id":"public/js/utils.js","hash":"730cca7f164eaf258661a61ff3f769851ff1e5da","modified":1742453879827},{"_id":"public/js/schemes/pisces.js","hash":"0ac5ce155bc58c972fe21c4c447f85e6f8755c62","modified":1742453879827},{"_id":"public/js/schemes/muse.js","hash":"1eb9b88103ddcf8827b1a7cbc56471a9c5592d53","modified":1742453879827},{"_id":"public/js/next-boot.js","hash":"a1b0636423009d4a4e4cea97bcbf1842bfab582c","modified":1742453879827},{"_id":"public/lib/pjax/CHANGELOG.html","hash":"963959b2192407bb369887950b5e652820f8a8ad","modified":1742453879827},{"_id":"public/lib/pjax/pjax.js","hash":"e973e2c6ffb6f6d7b4a1730ba0ed6d3628b2b39f","modified":1742453879827},{"_id":"public/lib/pjax/index.js","hash":"4af61bcf253ce67d67bb0f524b441869301de9a5","modified":1742453879827},{"_id":"public/lib/pjax/package.json","hash":"da55143475b12cb91a44aeb5a995c53879fe3326","modified":1742453879827},{"_id":"public/lib/pjax/pjax.min.js","hash":"68d0c6ad28f042c78fc18a6e9d782a7047c01905","modified":1742453879827},{"_id":"public/lib/pjax/README.html","hash":"ac4d2dd85201c0e4c7cca73c76c9c72f01439153","modified":1742453879827},{"_id":"public/lib/velocity/velocity.min.js","hash":"2f1afadc12e4cf59ef3b405308d21baa97e739c6","modified":1742453879827},{"_id":"public/lib/velocity/velocity.ui.min.js","hash":"ed5e534cd680a25d8d14429af824f38a2c7d9908","modified":1742453879827},{"_id":"public/lib/font-awesome/css/all.min.css","hash":"0038dc97c79451578b7bd48af60ba62282b4082b","modified":1742453879827},{"_id":"public/lib/pjax/example/example.js","hash":"48b27c93eb351d2fa91247c35038a9f1627e971a","modified":1742453879827},{"_id":"public/lib/pjax/example/forms.html","hash":"aa62fc6e697e17e16a947eb0176dc24e2e42d72c","modified":1742453879827},{"_id":"public/lib/pjax/lib/abort-request.js","hash":"cbae038c94f70163340801cb608f4e503640b88d","modified":1742453879827},{"_id":"public/lib/pjax/lib/eval-script.js","hash":"d5980b3c5f3c1a95427eab0d7d2263324fb72bca","modified":1742453879827},{"_id":"public/lib/pjax/example/index.html","hash":"4a00b556253eeb11fc10013c8c3940a019154001","modified":1742453879827},{"_id":"public/lib/pjax/example/page2.html","hash":"7654e0a5a471bec5d74fb88d9f2f218817afdd52","modified":1742453879827},{"_id":"public/lib/pjax/lib/execute-scripts.js","hash":"8c7c18b14c11af7e2d8dd0146b7df428bf8149bb","modified":1742453879827},{"_id":"public/lib/pjax/lib/is-supported.js","hash":"62ede7268080ab7efddd64fced8b3e79af2901f6","modified":1742453879827},{"_id":"public/lib/pjax/lib/foreach-els.js","hash":"d8e50967878930891112f4acb46f47697ddc3ad7","modified":1742453879827},{"_id":"public/lib/pjax/lib/switches-selectors.js","hash":"f31b4af163db3d9a5b3bb7c7534a4a2dd5543435","modified":1742453879827},{"_id":"public/lib/pjax/lib/send-request.js","hash":"15c341d066b0d60879ef3ca5ff858103381fb378","modified":1742453879827},{"_id":"public/lib/pjax/lib/foreach-selectors.js","hash":"b11d36344a7a6aa36d533a335785772c9bfb6bc9","modified":1742453879827},{"_id":"public/lib/pjax/example/page3.html","hash":"8d1d6a9afd80caf29e5b22f10a849b23a864a6a4","modified":1742453879827},{"_id":"public/lib/pjax/lib/events/off.js","hash":"79920b24246e150f1c73511f1a271b93b9c5d9c8","modified":1742453879827},{"_id":"public/lib/pjax/lib/parse-options.js","hash":"054a902e9f350f7414b80f62419da004dd90aa37","modified":1742453879827},{"_id":"public/lib/pjax/lib/switches.js","hash":"4e283f667c10e0fea15e269f1c73ccb699979bb4","modified":1742453879827},{"_id":"public/lib/pjax/tests/setup.js","hash":"4a9489e72cbe98527931d95d68b68e2d6a9bff6e","modified":1742453879827},{"_id":"public/lib/pjax/lib/proto/attach-link.js","hash":"d15a61235aa8c1615e60ea34d9bbee834be804d0","modified":1742453879827},{"_id":"public/lib/pjax/lib/events/trigger.js","hash":"c05ba73199c5e2c7682655491ed37743bd451751","modified":1742453879827},{"_id":"public/lib/pjax/lib/events/on.js","hash":"111fba16cc4ca701aefedea2589c832b60af303d","modified":1742453879827},{"_id":"public/lib/pjax/lib/proto/parse-element.js","hash":"52bef8a845aab0d54907f73875fa58b4084019be","modified":1742453879827},{"_id":"public/lib/pjax/lib/proto/attach-form.js","hash":"b41c66e58897e6371fbd04cd2b9eb7bae10e0d89","modified":1742453879827},{"_id":"public/lib/pjax/lib/uniqueid.js","hash":"eb127cf8936e5bdcaa7459cf5bd042ff96b5a387","modified":1742453879827},{"_id":"public/lib/pjax/lib/proto/log.js","hash":"c728730594aed7d81509787c5bb487af014415e1","modified":1742453879827},{"_id":"public/lib/pjax/lib/proto/handle-response.js","hash":"f68bab958cce4dbbcb91920d7a8aba55b9066b41","modified":1742453879827},{"_id":"public/lib/pjax/lib/util/clone.js","hash":"55027b88e007818c6963205fcd0090a61af8237c","modified":1742453879827},{"_id":"public/lib/pjax/lib/util/extend.js","hash":"e599aaef93e3a00ccbd92cd87df09898d712e398","modified":1742453879827},{"_id":"public/lib/pjax/lib/util/contains.js","hash":"fcc9be0477d941b7c4ba71de328773e556ded7ce","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/events.js","hash":"70ca7ec140d1dcf8c4958e7b6788b09ef2b60181","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/abort-request.js","hash":"96d7a604ca275455199bd94d16b9cdc254bee9b9","modified":1742453879827},{"_id":"public/lib/pjax/lib/util/update-query-string.js","hash":"eefcbff322f8c607865d47ee8fa49f1c8bfa8d7a","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/execute-scripts.js","hash":"2d02a0151ead9924516cdae9308f8667b27a7ea7","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/foreach-els.js","hash":"85995dd27d0d866cfaf5c560732fab8b0099938c","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/foreach-selectors.js","hash":"898b48ef191719070488f16a54fba8a794b10057","modified":1742453879827},{"_id":"public/lib/pjax/lib/util/noop.js","hash":"2c9f7e231af1a62db195875ca62df7edc253c3c4","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/switch-selectors.js","hash":"993dd67b1b629288feb8d167fb12b602626cf1d5","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/send-request.js","hash":"d6adad7f95754c53b9885c0999a5118995fdc76f","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/is-supported.js","hash":"e25c7e69c8b7a64a44affcae317adb2b3889aee6","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/parse-options.js","hash":"45ab302da33e5457a71fd22bbd082e51f5b06b49","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/switches.js","hash":"f5dfdf57bafd2b4edd5f854d70837b5fe760756e","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/eval-scripts.js","hash":"3c9125a04cec9d511f16f9311656533ef6a115d1","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/uniqueid.js","hash":"430550f31b9e6356c2e3a033c1b418bd6acd9667","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/util/clone.js","hash":"4b0e62bd0f4eef3549b0da70987f63ce4953bef0","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/proto/attach-form.js","hash":"3614a962e3185b354a2e0d987bcecdef4a209da6","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/util/contains.js","hash":"bc0c8be855e9736cb256a2aa44fd46f3a7d1d6c5","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/proto/attach-link.js","hash":"9d283cb0b09730e932673c73a3352a2d2e8939fa","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/util/update-query-string.js","hash":"4158dcd877c5f229400417e0da22eb4c5b8e8e38","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/util/extend.js","hash":"d45488ff6f8e3e1b59a62d0c88c86b750fe50ed4","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/util/noop.js","hash":"6959a4c02c41453a5edfa4163e36752fcfa3edf9","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/proto/handle-response.js","hash":"26e0e334f0b20ff2fe75b3ab12a93ce44a5823f0","modified":1742453879827},{"_id":"public/lib/pjax/tests/lib/proto/parse-element.js","hash":"474edc8844028de67cbad054da79c05727620785","modified":1742453879827},{"_id":"public/2025/03/20/李哥考研复试项目/线性回归练习代码解读/Pastedimage20250319234616.png","hash":"79782b5dd1386f1ea3b26403342af7d038f84d7a","modified":1742453879827},{"_id":"public/2025/03/20/李哥考研复试项目/线性回归练习代码解读/0b0fcf44d3b47e8a5e9e12faccd42bc.png","hash":"7576a9d367eb4092b23dcf617653f612b48b6350","modified":1742453879827},{"_id":"public/2025/03/20/李哥考研复试项目/线性回归练习代码解读/607dc97725be3eecbb191ed4887eadb.png","hash":"5b363b29502702a217ca028265822eef3455faa0","modified":1742453879827}],"Category":[{"name":"python","_id":"cm8h01uzl0005ekeqfwzr2leq"},{"name":"深度学习","_id":"cm8h01uzs000aekeq7lgdb25u"},{"name":"李哥考研复试项目","_id":"cm8h01v0n000iekeq1ecrgxd0"}],"Data":[{"_id":"footer","data":"<script color=\"0,0,255\" opacity=\"0.5\" zIndex=\"-1\" count=\"99\" src=\"https://cdn.jsdelivr.net/npm/canvas-nest.js@1/dist/canvas-nest.js\"></script>"},{"_id":"styles","data":""}],"Page":[{"title":"404 Not Found","_content":"\n<center>\n对不起，您所访问的页面不存在或已删除。\n您可以<a href=\"https://geek-martin.github.io\">点击此处</a>返回首页。\n</center>\n\n<blockquote class=\"blockquote-center\">\n    Geek-Martin\n</blockquote>","source":"404.md","raw":"---\ntitle: 404 Not Found\n---\n\n<center>\n对不起，您所访问的页面不存在或已删除。\n您可以<a href=\"https://geek-martin.github.io\">点击此处</a>返回首页。\n</center>\n\n<blockquote class=\"blockquote-center\">\n    Geek-Martin\n</blockquote>","date":"2025-03-16T10:24:06.418Z","updated":"2025-03-16T10:24:06.418Z","path":"404.html","comments":1,"layout":"page","_id":"cm8h01uzg0000ekeq2s6b8mad","content":"<center>\n对不起，您所访问的页面不存在或已删除。\n您可以<a href=\"https://geek-martin.github.io\">点击此处</a>返回首页。\n</center>\n\n<blockquote class=\"blockquote-center\">\n    Geek-Martin\n</blockquote>","excerpt":"","more":"<center>\n对不起，您所访问的页面不存在或已删除。\n您可以<a href=\"https://geek-martin.github.io\">点击此处</a>返回首页。\n</center>\n\n<blockquote class=\"blockquote-center\">\n    Geek-Martin\n</blockquote>"},{"title":"About","date":"2025-03-16T13:52:11.000Z","comments":0,"_content":"","source":"about/index.md","raw":"---\ntitle: About\ndate: 2025-03-16 21:52:11\ncomments: false\n---\n","updated":"2025-03-18T10:25:33.208Z","path":"about/index.html","layout":"page","_id":"cm8h01uzj0002ekeq4y2b96i9","content":"","excerpt":"","more":""},{"title":"Tags","date":"2025-03-16T13:55:08.000Z","type":"tags","comments":0,"_content":"","source":"tags/index.md","raw":"---\ntitle: Tags\ndate: 2025-03-16 21:55:08\ntype: tags\ncomments: false\n---\n","updated":"2025-03-18T10:25:19.203Z","path":"tags/index.html","layout":"page","_id":"cm8h01uzl0004ekeq62sw49wy","content":"","excerpt":"","more":""},{"title":"Categories","date":"2025-03-16T13:56:51.000Z","type":"categories","comments":0,"_content":"","source":"categories/index.md","raw":"---\ntitle: Categories\ndate: 2025-03-16 21:56:51\ntype: categories\ncomments: false\n---\n","updated":"2025-03-18T10:22:44.614Z","path":"categories/index.html","layout":"page","_id":"cm8h01v0l000fekeq2ljbbmtk","content":"","excerpt":"","more":""}],"Post":[{"title":"python基础","_content":"# 数据结构\n## 整型/浮点型/字符串\n```python\na = 3 #整型\na = 3.0 #浮点型\nname = \"martin\"\n```\n### 使用函数str()避免类型错误\n```python\nage = 23\nmessage = \"Happy\" + str(age) + \"rd Birthday!\"\nprint(message)\n```\n这里，调用函数str()，python将非字符串值表示为字符串\n## 列表\n关键词：list \\[]\n```python\nlist1 = [1, 2, 3, 4, 5]\nprint(list1)\nprint(list1[2]) # 3\nprint(list1[-1]) # 5\n```\nlist\\[-1]将访问列表最后一个值，-2为倒数第二个值，以此类推。\n\n```python\nlist2 = [1, \"art\", dict1] #同一个列表中支持多种数据类型\n```\n## 字典（哈希表）\n关键词： dict {key: value}\n```python\ndict1 = {\"name\": \"martin\", \"age\": 18, 20: 80}\nprint(dict1[\"name\"])\nprint(dict1[\"age\"])\nprint(dict1[20])\n```\n\n# 运算\n`//` 地板除\n`/` 普通除\n`**` 乘方\n```python\nprint(9/2) # 4.5\nprint(9//2) # 4\nprint(9**2) # 81\n```\n\n# 函数\n带初值的函数示例\n```python\ndef func1(a, b = 2) #如果传入b，返回a**b；否则，返回a**2\n\treturn a ** b\n\nprint(func1(2)) # 4\n```\n\n# 列表\n## 切片\n```python\nlist1 = [1, 2, 3, 4, 5]\nprint(list1[1:4]) # [2, 3, 4] 左闭右开\nprint(list1[1:-1]) # [2, 3, 4]\nprint(list1[:]) # [1, 2, 3, 4, 5]\nprint(list1[1:]) # [2, 3, 4, 5]\n```\n## 操作\n### 添加元素\n```python\nlist1.append(6) # [1, 2, 3, 4, 5, 6] 在末尾添加\nlist1.insert(0, 0.5) #[0.5, 1, 2, 3, 4, 5] 在指定位置添加\nlist1.extend([8, 9]) #在列表末尾一次性追加另一个列表的多个值，即用新列表扩展原来的列表\n```\n### 删除元素\n```python\ndel list1[0] #删除索引0处的元素，且不再以任何方式使用它\nlist1.pop(index) #删除索引index处的元素，但会继续使用它，index为空时默认为-1，即栈顶\nlist1.remove(key) #删除列表中值为key的元素，也可继续使用它的值；若列表中有多个值为key，remove只删除第一个指定的值\n```\n### 列表排序\n```python\nlist1.sort() # 永久顺序排序\nlist1.sort(reverse = True) # 永久倒序排序\nsorted(list1, reverse = True) # 临时倒序排序\n```\n## 数值列表\nrange()\n```python\nfor value in range(1,5):\n\tprint(value)\n```\n将打印数字1 2 3 4. 左闭右开\n要创建数字列表，可使用函数list()将range()的结果直接转换为列表\n```python\nlist(range(1,5))\nprint(list) # [1, 2, 3, 4]\n```\n函数range()还可指定步长\n```python\neven_numbers = list(range(2,11,2))\nprint(even_numbers) # [2, 4, 6, 8, 10]\n```\n## 确定列表的长度\n```python\nlen(list)\n```\n# 字典\n## 遍历字典\n```python\nfavorite_languages = {\n\t'jen':'python',\n\t'sarah':'c',\n\t'edward':'ruby',\n\t'phil':'python',\n}\n# 遍历所有的键值对\nfor key, value in favorite_languages.items()\n# 遍历字典中所有的键\nfor key in favorite_languages.keys()\n# 遍历字典中所有的值\nfor value in favorite_languages.values()\n# 按顺序遍历\nfor key in sorted(favorite_languages.keys())\n# 去重遍历\nfor value in set(favorite_languages.values())\n```\n# 类\n## 类编码风格\n类名应采用驼峰命名法，即将类名中的每个单词的首字母都大写，而不使用下划线。实例名和模块名都采用小写格式，并在单词间加上下划线。\n对于每个类，都应紧跟在类定义后面包含一个文档字符串。这种文档字符串简要地描述类的功能，并遵循编写函数的文档字符串时采用的格式约定。每个模块也都应包含一个文档字符串，对其中的类可用于做什么进行描述。\n可使用空行来组织代码，但不要滥用。**在类中，可使用一个空行来分隔方法；而在模块中，可使用两个空行来分隔类。**\n需要同时导入标准库中的模块和你编写的模块时，先编写导入标准库模块的import语句，再添加一个空行，然后编写导入你自己编写模块的import语句。在包含多条import语句的程序中，这种做法让人更容易明白程序使用的各个模块都来自何方。\n## 创建和使用类\n### 创建类\n```python\nclass Dog():\n\tdef __init__(self, name, age):\n\t\t\"\"\"初始化属性name和age\"\"\"\n\t\tself.name = name\n\t\tself.age = age\n\n\tdef sit(self):\n\t\t\"\"\"模拟小狗被命令时蹲下\"\"\"\n\t\tprint(self.name.title() + \" is now sitting.\")\n\n\tdef roll_over(self):\n\t\t\"\"\"模拟小狗被命令时打滚\"\"\"\n\t\tprint(self.name.title() + \" rolled over!\")\n```\n- 根据约定，在python中，首字母大写的名称指的是类。\n- 方法__init__()\n\t- 类中的函数称为方法。\n\t- 方法__init__()是一个特殊的方法，每当你根据Dog类创建新实例时，python都会自动运行它。在这个方法的名称中，开头和末尾各有两个下划线，这是一种约定，旨在避免python默认方法与普通方法发生名称冲突。\n\t- 在这个方法的定义中，形参self必不可少，还必须位于其他形参的前面。因为python在调用这个__init__()方法来创建实例时，将自动传入形参self。每个与类相关联的方法调用都自动传递实参self，它是一个指向实例本身的引用，让实例能够访问类中的属性和方法。本例，我们创建Dog实例时，python将调用Dog类的方法__init__()。我们将通过实参向Dog()传递名字和年龄；self会自动传递，因此我们不需要传递它。每当我们根据Dog类创建实例时，都只需给最后两个形参(name和age)提供值。\n\t- 定义的两个变量都有前缀self。以self为前缀的变量都可供类中的所有方法使用，我们还可以通过类的任何实例来访问这些变量。self.name = name获取存储在形参name中的值，并将其存储到变量name中，然后该变量被关联到当前创建的实例。self.age = age的作用与此类似。像这样可通过实例访问的变量称为属性。\n- Dog类还定义了另外两个方法：sit()和roll_over()。由于这些方法不需要额外的信息，如名字和年龄，因此它们只有一个形参self。\n### 根据类创建实例\n```python\n# 创建实例 my_dog\nmy_dog = Dog('Heymi', 4)\n# 访问属性\nmy_dog.name\n# 调用方法\nmy_dog.sit()\n```\n## 使用类和实例\n### 给属性指定默认值\n类中的每个属性都必须有初始值，哪怕这个值是0或空字符串。在有些情况下，如设置默认值时，在方法__init__()内指定这种初始值是可行的；如果你对某个属性这样做了，就无需包含为它提供初始值的形参。\n```python\nclass Car():\n\tdef __init__(self, make, model, year):\n\t\t\"\"\"初始化描述汽车的属性\"\"\"\n\t\tself.make = make\n\t\tself.model = model\n\t\tself.year = year\n\t\tself.odometer_reading = 0 #python将创建一个名为odometer_reading的属性，并将其初始值设置为0\t\n```\n### 修改属性的值\n```python\nmy_new_car = Car(\"audi\", \"a5\", 2025)\n# 直接修改属性的值\nmy_new_car.odometer_reading = 23\n# 通过方法修改属性的值\nclass Car():\n\t--snip--\n\t\n\tdef update_odometer(self, mileage):\n\t\t\"\"\"将里程表读数设置为指定的值\"\"\"\n\t\tself.odometer_reading = milege\nmy_new_car.update_odometer(23)\n# 通过方法对属性的值进行递增\nclass Car():\n\t--snip--\n\t\n\tdef increment_odometer(self, miles):\n\t\t\"\"\"将里程表读数增加指定的量\"\"\"\n\t\tself.odometer_reading += miles\n```\n## 继承\n一个类继承另一个类时，它将自动获得另一个类的所有属性和方法；原有的类称为父类，而新类称为子类。子类继承了其父类的所有属性和方法，同时还可以定义自己的属性和方法。\n### 子类的方法__init__()\n```python\nclass ElectricCar(Car):\n\t\"\"\"电动汽车的独特之处\"\"\"\n\n\tdef __init__(self, make, model, year):\n\t\t\"\"\"初始化父类的属性\"\"\"\n\t\tsuper().__init__(make, model, year)\n```\n### python 2.7中的继承\n```python\nclass Car(object):\n\t--snip--\n\nclass ElectricCar(Car):\n\tdef __init__(self, make, model, year):\n\t\tsuper(ElectricCar, self).__init__(make, model, year) # 1\n```\n1 函数super()需要两个实参：子类名和对象self。\n在python 2.7中使用继承时，务必在定义父类时在括号内指定object\n\n### 给子类定义属性和方法\n```python\nclass Car():\n\t--snip--\n\nclass ElectricCar(Car):\n\tdef __init__(self, make, model, year):\n\t\tsuper().__init__(make, model, year)\n\t\tself.battery_size = 70 # 1\n```\n1 添加了新属性self.battery_size，并设置其初始值。根据ElectricCar类创建的所有实例都将包含这个属性，但所有Car实例都不包含它。\n### 重写父类的方法\n对于父类的方法，只要它不符合子类的行为，都可对其进行重写。为此，可在子类中定义一个与要重写的父类方法同名的方法。这样，python将不会考虑这个父类方法，而只关注你在子类中定义的方法。\n### 将实例用作属性\n使用代码模拟实物时，你可能会发现自己给类添加的细节越来越多：属性和方法清单以及文件都越来越长。在这种情况下，可能需要将类的一部分作为一个独立的类提取出来。\n例如，不断给ElectricCar类添加细节时，可能其中包含很多专门针对Battery的属性和方法，则可将这些属性和方法提取出来，放到一个名为Battery的类中，并将一个Battery实例用作ElectricCar类的一个属性：\n```python\nclass Car():\n\t--snip--\n\nclass Battery():\n\tdef __init__(self, battery_size=70): # 1\n\t\tself.battery_size = battery_size\n\nclass ElectricCar(Car):\n\tdef __init__(self, make, model, year):\n\t\tsuper().__init__(make, model, year)\n\t\tself.battery = Battery() # 2\n```\n1: \\_\\_init__()除self外，还有另一个形参battery_size。这个形参是可选的：如果没有给它提供值，电瓶容量将被设置为70。\n2: 在ElectricCar类中，我们添加了一个名为self.battery的属性。这行代码让python创建一个新的Battery实例（由于没有指定尺寸，因此为默认值70），并将该实例存储在属性self.battery中。每当方法__init__()被调用时，都将执行该操作；因此现在每个ElectricCar实例都包含一个自动创建的Battery实例。\n# 导入\n## 导入函数\n```python\nimport module_name # 导入整个模块\nmodule_name.function_name() # 使用模块中的函数需要使用句点\n\nfrom module_name import function_name # 导入特定函数，该函数后续使用时不需要句点\n\nimport pizza as p # 使用as给模块指定别名\n```\n\n## 导入类\n```python\nfrom car import Car, ElectricCar # 在模块文件car.py中导入Car类、ElectricCar类\nimport car # 导入整个car模块\nmy_beetle = car.Car('volkswagen', 'beetle', 2025) # 创建类实例代码都必须包含模块名，即需要使用句点访问\n```\n\n# 注释\nPython 中的注释有**单行注释**和**多行注释**。\n## 单行注释\n单行注释以 # 开头，例如：\n```python\n#这是一个注释\nprint(hello, world)\n```\n## 多行注释\n多行注释用三个单引号 ''' 或者三个双引号 \"\"\" 将注释括起来，例如\n### 单引号\n```python\n#!/usr/bin/python3 \n'''\n这是多行注释，用三个单引号\n这是多行注释，用三个单引号 \n这是多行注释，用三个单引号\n'''\nprint(\"Hello, World!\")\n```\n### 双引号\n```python\n#!/usr/bin/python3 \n\"\"\"\n这是多行注释（字符串），用三个双引号\n这是多行注释（字符串），用三个双引号 \n这是多行注释（字符串），用三个双引号\n\"\"\"\nprint(\"Hello, World!\")\n```\n## 拓展说明\n在 Python 中，多行注释是由三个单引号 ''' 或三个双引号 \"\"\" 来定义的，而且这种注释方式并不能嵌套使用。\n当你开始一个多行注释块时，Python 会一直将后续的行都当作注释，直到遇到另一组三个单引号或三个双引号。\n**嵌套多行注释会导致语法错误。**\n例如，下面的示例是不合法的：\n```python\n'''\n这是外部的多行注释\n可以包含一些描述性的内容\n\n    '''\n    这是尝试嵌套的多行注释\n    会导致语法错误\n    '''\n'''\n```\n在这个例子中，内部的三个单引号并没有被正确识别为多行注释的结束，而是被解释为普通的字符串。\n这将导致代码结构不正确，最终可能导致语法错误。\n如果你需要在注释中包含嵌套结构，推荐使用单行注释（以#开头）而不是多行注释。\n单行注释可以嵌套在多行注释中，而且不会引起语法错误。例如：\n```python\n'''\n这是外部的多行注释\n可以包含一些描述性的内容\n\n# 这是内部的单行注释\n# 可以嵌套在多行注释中\n'''\n```\n这样的结构是合法的，并且通常能够满足文档化和注释的需求。","source":"_posts/python/python基础.md","raw":"---\ntitle: python基础\ntags: \n- python\ncategories: \n- [python]\n---\n# 数据结构\n## 整型/浮点型/字符串\n```python\na = 3 #整型\na = 3.0 #浮点型\nname = \"martin\"\n```\n### 使用函数str()避免类型错误\n```python\nage = 23\nmessage = \"Happy\" + str(age) + \"rd Birthday!\"\nprint(message)\n```\n这里，调用函数str()，python将非字符串值表示为字符串\n## 列表\n关键词：list \\[]\n```python\nlist1 = [1, 2, 3, 4, 5]\nprint(list1)\nprint(list1[2]) # 3\nprint(list1[-1]) # 5\n```\nlist\\[-1]将访问列表最后一个值，-2为倒数第二个值，以此类推。\n\n```python\nlist2 = [1, \"art\", dict1] #同一个列表中支持多种数据类型\n```\n## 字典（哈希表）\n关键词： dict {key: value}\n```python\ndict1 = {\"name\": \"martin\", \"age\": 18, 20: 80}\nprint(dict1[\"name\"])\nprint(dict1[\"age\"])\nprint(dict1[20])\n```\n\n# 运算\n`//` 地板除\n`/` 普通除\n`**` 乘方\n```python\nprint(9/2) # 4.5\nprint(9//2) # 4\nprint(9**2) # 81\n```\n\n# 函数\n带初值的函数示例\n```python\ndef func1(a, b = 2) #如果传入b，返回a**b；否则，返回a**2\n\treturn a ** b\n\nprint(func1(2)) # 4\n```\n\n# 列表\n## 切片\n```python\nlist1 = [1, 2, 3, 4, 5]\nprint(list1[1:4]) # [2, 3, 4] 左闭右开\nprint(list1[1:-1]) # [2, 3, 4]\nprint(list1[:]) # [1, 2, 3, 4, 5]\nprint(list1[1:]) # [2, 3, 4, 5]\n```\n## 操作\n### 添加元素\n```python\nlist1.append(6) # [1, 2, 3, 4, 5, 6] 在末尾添加\nlist1.insert(0, 0.5) #[0.5, 1, 2, 3, 4, 5] 在指定位置添加\nlist1.extend([8, 9]) #在列表末尾一次性追加另一个列表的多个值，即用新列表扩展原来的列表\n```\n### 删除元素\n```python\ndel list1[0] #删除索引0处的元素，且不再以任何方式使用它\nlist1.pop(index) #删除索引index处的元素，但会继续使用它，index为空时默认为-1，即栈顶\nlist1.remove(key) #删除列表中值为key的元素，也可继续使用它的值；若列表中有多个值为key，remove只删除第一个指定的值\n```\n### 列表排序\n```python\nlist1.sort() # 永久顺序排序\nlist1.sort(reverse = True) # 永久倒序排序\nsorted(list1, reverse = True) # 临时倒序排序\n```\n## 数值列表\nrange()\n```python\nfor value in range(1,5):\n\tprint(value)\n```\n将打印数字1 2 3 4. 左闭右开\n要创建数字列表，可使用函数list()将range()的结果直接转换为列表\n```python\nlist(range(1,5))\nprint(list) # [1, 2, 3, 4]\n```\n函数range()还可指定步长\n```python\neven_numbers = list(range(2,11,2))\nprint(even_numbers) # [2, 4, 6, 8, 10]\n```\n## 确定列表的长度\n```python\nlen(list)\n```\n# 字典\n## 遍历字典\n```python\nfavorite_languages = {\n\t'jen':'python',\n\t'sarah':'c',\n\t'edward':'ruby',\n\t'phil':'python',\n}\n# 遍历所有的键值对\nfor key, value in favorite_languages.items()\n# 遍历字典中所有的键\nfor key in favorite_languages.keys()\n# 遍历字典中所有的值\nfor value in favorite_languages.values()\n# 按顺序遍历\nfor key in sorted(favorite_languages.keys())\n# 去重遍历\nfor value in set(favorite_languages.values())\n```\n# 类\n## 类编码风格\n类名应采用驼峰命名法，即将类名中的每个单词的首字母都大写，而不使用下划线。实例名和模块名都采用小写格式，并在单词间加上下划线。\n对于每个类，都应紧跟在类定义后面包含一个文档字符串。这种文档字符串简要地描述类的功能，并遵循编写函数的文档字符串时采用的格式约定。每个模块也都应包含一个文档字符串，对其中的类可用于做什么进行描述。\n可使用空行来组织代码，但不要滥用。**在类中，可使用一个空行来分隔方法；而在模块中，可使用两个空行来分隔类。**\n需要同时导入标准库中的模块和你编写的模块时，先编写导入标准库模块的import语句，再添加一个空行，然后编写导入你自己编写模块的import语句。在包含多条import语句的程序中，这种做法让人更容易明白程序使用的各个模块都来自何方。\n## 创建和使用类\n### 创建类\n```python\nclass Dog():\n\tdef __init__(self, name, age):\n\t\t\"\"\"初始化属性name和age\"\"\"\n\t\tself.name = name\n\t\tself.age = age\n\n\tdef sit(self):\n\t\t\"\"\"模拟小狗被命令时蹲下\"\"\"\n\t\tprint(self.name.title() + \" is now sitting.\")\n\n\tdef roll_over(self):\n\t\t\"\"\"模拟小狗被命令时打滚\"\"\"\n\t\tprint(self.name.title() + \" rolled over!\")\n```\n- 根据约定，在python中，首字母大写的名称指的是类。\n- 方法__init__()\n\t- 类中的函数称为方法。\n\t- 方法__init__()是一个特殊的方法，每当你根据Dog类创建新实例时，python都会自动运行它。在这个方法的名称中，开头和末尾各有两个下划线，这是一种约定，旨在避免python默认方法与普通方法发生名称冲突。\n\t- 在这个方法的定义中，形参self必不可少，还必须位于其他形参的前面。因为python在调用这个__init__()方法来创建实例时，将自动传入形参self。每个与类相关联的方法调用都自动传递实参self，它是一个指向实例本身的引用，让实例能够访问类中的属性和方法。本例，我们创建Dog实例时，python将调用Dog类的方法__init__()。我们将通过实参向Dog()传递名字和年龄；self会自动传递，因此我们不需要传递它。每当我们根据Dog类创建实例时，都只需给最后两个形参(name和age)提供值。\n\t- 定义的两个变量都有前缀self。以self为前缀的变量都可供类中的所有方法使用，我们还可以通过类的任何实例来访问这些变量。self.name = name获取存储在形参name中的值，并将其存储到变量name中，然后该变量被关联到当前创建的实例。self.age = age的作用与此类似。像这样可通过实例访问的变量称为属性。\n- Dog类还定义了另外两个方法：sit()和roll_over()。由于这些方法不需要额外的信息，如名字和年龄，因此它们只有一个形参self。\n### 根据类创建实例\n```python\n# 创建实例 my_dog\nmy_dog = Dog('Heymi', 4)\n# 访问属性\nmy_dog.name\n# 调用方法\nmy_dog.sit()\n```\n## 使用类和实例\n### 给属性指定默认值\n类中的每个属性都必须有初始值，哪怕这个值是0或空字符串。在有些情况下，如设置默认值时，在方法__init__()内指定这种初始值是可行的；如果你对某个属性这样做了，就无需包含为它提供初始值的形参。\n```python\nclass Car():\n\tdef __init__(self, make, model, year):\n\t\t\"\"\"初始化描述汽车的属性\"\"\"\n\t\tself.make = make\n\t\tself.model = model\n\t\tself.year = year\n\t\tself.odometer_reading = 0 #python将创建一个名为odometer_reading的属性，并将其初始值设置为0\t\n```\n### 修改属性的值\n```python\nmy_new_car = Car(\"audi\", \"a5\", 2025)\n# 直接修改属性的值\nmy_new_car.odometer_reading = 23\n# 通过方法修改属性的值\nclass Car():\n\t--snip--\n\t\n\tdef update_odometer(self, mileage):\n\t\t\"\"\"将里程表读数设置为指定的值\"\"\"\n\t\tself.odometer_reading = milege\nmy_new_car.update_odometer(23)\n# 通过方法对属性的值进行递增\nclass Car():\n\t--snip--\n\t\n\tdef increment_odometer(self, miles):\n\t\t\"\"\"将里程表读数增加指定的量\"\"\"\n\t\tself.odometer_reading += miles\n```\n## 继承\n一个类继承另一个类时，它将自动获得另一个类的所有属性和方法；原有的类称为父类，而新类称为子类。子类继承了其父类的所有属性和方法，同时还可以定义自己的属性和方法。\n### 子类的方法__init__()\n```python\nclass ElectricCar(Car):\n\t\"\"\"电动汽车的独特之处\"\"\"\n\n\tdef __init__(self, make, model, year):\n\t\t\"\"\"初始化父类的属性\"\"\"\n\t\tsuper().__init__(make, model, year)\n```\n### python 2.7中的继承\n```python\nclass Car(object):\n\t--snip--\n\nclass ElectricCar(Car):\n\tdef __init__(self, make, model, year):\n\t\tsuper(ElectricCar, self).__init__(make, model, year) # 1\n```\n1 函数super()需要两个实参：子类名和对象self。\n在python 2.7中使用继承时，务必在定义父类时在括号内指定object\n\n### 给子类定义属性和方法\n```python\nclass Car():\n\t--snip--\n\nclass ElectricCar(Car):\n\tdef __init__(self, make, model, year):\n\t\tsuper().__init__(make, model, year)\n\t\tself.battery_size = 70 # 1\n```\n1 添加了新属性self.battery_size，并设置其初始值。根据ElectricCar类创建的所有实例都将包含这个属性，但所有Car实例都不包含它。\n### 重写父类的方法\n对于父类的方法，只要它不符合子类的行为，都可对其进行重写。为此，可在子类中定义一个与要重写的父类方法同名的方法。这样，python将不会考虑这个父类方法，而只关注你在子类中定义的方法。\n### 将实例用作属性\n使用代码模拟实物时，你可能会发现自己给类添加的细节越来越多：属性和方法清单以及文件都越来越长。在这种情况下，可能需要将类的一部分作为一个独立的类提取出来。\n例如，不断给ElectricCar类添加细节时，可能其中包含很多专门针对Battery的属性和方法，则可将这些属性和方法提取出来，放到一个名为Battery的类中，并将一个Battery实例用作ElectricCar类的一个属性：\n```python\nclass Car():\n\t--snip--\n\nclass Battery():\n\tdef __init__(self, battery_size=70): # 1\n\t\tself.battery_size = battery_size\n\nclass ElectricCar(Car):\n\tdef __init__(self, make, model, year):\n\t\tsuper().__init__(make, model, year)\n\t\tself.battery = Battery() # 2\n```\n1: \\_\\_init__()除self外，还有另一个形参battery_size。这个形参是可选的：如果没有给它提供值，电瓶容量将被设置为70。\n2: 在ElectricCar类中，我们添加了一个名为self.battery的属性。这行代码让python创建一个新的Battery实例（由于没有指定尺寸，因此为默认值70），并将该实例存储在属性self.battery中。每当方法__init__()被调用时，都将执行该操作；因此现在每个ElectricCar实例都包含一个自动创建的Battery实例。\n# 导入\n## 导入函数\n```python\nimport module_name # 导入整个模块\nmodule_name.function_name() # 使用模块中的函数需要使用句点\n\nfrom module_name import function_name # 导入特定函数，该函数后续使用时不需要句点\n\nimport pizza as p # 使用as给模块指定别名\n```\n\n## 导入类\n```python\nfrom car import Car, ElectricCar # 在模块文件car.py中导入Car类、ElectricCar类\nimport car # 导入整个car模块\nmy_beetle = car.Car('volkswagen', 'beetle', 2025) # 创建类实例代码都必须包含模块名，即需要使用句点访问\n```\n\n# 注释\nPython 中的注释有**单行注释**和**多行注释**。\n## 单行注释\n单行注释以 # 开头，例如：\n```python\n#这是一个注释\nprint(hello, world)\n```\n## 多行注释\n多行注释用三个单引号 ''' 或者三个双引号 \"\"\" 将注释括起来，例如\n### 单引号\n```python\n#!/usr/bin/python3 \n'''\n这是多行注释，用三个单引号\n这是多行注释，用三个单引号 \n这是多行注释，用三个单引号\n'''\nprint(\"Hello, World!\")\n```\n### 双引号\n```python\n#!/usr/bin/python3 \n\"\"\"\n这是多行注释（字符串），用三个双引号\n这是多行注释（字符串），用三个双引号 \n这是多行注释（字符串），用三个双引号\n\"\"\"\nprint(\"Hello, World!\")\n```\n## 拓展说明\n在 Python 中，多行注释是由三个单引号 ''' 或三个双引号 \"\"\" 来定义的，而且这种注释方式并不能嵌套使用。\n当你开始一个多行注释块时，Python 会一直将后续的行都当作注释，直到遇到另一组三个单引号或三个双引号。\n**嵌套多行注释会导致语法错误。**\n例如，下面的示例是不合法的：\n```python\n'''\n这是外部的多行注释\n可以包含一些描述性的内容\n\n    '''\n    这是尝试嵌套的多行注释\n    会导致语法错误\n    '''\n'''\n```\n在这个例子中，内部的三个单引号并没有被正确识别为多行注释的结束，而是被解释为普通的字符串。\n这将导致代码结构不正确，最终可能导致语法错误。\n如果你需要在注释中包含嵌套结构，推荐使用单行注释（以#开头）而不是多行注释。\n单行注释可以嵌套在多行注释中，而且不会引起语法错误。例如：\n```python\n'''\n这是外部的多行注释\n可以包含一些描述性的内容\n\n# 这是内部的单行注释\n# 可以嵌套在多行注释中\n'''\n```\n这样的结构是合法的，并且通常能够满足文档化和注释的需求。","slug":"python/python基础","published":1,"date":"2025-03-17T08:14:17.481Z","updated":"2025-03-20T04:39:29.411Z","comments":1,"layout":"post","photos":[],"_id":"cm8h01uzh0001ekeq1hri3nza","content":"<h1 id=\"数据结构\"><a href=\"#数据结构\" class=\"headerlink\" title=\"数据结构\"></a>数据结构</h1><h2 id=\"整型-浮点型-字符串\"><a href=\"#整型-浮点型-字符串\" class=\"headerlink\" title=\"整型&#x2F;浮点型&#x2F;字符串\"></a>整型&#x2F;浮点型&#x2F;字符串</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = <span class=\"number\">3</span> <span class=\"comment\">#整型</span></span><br><span class=\"line\">a = <span class=\"number\">3.0</span> <span class=\"comment\">#浮点型</span></span><br><span class=\"line\">name = <span class=\"string\">&quot;martin&quot;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"使用函数str-避免类型错误\"><a href=\"#使用函数str-避免类型错误\" class=\"headerlink\" title=\"使用函数str()避免类型错误\"></a>使用函数str()避免类型错误</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">age = <span class=\"number\">23</span></span><br><span class=\"line\">message = <span class=\"string\">&quot;Happy&quot;</span> + <span class=\"built_in\">str</span>(age) + <span class=\"string\">&quot;rd Birthday!&quot;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(message)</span><br></pre></td></tr></table></figure>\n<p>这里，调用函数str()，python将非字符串值表示为字符串</p>\n<h2 id=\"列表\"><a href=\"#列表\" class=\"headerlink\" title=\"列表\"></a>列表</h2><p>关键词：list []</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list1 = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1[<span class=\"number\">2</span>]) <span class=\"comment\"># 3</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1[-<span class=\"number\">1</span>]) <span class=\"comment\"># 5</span></span><br></pre></td></tr></table></figure>\n<p>list[-1]将访问列表最后一个值，-2为倒数第二个值，以此类推。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list2 = [<span class=\"number\">1</span>, <span class=\"string\">&quot;art&quot;</span>, dict1] <span class=\"comment\">#同一个列表中支持多种数据类型</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"字典（哈希表）\"><a href=\"#字典（哈希表）\" class=\"headerlink\" title=\"字典（哈希表）\"></a>字典（哈希表）</h2><p>关键词： dict {key: value}</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dict1 = &#123;<span class=\"string\">&quot;name&quot;</span>: <span class=\"string\">&quot;martin&quot;</span>, <span class=\"string\">&quot;age&quot;</span>: <span class=\"number\">18</span>, <span class=\"number\">20</span>: <span class=\"number\">80</span>&#125;</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dict1[<span class=\"string\">&quot;name&quot;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dict1[<span class=\"string\">&quot;age&quot;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dict1[<span class=\"number\">20</span>])</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"运算\"><a href=\"#运算\" class=\"headerlink\" title=\"运算\"></a>运算</h1><p><code>//</code> 地板除<br><code>/</code> 普通除<br><code>**</code> 乘方</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"number\">9</span>/<span class=\"number\">2</span>) <span class=\"comment\"># 4.5</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"number\">9</span>//<span class=\"number\">2</span>) <span class=\"comment\"># 4</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"number\">9</span>**<span class=\"number\">2</span>) <span class=\"comment\"># 81</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h1><p>带初值的函数示例</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">func1</span>(<span class=\"params\">a, b = <span class=\"number\">2</span></span>) <span class=\"comment\">#如果传入b，返回a**b；否则，返回a**2</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> a ** b</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(func1(<span class=\"number\">2</span>)) <span class=\"comment\"># 4</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"列表-1\"><a href=\"#列表-1\" class=\"headerlink\" title=\"列表\"></a>列表</h1><h2 id=\"切片\"><a href=\"#切片\" class=\"headerlink\" title=\"切片\"></a>切片</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list1 = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1[<span class=\"number\">1</span>:<span class=\"number\">4</span>]) <span class=\"comment\"># [2, 3, 4] 左闭右开</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1[<span class=\"number\">1</span>:-<span class=\"number\">1</span>]) <span class=\"comment\"># [2, 3, 4]</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1[:]) <span class=\"comment\"># [1, 2, 3, 4, 5]</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1[<span class=\"number\">1</span>:]) <span class=\"comment\"># [2, 3, 4, 5]</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"操作\"><a href=\"#操作\" class=\"headerlink\" title=\"操作\"></a>操作</h2><h3 id=\"添加元素\"><a href=\"#添加元素\" class=\"headerlink\" title=\"添加元素\"></a>添加元素</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list1.append(<span class=\"number\">6</span>) <span class=\"comment\"># [1, 2, 3, 4, 5, 6] 在末尾添加</span></span><br><span class=\"line\">list1.insert(<span class=\"number\">0</span>, <span class=\"number\">0.5</span>) <span class=\"comment\">#[0.5, 1, 2, 3, 4, 5] 在指定位置添加</span></span><br><span class=\"line\">list1.extend([<span class=\"number\">8</span>, <span class=\"number\">9</span>]) <span class=\"comment\">#在列表末尾一次性追加另一个列表的多个值，即用新列表扩展原来的列表</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"删除元素\"><a href=\"#删除元素\" class=\"headerlink\" title=\"删除元素\"></a>删除元素</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">del</span> list1[<span class=\"number\">0</span>] <span class=\"comment\">#删除索引0处的元素，且不再以任何方式使用它</span></span><br><span class=\"line\">list1.pop(index) <span class=\"comment\">#删除索引index处的元素，但会继续使用它，index为空时默认为-1，即栈顶</span></span><br><span class=\"line\">list1.remove(key) <span class=\"comment\">#删除列表中值为key的元素，也可继续使用它的值；若列表中有多个值为key，remove只删除第一个指定的值</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"列表排序\"><a href=\"#列表排序\" class=\"headerlink\" title=\"列表排序\"></a>列表排序</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list1.sort() <span class=\"comment\"># 永久顺序排序</span></span><br><span class=\"line\">list1.sort(reverse = <span class=\"literal\">True</span>) <span class=\"comment\"># 永久倒序排序</span></span><br><span class=\"line\"><span class=\"built_in\">sorted</span>(list1, reverse = <span class=\"literal\">True</span>) <span class=\"comment\"># 临时倒序排序</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"数值列表\"><a href=\"#数值列表\" class=\"headerlink\" title=\"数值列表\"></a>数值列表</h2><p>range()</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> value <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"number\">5</span>):</span><br><span class=\"line\">\t<span class=\"built_in\">print</span>(value)</span><br></pre></td></tr></table></figure>\n<p>将打印数字1 2 3 4. 左闭右开<br>要创建数字列表，可使用函数list()将range()的结果直接转换为列表</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">list</span>) <span class=\"comment\"># [1, 2, 3, 4]</span></span><br></pre></td></tr></table></figure>\n<p>函数range()还可指定步长</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">even_numbers = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">2</span>,<span class=\"number\">11</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(even_numbers) <span class=\"comment\"># [2, 4, 6, 8, 10]</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"确定列表的长度\"><a href=\"#确定列表的长度\" class=\"headerlink\" title=\"确定列表的长度\"></a>确定列表的长度</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">len</span>(<span class=\"built_in\">list</span>)</span><br></pre></td></tr></table></figure>\n<h1 id=\"字典\"><a href=\"#字典\" class=\"headerlink\" title=\"字典\"></a>字典</h1><h2 id=\"遍历字典\"><a href=\"#遍历字典\" class=\"headerlink\" title=\"遍历字典\"></a>遍历字典</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">favorite_languages = &#123;</span><br><span class=\"line\">\t<span class=\"string\">&#x27;jen&#x27;</span>:<span class=\"string\">&#x27;python&#x27;</span>,</span><br><span class=\"line\">\t<span class=\"string\">&#x27;sarah&#x27;</span>:<span class=\"string\">&#x27;c&#x27;</span>,</span><br><span class=\"line\">\t<span class=\"string\">&#x27;edward&#x27;</span>:<span class=\"string\">&#x27;ruby&#x27;</span>,</span><br><span class=\"line\">\t<span class=\"string\">&#x27;phil&#x27;</span>:<span class=\"string\">&#x27;python&#x27;</span>,</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\"># 遍历所有的键值对</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key, value <span class=\"keyword\">in</span> favorite_languages.items()</span><br><span class=\"line\"><span class=\"comment\"># 遍历字典中所有的键</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> favorite_languages.keys()</span><br><span class=\"line\"><span class=\"comment\"># 遍历字典中所有的值</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> value <span class=\"keyword\">in</span> favorite_languages.values()</span><br><span class=\"line\"><span class=\"comment\"># 按顺序遍历</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> <span class=\"built_in\">sorted</span>(favorite_languages.keys())</span><br><span class=\"line\"><span class=\"comment\"># 去重遍历</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> value <span class=\"keyword\">in</span> <span class=\"built_in\">set</span>(favorite_languages.values())</span><br></pre></td></tr></table></figure>\n<h1 id=\"类\"><a href=\"#类\" class=\"headerlink\" title=\"类\"></a>类</h1><h2 id=\"类编码风格\"><a href=\"#类编码风格\" class=\"headerlink\" title=\"类编码风格\"></a>类编码风格</h2><p>类名应采用驼峰命名法，即将类名中的每个单词的首字母都大写，而不使用下划线。实例名和模块名都采用小写格式，并在单词间加上下划线。<br>对于每个类，都应紧跟在类定义后面包含一个文档字符串。这种文档字符串简要地描述类的功能，并遵循编写函数的文档字符串时采用的格式约定。每个模块也都应包含一个文档字符串，对其中的类可用于做什么进行描述。<br>可使用空行来组织代码，但不要滥用。<strong>在类中，可使用一个空行来分隔方法；而在模块中，可使用两个空行来分隔类。</strong><br>需要同时导入标准库中的模块和你编写的模块时，先编写导入标准库模块的import语句，再添加一个空行，然后编写导入你自己编写模块的import语句。在包含多条import语句的程序中，这种做法让人更容易明白程序使用的各个模块都来自何方。</p>\n<h2 id=\"创建和使用类\"><a href=\"#创建和使用类\" class=\"headerlink\" title=\"创建和使用类\"></a>创建和使用类</h2><h3 id=\"创建类\"><a href=\"#创建类\" class=\"headerlink\" title=\"创建类\"></a>创建类</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Dog</span>():</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, name, age</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;初始化属性name和age&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.name = name</span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.age = age</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">sit</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;模拟小狗被命令时蹲下&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"built_in\">print</span>(<span class=\"variable language_\">self</span>.name.title() + <span class=\"string\">&quot; is now sitting.&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">roll_over</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;模拟小狗被命令时打滚&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"built_in\">print</span>(<span class=\"variable language_\">self</span>.name.title() + <span class=\"string\">&quot; rolled over!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>根据约定，在python中，首字母大写的名称指的是类。</li>\n<li>方法__init__()<ul>\n<li>类中的函数称为方法。</li>\n<li>方法__init__()是一个特殊的方法，每当你根据Dog类创建新实例时，python都会自动运行它。在这个方法的名称中，开头和末尾各有两个下划线，这是一种约定，旨在避免python默认方法与普通方法发生名称冲突。</li>\n<li>在这个方法的定义中，形参self必不可少，还必须位于其他形参的前面。因为python在调用这个__init__()方法来创建实例时，将自动传入形参self。每个与类相关联的方法调用都自动传递实参self，它是一个指向实例本身的引用，让实例能够访问类中的属性和方法。本例，我们创建Dog实例时，python将调用Dog类的方法__init__()。我们将通过实参向Dog()传递名字和年龄；self会自动传递，因此我们不需要传递它。每当我们根据Dog类创建实例时，都只需给最后两个形参(name和age)提供值。</li>\n<li>定义的两个变量都有前缀self。以self为前缀的变量都可供类中的所有方法使用，我们还可以通过类的任何实例来访问这些变量。self.name &#x3D; name获取存储在形参name中的值，并将其存储到变量name中，然后该变量被关联到当前创建的实例。self.age &#x3D; age的作用与此类似。像这样可通过实例访问的变量称为属性。</li>\n</ul>\n</li>\n<li>Dog类还定义了另外两个方法：sit()和roll_over()。由于这些方法不需要额外的信息，如名字和年龄，因此它们只有一个形参self。</li>\n</ul>\n<h3 id=\"根据类创建实例\"><a href=\"#根据类创建实例\" class=\"headerlink\" title=\"根据类创建实例\"></a>根据类创建实例</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 创建实例 my_dog</span></span><br><span class=\"line\">my_dog = Dog(<span class=\"string\">&#x27;Heymi&#x27;</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"comment\"># 访问属性</span></span><br><span class=\"line\">my_dog.name</span><br><span class=\"line\"><span class=\"comment\"># 调用方法</span></span><br><span class=\"line\">my_dog.sit()</span><br></pre></td></tr></table></figure>\n<h2 id=\"使用类和实例\"><a href=\"#使用类和实例\" class=\"headerlink\" title=\"使用类和实例\"></a>使用类和实例</h2><h3 id=\"给属性指定默认值\"><a href=\"#给属性指定默认值\" class=\"headerlink\" title=\"给属性指定默认值\"></a>给属性指定默认值</h3><p>类中的每个属性都必须有初始值，哪怕这个值是0或空字符串。在有些情况下，如设置默认值时，在方法__init__()内指定这种初始值是可行的；如果你对某个属性这样做了，就无需包含为它提供初始值的形参。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Car</span>():</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, make, model, year</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;初始化描述汽车的属性&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.make = make</span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.model = model</span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.year = year</span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.odometer_reading = <span class=\"number\">0</span> <span class=\"comment\">#python将创建一个名为odometer_reading的属性，并将其初始值设置为0\t</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"修改属性的值\"><a href=\"#修改属性的值\" class=\"headerlink\" title=\"修改属性的值\"></a>修改属性的值</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">my_new_car = Car(<span class=\"string\">&quot;audi&quot;</span>, <span class=\"string\">&quot;a5&quot;</span>, <span class=\"number\">2025</span>)</span><br><span class=\"line\"><span class=\"comment\"># 直接修改属性的值</span></span><br><span class=\"line\">my_new_car.odometer_reading = <span class=\"number\">23</span></span><br><span class=\"line\"><span class=\"comment\"># 通过方法修改属性的值</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Car</span>():</span><br><span class=\"line\">\t--snip--</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">update_odometer</span>(<span class=\"params\">self, mileage</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;将里程表读数设置为指定的值&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.odometer_reading = milege</span><br><span class=\"line\">my_new_car.update_odometer(<span class=\"number\">23</span>)</span><br><span class=\"line\"><span class=\"comment\"># 通过方法对属性的值进行递增</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Car</span>():</span><br><span class=\"line\">\t--snip--</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">increment_odometer</span>(<span class=\"params\">self, miles</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;将里程表读数增加指定的量&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.odometer_reading += miles</span><br></pre></td></tr></table></figure>\n<h2 id=\"继承\"><a href=\"#继承\" class=\"headerlink\" title=\"继承\"></a>继承</h2><p>一个类继承另一个类时，它将自动获得另一个类的所有属性和方法；原有的类称为父类，而新类称为子类。子类继承了其父类的所有属性和方法，同时还可以定义自己的属性和方法。</p>\n<h3 id=\"子类的方法-init\"><a href=\"#子类的方法-init\" class=\"headerlink\" title=\"子类的方法__init__()\"></a>子类的方法__init__()</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ElectricCar</span>(<span class=\"title class_ inherited__\">Car</span>):</span><br><span class=\"line\">\t<span class=\"string\">&quot;&quot;&quot;电动汽车的独特之处&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, make, model, year</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;初始化父类的属性&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"built_in\">super</span>().__init__(make, model, year)</span><br></pre></td></tr></table></figure>\n<h3 id=\"python-2-7中的继承\"><a href=\"#python-2-7中的继承\" class=\"headerlink\" title=\"python 2.7中的继承\"></a>python 2.7中的继承</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Car</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">\t--snip--</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ElectricCar</span>(<span class=\"title class_ inherited__\">Car</span>):</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, make, model, year</span>):</span><br><span class=\"line\">\t\t<span class=\"built_in\">super</span>(ElectricCar, <span class=\"variable language_\">self</span>).__init__(make, model, year) <span class=\"comment\"># 1</span></span><br></pre></td></tr></table></figure>\n<p>1 函数super()需要两个实参：子类名和对象self。<br>在python 2.7中使用继承时，务必在定义父类时在括号内指定object</p>\n<h3 id=\"给子类定义属性和方法\"><a href=\"#给子类定义属性和方法\" class=\"headerlink\" title=\"给子类定义属性和方法\"></a>给子类定义属性和方法</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Car</span>():</span><br><span class=\"line\">\t--snip--</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ElectricCar</span>(<span class=\"title class_ inherited__\">Car</span>):</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, make, model, year</span>):</span><br><span class=\"line\">\t\t<span class=\"built_in\">super</span>().__init__(make, model, year)</span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.battery_size = <span class=\"number\">70</span> <span class=\"comment\"># 1</span></span><br></pre></td></tr></table></figure>\n<p>1 添加了新属性self.battery_size，并设置其初始值。根据ElectricCar类创建的所有实例都将包含这个属性，但所有Car实例都不包含它。</p>\n<h3 id=\"重写父类的方法\"><a href=\"#重写父类的方法\" class=\"headerlink\" title=\"重写父类的方法\"></a>重写父类的方法</h3><p>对于父类的方法，只要它不符合子类的行为，都可对其进行重写。为此，可在子类中定义一个与要重写的父类方法同名的方法。这样，python将不会考虑这个父类方法，而只关注你在子类中定义的方法。</p>\n<h3 id=\"将实例用作属性\"><a href=\"#将实例用作属性\" class=\"headerlink\" title=\"将实例用作属性\"></a>将实例用作属性</h3><p>使用代码模拟实物时，你可能会发现自己给类添加的细节越来越多：属性和方法清单以及文件都越来越长。在这种情况下，可能需要将类的一部分作为一个独立的类提取出来。<br>例如，不断给ElectricCar类添加细节时，可能其中包含很多专门针对Battery的属性和方法，则可将这些属性和方法提取出来，放到一个名为Battery的类中，并将一个Battery实例用作ElectricCar类的一个属性：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Car</span>():</span><br><span class=\"line\">\t--snip--</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Battery</span>():</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, battery_size=<span class=\"number\">70</span></span>): <span class=\"comment\"># 1</span></span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.battery_size = battery_size</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ElectricCar</span>(<span class=\"title class_ inherited__\">Car</span>):</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, make, model, year</span>):</span><br><span class=\"line\">\t\t<span class=\"built_in\">super</span>().__init__(make, model, year)</span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.battery = Battery() <span class=\"comment\"># 2</span></span><br></pre></td></tr></table></figure>\n<p>1: __init__()除self外，还有另一个形参battery_size。这个形参是可选的：如果没有给它提供值，电瓶容量将被设置为70。<br>2: 在ElectricCar类中，我们添加了一个名为self.battery的属性。这行代码让python创建一个新的Battery实例（由于没有指定尺寸，因此为默认值70），并将该实例存储在属性self.battery中。每当方法__init__()被调用时，都将执行该操作；因此现在每个ElectricCar实例都包含一个自动创建的Battery实例。</p>\n<h1 id=\"导入\"><a href=\"#导入\" class=\"headerlink\" title=\"导入\"></a>导入</h1><h2 id=\"导入函数\"><a href=\"#导入函数\" class=\"headerlink\" title=\"导入函数\"></a>导入函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> module_name <span class=\"comment\"># 导入整个模块</span></span><br><span class=\"line\">module_name.function_name() <span class=\"comment\"># 使用模块中的函数需要使用句点</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> module_name <span class=\"keyword\">import</span> function_name <span class=\"comment\"># 导入特定函数，该函数后续使用时不需要句点</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> pizza <span class=\"keyword\">as</span> p <span class=\"comment\"># 使用as给模块指定别名</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"导入类\"><a href=\"#导入类\" class=\"headerlink\" title=\"导入类\"></a>导入类</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> car <span class=\"keyword\">import</span> Car, ElectricCar <span class=\"comment\"># 在模块文件car.py中导入Car类、ElectricCar类</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> car <span class=\"comment\"># 导入整个car模块</span></span><br><span class=\"line\">my_beetle = car.Car(<span class=\"string\">&#x27;volkswagen&#x27;</span>, <span class=\"string\">&#x27;beetle&#x27;</span>, <span class=\"number\">2025</span>) <span class=\"comment\"># 创建类实例代码都必须包含模块名，即需要使用句点访问</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"注释\"><a href=\"#注释\" class=\"headerlink\" title=\"注释\"></a>注释</h1><p>Python 中的注释有<strong>单行注释</strong>和<strong>多行注释</strong>。</p>\n<h2 id=\"单行注释\"><a href=\"#单行注释\" class=\"headerlink\" title=\"单行注释\"></a>单行注释</h2><p>单行注释以 # 开头，例如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#这是一个注释</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(hello, world)</span><br></pre></td></tr></table></figure>\n<h2 id=\"多行注释\"><a href=\"#多行注释\" class=\"headerlink\" title=\"多行注释\"></a>多行注释</h2><p>多行注释用三个单引号 ‘’’ 或者三个双引号 “”” 将注释括起来，例如</p>\n<h3 id=\"单引号\"><a href=\"#单引号\" class=\"headerlink\" title=\"单引号\"></a>单引号</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/usr/bin/python3 </span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">这是多行注释，用三个单引号</span></span><br><span class=\"line\"><span class=\"string\">这是多行注释，用三个单引号 </span></span><br><span class=\"line\"><span class=\"string\">这是多行注释，用三个单引号</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Hello, World!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"双引号\"><a href=\"#双引号\" class=\"headerlink\" title=\"双引号\"></a>双引号</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/usr/bin/python3 </span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">这是多行注释（字符串），用三个双引号</span></span><br><span class=\"line\"><span class=\"string\">这是多行注释（字符串），用三个双引号 </span></span><br><span class=\"line\"><span class=\"string\">这是多行注释（字符串），用三个双引号</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Hello, World!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"拓展说明\"><a href=\"#拓展说明\" class=\"headerlink\" title=\"拓展说明\"></a>拓展说明</h2><p>在 Python 中，多行注释是由三个单引号 ‘’’ 或三个双引号 “”” 来定义的，而且这种注释方式并不能嵌套使用。<br>当你开始一个多行注释块时，Python 会一直将后续的行都当作注释，直到遇到另一组三个单引号或三个双引号。<br><strong>嵌套多行注释会导致语法错误。</strong><br>例如，下面的示例是不合法的：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">这是外部的多行注释</span></span><br><span class=\"line\"><span class=\"string\">可以包含一些描述性的内容</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    这是尝试嵌套的多行注释</span><br><span class=\"line\">    会导致语法错误</span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>在这个例子中，内部的三个单引号并没有被正确识别为多行注释的结束，而是被解释为普通的字符串。<br>这将导致代码结构不正确，最终可能导致语法错误。<br>如果你需要在注释中包含嵌套结构，推荐使用单行注释（以#开头）而不是多行注释。<br>单行注释可以嵌套在多行注释中，而且不会引起语法错误。例如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">这是外部的多行注释</span></span><br><span class=\"line\"><span class=\"string\">可以包含一些描述性的内容</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"># 这是内部的单行注释</span></span><br><span class=\"line\"><span class=\"string\"># 可以嵌套在多行注释中</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>这样的结构是合法的，并且通常能够满足文档化和注释的需求。</p>\n","excerpt":"","more":"<h1 id=\"数据结构\"><a href=\"#数据结构\" class=\"headerlink\" title=\"数据结构\"></a>数据结构</h1><h2 id=\"整型-浮点型-字符串\"><a href=\"#整型-浮点型-字符串\" class=\"headerlink\" title=\"整型&#x2F;浮点型&#x2F;字符串\"></a>整型&#x2F;浮点型&#x2F;字符串</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">a = <span class=\"number\">3</span> <span class=\"comment\">#整型</span></span><br><span class=\"line\">a = <span class=\"number\">3.0</span> <span class=\"comment\">#浮点型</span></span><br><span class=\"line\">name = <span class=\"string\">&quot;martin&quot;</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"使用函数str-避免类型错误\"><a href=\"#使用函数str-避免类型错误\" class=\"headerlink\" title=\"使用函数str()避免类型错误\"></a>使用函数str()避免类型错误</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">age = <span class=\"number\">23</span></span><br><span class=\"line\">message = <span class=\"string\">&quot;Happy&quot;</span> + <span class=\"built_in\">str</span>(age) + <span class=\"string\">&quot;rd Birthday!&quot;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(message)</span><br></pre></td></tr></table></figure>\n<p>这里，调用函数str()，python将非字符串值表示为字符串</p>\n<h2 id=\"列表\"><a href=\"#列表\" class=\"headerlink\" title=\"列表\"></a>列表</h2><p>关键词：list []</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list1 = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1[<span class=\"number\">2</span>]) <span class=\"comment\"># 3</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1[-<span class=\"number\">1</span>]) <span class=\"comment\"># 5</span></span><br></pre></td></tr></table></figure>\n<p>list[-1]将访问列表最后一个值，-2为倒数第二个值，以此类推。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list2 = [<span class=\"number\">1</span>, <span class=\"string\">&quot;art&quot;</span>, dict1] <span class=\"comment\">#同一个列表中支持多种数据类型</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"字典（哈希表）\"><a href=\"#字典（哈希表）\" class=\"headerlink\" title=\"字典（哈希表）\"></a>字典（哈希表）</h2><p>关键词： dict {key: value}</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">dict1 = &#123;<span class=\"string\">&quot;name&quot;</span>: <span class=\"string\">&quot;martin&quot;</span>, <span class=\"string\">&quot;age&quot;</span>: <span class=\"number\">18</span>, <span class=\"number\">20</span>: <span class=\"number\">80</span>&#125;</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dict1[<span class=\"string\">&quot;name&quot;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dict1[<span class=\"string\">&quot;age&quot;</span>])</span><br><span class=\"line\"><span class=\"built_in\">print</span>(dict1[<span class=\"number\">20</span>])</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"运算\"><a href=\"#运算\" class=\"headerlink\" title=\"运算\"></a>运算</h1><p><code>//</code> 地板除<br><code>/</code> 普通除<br><code>**</code> 乘方</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"number\">9</span>/<span class=\"number\">2</span>) <span class=\"comment\"># 4.5</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"number\">9</span>//<span class=\"number\">2</span>) <span class=\"comment\"># 4</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"number\">9</span>**<span class=\"number\">2</span>) <span class=\"comment\"># 81</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"函数\"><a href=\"#函数\" class=\"headerlink\" title=\"函数\"></a>函数</h1><p>带初值的函数示例</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">func1</span>(<span class=\"params\">a, b = <span class=\"number\">2</span></span>) <span class=\"comment\">#如果传入b，返回a**b；否则，返回a**2</span></span><br><span class=\"line\">\t<span class=\"keyword\">return</span> a ** b</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"built_in\">print</span>(func1(<span class=\"number\">2</span>)) <span class=\"comment\"># 4</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"列表-1\"><a href=\"#列表-1\" class=\"headerlink\" title=\"列表\"></a>列表</h1><h2 id=\"切片\"><a href=\"#切片\" class=\"headerlink\" title=\"切片\"></a>切片</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list1 = [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>, <span class=\"number\">5</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1[<span class=\"number\">1</span>:<span class=\"number\">4</span>]) <span class=\"comment\"># [2, 3, 4] 左闭右开</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1[<span class=\"number\">1</span>:-<span class=\"number\">1</span>]) <span class=\"comment\"># [2, 3, 4]</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1[:]) <span class=\"comment\"># [1, 2, 3, 4, 5]</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(list1[<span class=\"number\">1</span>:]) <span class=\"comment\"># [2, 3, 4, 5]</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"操作\"><a href=\"#操作\" class=\"headerlink\" title=\"操作\"></a>操作</h2><h3 id=\"添加元素\"><a href=\"#添加元素\" class=\"headerlink\" title=\"添加元素\"></a>添加元素</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list1.append(<span class=\"number\">6</span>) <span class=\"comment\"># [1, 2, 3, 4, 5, 6] 在末尾添加</span></span><br><span class=\"line\">list1.insert(<span class=\"number\">0</span>, <span class=\"number\">0.5</span>) <span class=\"comment\">#[0.5, 1, 2, 3, 4, 5] 在指定位置添加</span></span><br><span class=\"line\">list1.extend([<span class=\"number\">8</span>, <span class=\"number\">9</span>]) <span class=\"comment\">#在列表末尾一次性追加另一个列表的多个值，即用新列表扩展原来的列表</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"删除元素\"><a href=\"#删除元素\" class=\"headerlink\" title=\"删除元素\"></a>删除元素</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">del</span> list1[<span class=\"number\">0</span>] <span class=\"comment\">#删除索引0处的元素，且不再以任何方式使用它</span></span><br><span class=\"line\">list1.pop(index) <span class=\"comment\">#删除索引index处的元素，但会继续使用它，index为空时默认为-1，即栈顶</span></span><br><span class=\"line\">list1.remove(key) <span class=\"comment\">#删除列表中值为key的元素，也可继续使用它的值；若列表中有多个值为key，remove只删除第一个指定的值</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"列表排序\"><a href=\"#列表排序\" class=\"headerlink\" title=\"列表排序\"></a>列表排序</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list1.sort() <span class=\"comment\"># 永久顺序排序</span></span><br><span class=\"line\">list1.sort(reverse = <span class=\"literal\">True</span>) <span class=\"comment\"># 永久倒序排序</span></span><br><span class=\"line\"><span class=\"built_in\">sorted</span>(list1, reverse = <span class=\"literal\">True</span>) <span class=\"comment\"># 临时倒序排序</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"数值列表\"><a href=\"#数值列表\" class=\"headerlink\" title=\"数值列表\"></a>数值列表</h2><p>range()</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> value <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"number\">5</span>):</span><br><span class=\"line\">\t<span class=\"built_in\">print</span>(value)</span><br></pre></td></tr></table></figure>\n<p>将打印数字1 2 3 4. 左闭右开<br>要创建数字列表，可使用函数list()将range()的结果直接转换为列表</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">1</span>,<span class=\"number\">5</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"built_in\">list</span>) <span class=\"comment\"># [1, 2, 3, 4]</span></span><br></pre></td></tr></table></figure>\n<p>函数range()还可指定步长</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">even_numbers = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(<span class=\"number\">2</span>,<span class=\"number\">11</span>,<span class=\"number\">2</span>))</span><br><span class=\"line\"><span class=\"built_in\">print</span>(even_numbers) <span class=\"comment\"># [2, 4, 6, 8, 10]</span></span><br></pre></td></tr></table></figure>\n<h2 id=\"确定列表的长度\"><a href=\"#确定列表的长度\" class=\"headerlink\" title=\"确定列表的长度\"></a>确定列表的长度</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">len</span>(<span class=\"built_in\">list</span>)</span><br></pre></td></tr></table></figure>\n<h1 id=\"字典\"><a href=\"#字典\" class=\"headerlink\" title=\"字典\"></a>字典</h1><h2 id=\"遍历字典\"><a href=\"#遍历字典\" class=\"headerlink\" title=\"遍历字典\"></a>遍历字典</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">favorite_languages = &#123;</span><br><span class=\"line\">\t<span class=\"string\">&#x27;jen&#x27;</span>:<span class=\"string\">&#x27;python&#x27;</span>,</span><br><span class=\"line\">\t<span class=\"string\">&#x27;sarah&#x27;</span>:<span class=\"string\">&#x27;c&#x27;</span>,</span><br><span class=\"line\">\t<span class=\"string\">&#x27;edward&#x27;</span>:<span class=\"string\">&#x27;ruby&#x27;</span>,</span><br><span class=\"line\">\t<span class=\"string\">&#x27;phil&#x27;</span>:<span class=\"string\">&#x27;python&#x27;</span>,</span><br><span class=\"line\">&#125;</span><br><span class=\"line\"><span class=\"comment\"># 遍历所有的键值对</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key, value <span class=\"keyword\">in</span> favorite_languages.items()</span><br><span class=\"line\"><span class=\"comment\"># 遍历字典中所有的键</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> favorite_languages.keys()</span><br><span class=\"line\"><span class=\"comment\"># 遍历字典中所有的值</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> value <span class=\"keyword\">in</span> favorite_languages.values()</span><br><span class=\"line\"><span class=\"comment\"># 按顺序遍历</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> key <span class=\"keyword\">in</span> <span class=\"built_in\">sorted</span>(favorite_languages.keys())</span><br><span class=\"line\"><span class=\"comment\"># 去重遍历</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> value <span class=\"keyword\">in</span> <span class=\"built_in\">set</span>(favorite_languages.values())</span><br></pre></td></tr></table></figure>\n<h1 id=\"类\"><a href=\"#类\" class=\"headerlink\" title=\"类\"></a>类</h1><h2 id=\"类编码风格\"><a href=\"#类编码风格\" class=\"headerlink\" title=\"类编码风格\"></a>类编码风格</h2><p>类名应采用驼峰命名法，即将类名中的每个单词的首字母都大写，而不使用下划线。实例名和模块名都采用小写格式，并在单词间加上下划线。<br>对于每个类，都应紧跟在类定义后面包含一个文档字符串。这种文档字符串简要地描述类的功能，并遵循编写函数的文档字符串时采用的格式约定。每个模块也都应包含一个文档字符串，对其中的类可用于做什么进行描述。<br>可使用空行来组织代码，但不要滥用。<strong>在类中，可使用一个空行来分隔方法；而在模块中，可使用两个空行来分隔类。</strong><br>需要同时导入标准库中的模块和你编写的模块时，先编写导入标准库模块的import语句，再添加一个空行，然后编写导入你自己编写模块的import语句。在包含多条import语句的程序中，这种做法让人更容易明白程序使用的各个模块都来自何方。</p>\n<h2 id=\"创建和使用类\"><a href=\"#创建和使用类\" class=\"headerlink\" title=\"创建和使用类\"></a>创建和使用类</h2><h3 id=\"创建类\"><a href=\"#创建类\" class=\"headerlink\" title=\"创建类\"></a>创建类</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Dog</span>():</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, name, age</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;初始化属性name和age&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.name = name</span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.age = age</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">sit</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;模拟小狗被命令时蹲下&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"built_in\">print</span>(<span class=\"variable language_\">self</span>.name.title() + <span class=\"string\">&quot; is now sitting.&quot;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">roll_over</span>(<span class=\"params\">self</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;模拟小狗被命令时打滚&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"built_in\">print</span>(<span class=\"variable language_\">self</span>.name.title() + <span class=\"string\">&quot; rolled over!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<ul>\n<li>根据约定，在python中，首字母大写的名称指的是类。</li>\n<li>方法__init__()<ul>\n<li>类中的函数称为方法。</li>\n<li>方法__init__()是一个特殊的方法，每当你根据Dog类创建新实例时，python都会自动运行它。在这个方法的名称中，开头和末尾各有两个下划线，这是一种约定，旨在避免python默认方法与普通方法发生名称冲突。</li>\n<li>在这个方法的定义中，形参self必不可少，还必须位于其他形参的前面。因为python在调用这个__init__()方法来创建实例时，将自动传入形参self。每个与类相关联的方法调用都自动传递实参self，它是一个指向实例本身的引用，让实例能够访问类中的属性和方法。本例，我们创建Dog实例时，python将调用Dog类的方法__init__()。我们将通过实参向Dog()传递名字和年龄；self会自动传递，因此我们不需要传递它。每当我们根据Dog类创建实例时，都只需给最后两个形参(name和age)提供值。</li>\n<li>定义的两个变量都有前缀self。以self为前缀的变量都可供类中的所有方法使用，我们还可以通过类的任何实例来访问这些变量。self.name &#x3D; name获取存储在形参name中的值，并将其存储到变量name中，然后该变量被关联到当前创建的实例。self.age &#x3D; age的作用与此类似。像这样可通过实例访问的变量称为属性。</li>\n</ul>\n</li>\n<li>Dog类还定义了另外两个方法：sit()和roll_over()。由于这些方法不需要额外的信息，如名字和年龄，因此它们只有一个形参self。</li>\n</ul>\n<h3 id=\"根据类创建实例\"><a href=\"#根据类创建实例\" class=\"headerlink\" title=\"根据类创建实例\"></a>根据类创建实例</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 创建实例 my_dog</span></span><br><span class=\"line\">my_dog = Dog(<span class=\"string\">&#x27;Heymi&#x27;</span>, <span class=\"number\">4</span>)</span><br><span class=\"line\"><span class=\"comment\"># 访问属性</span></span><br><span class=\"line\">my_dog.name</span><br><span class=\"line\"><span class=\"comment\"># 调用方法</span></span><br><span class=\"line\">my_dog.sit()</span><br></pre></td></tr></table></figure>\n<h2 id=\"使用类和实例\"><a href=\"#使用类和实例\" class=\"headerlink\" title=\"使用类和实例\"></a>使用类和实例</h2><h3 id=\"给属性指定默认值\"><a href=\"#给属性指定默认值\" class=\"headerlink\" title=\"给属性指定默认值\"></a>给属性指定默认值</h3><p>类中的每个属性都必须有初始值，哪怕这个值是0或空字符串。在有些情况下，如设置默认值时，在方法__init__()内指定这种初始值是可行的；如果你对某个属性这样做了，就无需包含为它提供初始值的形参。</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Car</span>():</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, make, model, year</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;初始化描述汽车的属性&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.make = make</span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.model = model</span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.year = year</span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.odometer_reading = <span class=\"number\">0</span> <span class=\"comment\">#python将创建一个名为odometer_reading的属性，并将其初始值设置为0\t</span></span><br></pre></td></tr></table></figure>\n<h3 id=\"修改属性的值\"><a href=\"#修改属性的值\" class=\"headerlink\" title=\"修改属性的值\"></a>修改属性的值</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">my_new_car = Car(<span class=\"string\">&quot;audi&quot;</span>, <span class=\"string\">&quot;a5&quot;</span>, <span class=\"number\">2025</span>)</span><br><span class=\"line\"><span class=\"comment\"># 直接修改属性的值</span></span><br><span class=\"line\">my_new_car.odometer_reading = <span class=\"number\">23</span></span><br><span class=\"line\"><span class=\"comment\"># 通过方法修改属性的值</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Car</span>():</span><br><span class=\"line\">\t--snip--</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">update_odometer</span>(<span class=\"params\">self, mileage</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;将里程表读数设置为指定的值&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.odometer_reading = milege</span><br><span class=\"line\">my_new_car.update_odometer(<span class=\"number\">23</span>)</span><br><span class=\"line\"><span class=\"comment\"># 通过方法对属性的值进行递增</span></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Car</span>():</span><br><span class=\"line\">\t--snip--</span><br><span class=\"line\">\t</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">increment_odometer</span>(<span class=\"params\">self, miles</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;将里程表读数增加指定的量&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.odometer_reading += miles</span><br></pre></td></tr></table></figure>\n<h2 id=\"继承\"><a href=\"#继承\" class=\"headerlink\" title=\"继承\"></a>继承</h2><p>一个类继承另一个类时，它将自动获得另一个类的所有属性和方法；原有的类称为父类，而新类称为子类。子类继承了其父类的所有属性和方法，同时还可以定义自己的属性和方法。</p>\n<h3 id=\"子类的方法-init\"><a href=\"#子类的方法-init\" class=\"headerlink\" title=\"子类的方法__init__()\"></a>子类的方法__init__()</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ElectricCar</span>(<span class=\"title class_ inherited__\">Car</span>):</span><br><span class=\"line\">\t<span class=\"string\">&quot;&quot;&quot;电动汽车的独特之处&quot;&quot;&quot;</span></span><br><span class=\"line\"></span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, make, model, year</span>):</span><br><span class=\"line\">\t\t<span class=\"string\">&quot;&quot;&quot;初始化父类的属性&quot;&quot;&quot;</span></span><br><span class=\"line\">\t\t<span class=\"built_in\">super</span>().__init__(make, model, year)</span><br></pre></td></tr></table></figure>\n<h3 id=\"python-2-7中的继承\"><a href=\"#python-2-7中的继承\" class=\"headerlink\" title=\"python 2.7中的继承\"></a>python 2.7中的继承</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Car</span>(<span class=\"title class_ inherited__\">object</span>):</span><br><span class=\"line\">\t--snip--</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ElectricCar</span>(<span class=\"title class_ inherited__\">Car</span>):</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, make, model, year</span>):</span><br><span class=\"line\">\t\t<span class=\"built_in\">super</span>(ElectricCar, <span class=\"variable language_\">self</span>).__init__(make, model, year) <span class=\"comment\"># 1</span></span><br></pre></td></tr></table></figure>\n<p>1 函数super()需要两个实参：子类名和对象self。<br>在python 2.7中使用继承时，务必在定义父类时在括号内指定object</p>\n<h3 id=\"给子类定义属性和方法\"><a href=\"#给子类定义属性和方法\" class=\"headerlink\" title=\"给子类定义属性和方法\"></a>给子类定义属性和方法</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Car</span>():</span><br><span class=\"line\">\t--snip--</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ElectricCar</span>(<span class=\"title class_ inherited__\">Car</span>):</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, make, model, year</span>):</span><br><span class=\"line\">\t\t<span class=\"built_in\">super</span>().__init__(make, model, year)</span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.battery_size = <span class=\"number\">70</span> <span class=\"comment\"># 1</span></span><br></pre></td></tr></table></figure>\n<p>1 添加了新属性self.battery_size，并设置其初始值。根据ElectricCar类创建的所有实例都将包含这个属性，但所有Car实例都不包含它。</p>\n<h3 id=\"重写父类的方法\"><a href=\"#重写父类的方法\" class=\"headerlink\" title=\"重写父类的方法\"></a>重写父类的方法</h3><p>对于父类的方法，只要它不符合子类的行为，都可对其进行重写。为此，可在子类中定义一个与要重写的父类方法同名的方法。这样，python将不会考虑这个父类方法，而只关注你在子类中定义的方法。</p>\n<h3 id=\"将实例用作属性\"><a href=\"#将实例用作属性\" class=\"headerlink\" title=\"将实例用作属性\"></a>将实例用作属性</h3><p>使用代码模拟实物时，你可能会发现自己给类添加的细节越来越多：属性和方法清单以及文件都越来越长。在这种情况下，可能需要将类的一部分作为一个独立的类提取出来。<br>例如，不断给ElectricCar类添加细节时，可能其中包含很多专门针对Battery的属性和方法，则可将这些属性和方法提取出来，放到一个名为Battery的类中，并将一个Battery实例用作ElectricCar类的一个属性：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Car</span>():</span><br><span class=\"line\">\t--snip--</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">Battery</span>():</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, battery_size=<span class=\"number\">70</span></span>): <span class=\"comment\"># 1</span></span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.battery_size = battery_size</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">class</span> <span class=\"title class_\">ElectricCar</span>(<span class=\"title class_ inherited__\">Car</span>):</span><br><span class=\"line\">\t<span class=\"keyword\">def</span> <span class=\"title function_\">__init__</span>(<span class=\"params\">self, make, model, year</span>):</span><br><span class=\"line\">\t\t<span class=\"built_in\">super</span>().__init__(make, model, year)</span><br><span class=\"line\">\t\t<span class=\"variable language_\">self</span>.battery = Battery() <span class=\"comment\"># 2</span></span><br></pre></td></tr></table></figure>\n<p>1: __init__()除self外，还有另一个形参battery_size。这个形参是可选的：如果没有给它提供值，电瓶容量将被设置为70。<br>2: 在ElectricCar类中，我们添加了一个名为self.battery的属性。这行代码让python创建一个新的Battery实例（由于没有指定尺寸，因此为默认值70），并将该实例存储在属性self.battery中。每当方法__init__()被调用时，都将执行该操作；因此现在每个ElectricCar实例都包含一个自动创建的Battery实例。</p>\n<h1 id=\"导入\"><a href=\"#导入\" class=\"headerlink\" title=\"导入\"></a>导入</h1><h2 id=\"导入函数\"><a href=\"#导入函数\" class=\"headerlink\" title=\"导入函数\"></a>导入函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> module_name <span class=\"comment\"># 导入整个模块</span></span><br><span class=\"line\">module_name.function_name() <span class=\"comment\"># 使用模块中的函数需要使用句点</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">from</span> module_name <span class=\"keyword\">import</span> function_name <span class=\"comment\"># 导入特定函数，该函数后续使用时不需要句点</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">import</span> pizza <span class=\"keyword\">as</span> p <span class=\"comment\"># 使用as给模块指定别名</span></span><br></pre></td></tr></table></figure>\n\n<h2 id=\"导入类\"><a href=\"#导入类\" class=\"headerlink\" title=\"导入类\"></a>导入类</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">from</span> car <span class=\"keyword\">import</span> Car, ElectricCar <span class=\"comment\"># 在模块文件car.py中导入Car类、ElectricCar类</span></span><br><span class=\"line\"><span class=\"keyword\">import</span> car <span class=\"comment\"># 导入整个car模块</span></span><br><span class=\"line\">my_beetle = car.Car(<span class=\"string\">&#x27;volkswagen&#x27;</span>, <span class=\"string\">&#x27;beetle&#x27;</span>, <span class=\"number\">2025</span>) <span class=\"comment\"># 创建类实例代码都必须包含模块名，即需要使用句点访问</span></span><br></pre></td></tr></table></figure>\n\n<h1 id=\"注释\"><a href=\"#注释\" class=\"headerlink\" title=\"注释\"></a>注释</h1><p>Python 中的注释有<strong>单行注释</strong>和<strong>多行注释</strong>。</p>\n<h2 id=\"单行注释\"><a href=\"#单行注释\" class=\"headerlink\" title=\"单行注释\"></a>单行注释</h2><p>单行注释以 # 开头，例如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#这是一个注释</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(hello, world)</span><br></pre></td></tr></table></figure>\n<h2 id=\"多行注释\"><a href=\"#多行注释\" class=\"headerlink\" title=\"多行注释\"></a>多行注释</h2><p>多行注释用三个单引号 ‘’’ 或者三个双引号 “”” 将注释括起来，例如</p>\n<h3 id=\"单引号\"><a href=\"#单引号\" class=\"headerlink\" title=\"单引号\"></a>单引号</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/usr/bin/python3 </span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">这是多行注释，用三个单引号</span></span><br><span class=\"line\"><span class=\"string\">这是多行注释，用三个单引号 </span></span><br><span class=\"line\"><span class=\"string\">这是多行注释，用三个单引号</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Hello, World!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<h3 id=\"双引号\"><a href=\"#双引号\" class=\"headerlink\" title=\"双引号\"></a>双引号</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\">#!/usr/bin/python3 </span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"string\">这是多行注释（字符串），用三个双引号</span></span><br><span class=\"line\"><span class=\"string\">这是多行注释（字符串），用三个双引号 </span></span><br><span class=\"line\"><span class=\"string\">这是多行注释（字符串），用三个双引号</span></span><br><span class=\"line\"><span class=\"string\">&quot;&quot;&quot;</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;Hello, World!&quot;</span>)</span><br></pre></td></tr></table></figure>\n<h2 id=\"拓展说明\"><a href=\"#拓展说明\" class=\"headerlink\" title=\"拓展说明\"></a>拓展说明</h2><p>在 Python 中，多行注释是由三个单引号 ‘’’ 或三个双引号 “”” 来定义的，而且这种注释方式并不能嵌套使用。<br>当你开始一个多行注释块时，Python 会一直将后续的行都当作注释，直到遇到另一组三个单引号或三个双引号。<br><strong>嵌套多行注释会导致语法错误。</strong><br>例如，下面的示例是不合法的：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">这是外部的多行注释</span></span><br><span class=\"line\"><span class=\"string\">可以包含一些描述性的内容</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\">    &#x27;&#x27;&#x27;</span></span><br><span class=\"line\">    这是尝试嵌套的多行注释</span><br><span class=\"line\">    会导致语法错误</span><br><span class=\"line\">    <span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>在这个例子中，内部的三个单引号并没有被正确识别为多行注释的结束，而是被解释为普通的字符串。<br>这将导致代码结构不正确，最终可能导致语法错误。<br>如果你需要在注释中包含嵌套结构，推荐使用单行注释（以#开头）而不是多行注释。<br>单行注释可以嵌套在多行注释中，而且不会引起语法错误。例如：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br><span class=\"line\"><span class=\"string\">这是外部的多行注释</span></span><br><span class=\"line\"><span class=\"string\">可以包含一些描述性的内容</span></span><br><span class=\"line\"><span class=\"string\"></span></span><br><span class=\"line\"><span class=\"string\"># 这是内部的单行注释</span></span><br><span class=\"line\"><span class=\"string\"># 可以嵌套在多行注释中</span></span><br><span class=\"line\"><span class=\"string\">&#x27;&#x27;&#x27;</span></span><br></pre></td></tr></table></figure>\n<p>这样的结构是合法的，并且通常能够满足文档化和注释的需求。</p>\n"},{"title":"深度学习python库","_content":"# numpy\n```python\nimport numpy as np\n```\n## 变量及基本操作\n### 矩阵\n```python\nlist1 = [  \n    [1, 2, 3, 4],  \n    [5, 6, 7, 8],  \n    [9, 10, 11, 12],  \n    [13, 14, 15, 16]  \n]\n\narray1 = np.array(list1) # 将列表转化为矩阵，打印的效果会不一样\n\narray2 = array1\n\n# 矩阵合并，默认axis = 0, 纵向合并。 axis = 1为横向合并，以此类推\narray3 = np.concatenate((array1, array2), axis = 1)\n```\n#### 矩阵的切片\n```python\nprint(array3[1:3, 2:4]) # 可以在两个维度进行切片\n\n# 也可以跳着切\nlist2 = [1, 3]\nprint(array3[:, list2])\n```\n\n# torch\n\n## 变量及基本操作\n\n### 张量 tensor\n```python\ntensor1 = torch.tensor(list1)  \n  \nprint(array1)  \nprint(tensor1)\n```\n输出如下： tensor与array的区别仅在于tensor将array放在了一张张量网上，可以进行梯度计算\n![](Pastedimage20250319161350.png)\n\n### 梯度计算\n```python\nx = torch.tensor(3.0)\nx.requires_grad_(True)\ny = x**2\ny.backward() # 2x = 6\n\n# 每个变量的梯度会在变量中进行记录，如果对一个变量分别求两次梯度，需进行清零。否则会出现梯度累加，导致计算错误，如本例中不进行清零x的梯度将为6 + 6 = 12\nx.grad = torch.tensor(0.0)\ny2 = x**2\ny2.backward() #2x = 6\n\nx.detach() # 将x在张量网上摘下，不再计算梯度\n```\n\n### 创建张量\n```python\ntensor1 = torch.ones((10,1)) # 创建一个10行1列全是1的张量\ntensor0 = torch.zeros((10,1)) # 创建一个10行1列全是0的张量\ntensor2 = torch.normal(mean, std, shape) # 创建一个形状为shape，均值为mean，标准差为std的正态分布的张量\n# ep\ntensor2 = torch.normal(0, 0.01, (3, 10, 4)) # 均值为0，标准差为0.01，形状3维10行4列\n```\n\n### 张量求和\n```python\nsum1 = torch.sum(tensor1, dim = 0 or 1, keepdim = True) # 0按列求和，1按行求和; keepdim保持原shape打印\n```\n\n### 张量形状\n```python\ntensor.shape\n```\n\n## tensor.backward()\n在PyTorch中，`tensor.backward()` 是实现自动微分（Autograd）的核心方法。它的主要作用是计算当前张量相对于某个标量值的梯度，并存储在相应张量的 `.grad` 属性中。以下是关键要点：\n\n1. **基本用法**：\n```python\nx = torch.tensor(2.0, requires_grad=True)\ny = x ** 2\ny.backward()  # 计算梯度\nprint(x.grad)  # 输出：tensor(4.)\n```\n\n2. **工作原理**：\n- 构建计算图：所有涉及`requires_grad=True`张量的运算都会记录在动态计算图中\n- 反向传播：从调用`backward()`的张量开始，反向遍历计算图应用链式法则\n- 梯度累积：结果梯度会累加到叶子节点的`.grad`属性中\n\n3. **参数说明**：\n```python\n# 当输出是非标量时需指定梯度权重\nx = torch.randn(3, requires_grad=True)\ny = x * 2\ny.backward(gradient=torch.tensor([1., 1., 1.]))  # 等效于y.sum().backward()\n\n# 保留计算图结构（用于多次反向传播）\nloss.backward(retain_graph=True)\n```\n\n4. **典型应用场景**：\n```python\n# 训练循环中的典型用法\nfor data, target in dataloader:\n    optimizer.zero_grad()  # 清空梯度\n    output = model(data)\n    loss = loss_fn(output, target)\n    loss.backward()        # 计算梯度\n    optimizer.step()       # 更新参数\n```\n\n### 注意事项：\n- 梯度会自动累积，需手动调用`zero_grad()`清除\n- 默认会释放计算图，二次调用`backward()`需设置`retain_graph=True`\n- 只能对标量值直接调用`backward()`，多维张量需提供`gradient`参数\n以下是关于PyTorch中`tensor.backward()`两个关键机制的深入解释：\n\n---\n\n#### 1. **默认释放计算图与`retain_graph=True`的作用**\n##### 原理\nPyTorch的自动微分系统基于动态计算图（Dynamic Computation Graph）。当调用`backward()`时：\n- **默认行为**：反向传播完成后，计算图会被**立即释放**以节省内存。\n- **问题**：如果尝试再次调用`backward()`，由于计算图已销毁，会抛出`RuntimeError`。\n\n##### 示例\n```python\nx = torch.tensor(2.0, requires_grad=True)\ny = x ** 3\n\n# 第一次反向传播（正常执行）\ny.backward()  \nprint(x.grad)  # 输出: tensor(12.)\n\n# 第二次反向传播（会报错）\ny.backward()  # ❌ RuntimeError: Trying to backward through the graph a second time\n```\n\n##### 解决方案\n通过`retain_graph=True`保留计算图：\n```python\ny.backward(retain_graph=True)  # 第一次反向传播保留计算图\ny.backward()                   # ✅ 可再次执行\nprint(x.grad)                  # 梯度累积为 12 + 12 = 24\n```\n\n##### 注意\n- **内存开销**：保留计算图会增加内存占用，需谨慎使用\n- **应用场景**：如GAN需要交替训练生成器和判别器时\n- **替代方案**：使用`torch.autograd.grad()`直接计算梯度（不修改`.grad`属性）\n\n---\n\n#### 2. **标量限制与`gradient`参数的作用**\n##### 原理\n- **标量限制**：数学上梯度定义为**标量函数对张量的导数**。当输出是非标量时，PyTorch无法确定如何聚合多维输出的梯度。\n- **gradient参数**：本质是一个权重向量，用于计算**加权和的梯度**（相当于`torch.sum(gradient * output)`）。\n\n##### 示例\n```python\nx = torch.tensor([1.0, 2.0], requires_grad=True)\ny = x * 2  # y = [2.0, 4.0]\n\n# ❌ 直接调用会报错\ny.backward()  # RuntimeError: grad can be implicitly created only for scalar outputs\n\n# ✅ 正确方式：提供gradient参数\ny.backward(gradient=torch.tensor([1., 1.]))  \nprint(x.grad)  # 输出: tensor([2., 2.]) \n               # 计算过程：d(sum(y))/dx = d(2x1 + 2x2)/dx = [2, 2]\n```\n\n##### 数学等价性\n```python\n# 以下两种写法等价\ny.backward(gradient=torch.tensor([1., 1.]))\n# 等价于\ny.sum().backward()\n```\n\n##### 高级用法\n```python\n# 自定义权重计算梯度\ngradient_weights = torch.tensor([0.5, 2.0])\ny.backward(gradient=gradient_weights)\nprint(x.grad)  # 输出: tensor([1., 4.]) \n               # 计算：0.5*2 + 2.0*2 = [1, 4]\n```\n\n##### 注意\n- **默认行为**：当输出是标量时，等价于`gradient=torch.tensor(1.0)`\n- **广播机制**：`gradient`的形状必须与输出张量形状匹配\n- **物理意义**：可理解为对多维输出不同通道的梯度重要性加权\n\n### 梯度计算解牛\n\n#### **1. 梯度计算的本质**\nPyTorch的梯度计算本质上是 **逐元素（element-wise）** 进行的。对于任意形状的张量：\n- 梯度张量与原张量形状 **严格一致**\n- 每个元素的梯度表示 **该元素对最终标量损失值的贡献**\n\n##### 示例代码中的 `w_0`\n```python\ntrue_w = torch.tensor([10.0, 7.0, 5.0, 2.0])  # 4维向量\nw_0 = torch.normal(0, 0.01, true_w.shape, requires_grad=True)  # 形状为 (4,)\n```\n\n当调用 `loss.backward()` 时：\n- 系统会计算损失值对 `w_0` 的 **每个元素** 的偏导数\n- 最终 `w_0.grad` 也会是一个形状为 `(4,)` 的张量\n\n---\n\n#### **2. 计算过程可视化**\n假设当前 `w_0` 和梯度如下：\n```python\nw_0 = tensor([w1, w2, w3, w4], requires_grad=True)  # 实际值可能是 [0.1, -0.2, 0.05, 0.3]\ngrad = tensor([dw1, dw2, dw3, dw4])                # 例如 [1.2, 0.8, -0.5, 2.1]\n```\n\n参数更新操作：\n```python\nw_0 -= lr * w_0.grad\n# 等价于：\nw1_new = w1 - lr * dw1\nw2_new = w2 - lr * dw2\nw3_new = w3 - lr * dw3\nw4_new = w4 - lr * dw4\n```\n\n---\n\n#### **3. 数学推导验证**\n![](Snipaste_2025-03-20_14-56-59.jpg)\n\n---\n\n#### **4. 代码执行细节验证**\n在你的代码中：\n```python\ndef fun(x, w, b):\n    pred_y = torch.matmul(x, w) + b  # x形状(batch_size,4), w形状(4,)\n    return pred_y                    # 输出形状(batch_size,)\n\ndef maeLoss(pred_y, y):\n    return torch.sum(abs(pred_y-y))/len(y)  # 输出是标量\n```\n\n反向传播时：\n1. `loss` 是标量（关键！）\n2. 通过链式法则，PyTorch会自动计算标量损失对每个 `w_j` 的梯度\n3. 所有梯度按原始维度组织，存储在 `w_0.grad`\n\n---\n\n#### **5. 梯度形状验证实验**\n可以添加调试代码验证梯度形状：\n```python\nprint(w_0.shape)        # 输出: torch.Size([4])\nprint(w_0.grad.shape)   # 反向传播后会输出: torch.Size([4])\n```\n\n---\n\n#### **6. 与一维情况的对比**\n| 参数类型 | 参数形状 | 梯度形状 | 更新方式              |\n|----------|----------|----------|-----------------------|\n| 标量     | ()       | ()       | `w -= lr * w.grad`    |\n| 向量     | (4,)     | (4,)     | 逐元素更新            |\n| 矩阵     | (3,4)    | (3,4)    | 每个矩阵元素独立更新  |\n\n---\n\n#### **为什么没有维度冲突？**\n- **关键点**：损失函数最终输出始终是 **标量**（`maeLoss` 返回的是平均值）\n- **数学保证**：标量对任意维度张量的导数自然保持与原张量相同的形状\n- **PyTorch特性**：自动微分系统会处理任意维度的张量梯度计算\n\n---\n\n#### **常见误区澄清**\n误区：*\"多维张量需要特殊处理才能计算梯度\"*\n\n事实：\n- 只要最终损失是标量，PyTorch可以处理任意维度的参数梯度\n- 无论参数是标量、向量、矩阵还是高阶张量，梯度计算规则一致\n- 参数更新时的逐元素操作是自动完成的\n\n# matplotlib.pyplot\n\n## plt.plot()\n在使用 PyTorch 进行深度学习训练时，如果要将张量数据传递给 `matplotlib.pyplot.plot()` 进行可视化，通常需要添加 `.detach().numpy()` 操作。这是由 PyTorch 张量和 Matplotlib 的底层机制差异导致的，具体原因如下：\n\n---\n\n### **1. 核心原因：数据格式转换**\n| 操作步骤          | 作用                                                                                     | 必要性 |\n|--------------------|------------------------------------------------------------------------------------------|--------|\n| `.detach()`        | 将张量从计算图中分离，得到一个不需要梯度追踪的新张量                                      | 必要   |\n| `.numpy()`         | 将 PyTorch 张量转换为 NumPy 数组（Matplotlib 只能处理 NumPy 数组或 Python 原生数据类型） | 必要   |\n\n---\n\n### **2. 分步详解**\n#### **（1）脱离计算图（.detach()）**\n- **问题背景**：PyTorch 张量可能带有梯度信息（`requires_grad=True`）\n- **风险**：如果直接使用带有梯度的张量：\n  - 会增加不必要的内存占用（保持计算图）\n  - 可能引发意外的梯度传播（尽管绘图操作不需要梯度）\n- **示例对比**：\n  ```python\n  # 原始张量（带梯度）\n  tensor_with_grad = torch.tensor([1.0, 2.0], requires_grad=True)\n  \n  # 直接转换会报错\n  try:\n      plt.plot(tensor_with_grad)  # ❌ 报错：Can't call numpy() on Tensor that requires grad\n  except Exception as e:\n      print(e)\n  \n  # 正确做法\n  plt.plot(tensor_with_grad.detach().numpy())  # ✅ 正常工作\n  ```\n\n#### **（2）设备转移（CPU/GPU）**\n- **问题背景**：如果张量在 GPU 上（`device='cuda'`）\n- **风险**：Matplotlib 无法直接处理 GPU 上的张量\n- **完整转换流程**：\n  ```python\n  # GPU 张量处理流程\n  gpu_tensor = torch.tensor([1.0, 2.0], device='cuda')\n  \n  # 错误方式\n  plt.plot(gpu_tensor.cpu().detach().numpy())  # ❌ 顺序错误，应先 detach\n  \n  # 正确方式\n  plt.plot(gpu_tensor.detach().cpu().numpy())  # ✅ 正确顺序：detach → cpu → numpy\n  ```\n\n#### **（3）数据类型转换**\n| 数据类型           | 说明                                                                 |\n|--------------------|----------------------------------------------------------------------|\n| PyTorch Tensor     | 可以是任意形状和数据类型（float32, int64 等）                        |\n| NumPy Array        | Matplotlib 的底层数据容器，与 PyTorch 内存不兼容                     |\n\n---\n\n### **3. 完整转换流程**\n```python\n# 假设有一个需要绘制的 PyTorch 张量\noriginal_tensor = torch.randn(100, requires_grad=True, device='cuda')\n\n# 安全转换步骤\nplot_data = original_tensor.detach()  # 1. 脱离计算图\n                .cpu()                # 2. 转移到 CPU（如果是 GPU 张量）\n                .numpy()              # 3. 转换为 NumPy 数组\n\n# 绘制图形\nplt.plot(plot_data)\nplt.show()\n```\n\n---\n\n### **4. 常见错误场景**\n#### **场景 1：未分离计算图**\n```python\nx = torch.linspace(0, 2*np.pi, 100, requires_grad=True)\ny = torch.sin(x)\n\nplt.plot(x.numpy(), y.numpy())  # ❌ RuntimeError: Can't call numpy() on Tensor that requires grad\n```\n\n#### **场景 2：未处理 GPU 张量**\n```python\ngpu_data = torch.randn(10).cuda()\nplt.plot(gpu_data.detach().numpy())  # ❌ TypeError: can't convert cuda:0 device type tensor to numpy\n```\n\n#### **场景 3：错误操作顺序**\n```python\n# 先转 NumPy 再 detach 会丢失梯度信息\ntemp = y.numpy()         # ❌ 错误开始点\ndetached = temp.detach() # ❌ AttributeError: 'numpy.ndarray' object has no attribute 'detach'\n```\n\n---\n\n### **5. 最佳实践总结**\n| 操作类型          | 推荐写法                                  | 说明                          |\n|-------------------|------------------------------------------|------------------------------|\n| CPU + 无梯度      | `tensor.numpy()`                         | 直接转换                      |\n| CPU + 有梯度      | `tensor.detach().numpy()`                | 必须 detach                   |\n| GPU + 无梯度      | `tensor.cpu().numpy()`                   | 需要转移到 CPU                |\n| GPU + 有梯度      | `tensor.detach().cpu().numpy()`          | 完整流程                      |\n\n---\n\n### **6. 特殊场景处理**\n#### **保留梯度但需要可视化**\n如果需要在可视化后继续梯度计算（罕见需求）：\n```python\n# 使用 with torch.no_grad(): 临时禁用梯度\nwith torch.no_grad():\n    plt.plot(x.cpu().numpy(), y.cpu().numpy())\n```\n\n#### **批量处理张量**\n对于高维张量（如神经网络中间层输出）：\n```python\n# 假设 feature_map 是 4D 张量 (batch, channel, height, width)\nfeature_map = model(inputs)\nplt.imshow(feature_map[0, 0].detach().cpu().numpy())  # 可视化第一个样本的第一个通道\n```\n\n---\n\n### **总结**\n`.detach().numpy()`（对于 GPU 张量还需 `.cpu()`）的组合操作是 PyTorch 与 Matplotlib 协作的 **必要桥梁**，其主要作用包括：\n1. **断开梯度传播**：防止可视化操作影响反向传播\n2. **设备转移**：确保数据位于 CPU 内存\n3. **格式转换**：将张量转换为 Matplotlib 可识别的 NumPy 数组\n\n这种转换虽然增加了代码的复杂度，但能有效避免许多隐蔽的错误，是 PyTorch 可视化过程中必须掌握的关键技巧。\n","source":"_posts/深度学习/深度学习python库.md","raw":"---\ntitle: 深度学习python库\ntags: \n- 深度学习\n- python\ncategories: \n- [深度学习]\n- [python]\n---\n# numpy\n```python\nimport numpy as np\n```\n## 变量及基本操作\n### 矩阵\n```python\nlist1 = [  \n    [1, 2, 3, 4],  \n    [5, 6, 7, 8],  \n    [9, 10, 11, 12],  \n    [13, 14, 15, 16]  \n]\n\narray1 = np.array(list1) # 将列表转化为矩阵，打印的效果会不一样\n\narray2 = array1\n\n# 矩阵合并，默认axis = 0, 纵向合并。 axis = 1为横向合并，以此类推\narray3 = np.concatenate((array1, array2), axis = 1)\n```\n#### 矩阵的切片\n```python\nprint(array3[1:3, 2:4]) # 可以在两个维度进行切片\n\n# 也可以跳着切\nlist2 = [1, 3]\nprint(array3[:, list2])\n```\n\n# torch\n\n## 变量及基本操作\n\n### 张量 tensor\n```python\ntensor1 = torch.tensor(list1)  \n  \nprint(array1)  \nprint(tensor1)\n```\n输出如下： tensor与array的区别仅在于tensor将array放在了一张张量网上，可以进行梯度计算\n![](Pastedimage20250319161350.png)\n\n### 梯度计算\n```python\nx = torch.tensor(3.0)\nx.requires_grad_(True)\ny = x**2\ny.backward() # 2x = 6\n\n# 每个变量的梯度会在变量中进行记录，如果对一个变量分别求两次梯度，需进行清零。否则会出现梯度累加，导致计算错误，如本例中不进行清零x的梯度将为6 + 6 = 12\nx.grad = torch.tensor(0.0)\ny2 = x**2\ny2.backward() #2x = 6\n\nx.detach() # 将x在张量网上摘下，不再计算梯度\n```\n\n### 创建张量\n```python\ntensor1 = torch.ones((10,1)) # 创建一个10行1列全是1的张量\ntensor0 = torch.zeros((10,1)) # 创建一个10行1列全是0的张量\ntensor2 = torch.normal(mean, std, shape) # 创建一个形状为shape，均值为mean，标准差为std的正态分布的张量\n# ep\ntensor2 = torch.normal(0, 0.01, (3, 10, 4)) # 均值为0，标准差为0.01，形状3维10行4列\n```\n\n### 张量求和\n```python\nsum1 = torch.sum(tensor1, dim = 0 or 1, keepdim = True) # 0按列求和，1按行求和; keepdim保持原shape打印\n```\n\n### 张量形状\n```python\ntensor.shape\n```\n\n## tensor.backward()\n在PyTorch中，`tensor.backward()` 是实现自动微分（Autograd）的核心方法。它的主要作用是计算当前张量相对于某个标量值的梯度，并存储在相应张量的 `.grad` 属性中。以下是关键要点：\n\n1. **基本用法**：\n```python\nx = torch.tensor(2.0, requires_grad=True)\ny = x ** 2\ny.backward()  # 计算梯度\nprint(x.grad)  # 输出：tensor(4.)\n```\n\n2. **工作原理**：\n- 构建计算图：所有涉及`requires_grad=True`张量的运算都会记录在动态计算图中\n- 反向传播：从调用`backward()`的张量开始，反向遍历计算图应用链式法则\n- 梯度累积：结果梯度会累加到叶子节点的`.grad`属性中\n\n3. **参数说明**：\n```python\n# 当输出是非标量时需指定梯度权重\nx = torch.randn(3, requires_grad=True)\ny = x * 2\ny.backward(gradient=torch.tensor([1., 1., 1.]))  # 等效于y.sum().backward()\n\n# 保留计算图结构（用于多次反向传播）\nloss.backward(retain_graph=True)\n```\n\n4. **典型应用场景**：\n```python\n# 训练循环中的典型用法\nfor data, target in dataloader:\n    optimizer.zero_grad()  # 清空梯度\n    output = model(data)\n    loss = loss_fn(output, target)\n    loss.backward()        # 计算梯度\n    optimizer.step()       # 更新参数\n```\n\n### 注意事项：\n- 梯度会自动累积，需手动调用`zero_grad()`清除\n- 默认会释放计算图，二次调用`backward()`需设置`retain_graph=True`\n- 只能对标量值直接调用`backward()`，多维张量需提供`gradient`参数\n以下是关于PyTorch中`tensor.backward()`两个关键机制的深入解释：\n\n---\n\n#### 1. **默认释放计算图与`retain_graph=True`的作用**\n##### 原理\nPyTorch的自动微分系统基于动态计算图（Dynamic Computation Graph）。当调用`backward()`时：\n- **默认行为**：反向传播完成后，计算图会被**立即释放**以节省内存。\n- **问题**：如果尝试再次调用`backward()`，由于计算图已销毁，会抛出`RuntimeError`。\n\n##### 示例\n```python\nx = torch.tensor(2.0, requires_grad=True)\ny = x ** 3\n\n# 第一次反向传播（正常执行）\ny.backward()  \nprint(x.grad)  # 输出: tensor(12.)\n\n# 第二次反向传播（会报错）\ny.backward()  # ❌ RuntimeError: Trying to backward through the graph a second time\n```\n\n##### 解决方案\n通过`retain_graph=True`保留计算图：\n```python\ny.backward(retain_graph=True)  # 第一次反向传播保留计算图\ny.backward()                   # ✅ 可再次执行\nprint(x.grad)                  # 梯度累积为 12 + 12 = 24\n```\n\n##### 注意\n- **内存开销**：保留计算图会增加内存占用，需谨慎使用\n- **应用场景**：如GAN需要交替训练生成器和判别器时\n- **替代方案**：使用`torch.autograd.grad()`直接计算梯度（不修改`.grad`属性）\n\n---\n\n#### 2. **标量限制与`gradient`参数的作用**\n##### 原理\n- **标量限制**：数学上梯度定义为**标量函数对张量的导数**。当输出是非标量时，PyTorch无法确定如何聚合多维输出的梯度。\n- **gradient参数**：本质是一个权重向量，用于计算**加权和的梯度**（相当于`torch.sum(gradient * output)`）。\n\n##### 示例\n```python\nx = torch.tensor([1.0, 2.0], requires_grad=True)\ny = x * 2  # y = [2.0, 4.0]\n\n# ❌ 直接调用会报错\ny.backward()  # RuntimeError: grad can be implicitly created only for scalar outputs\n\n# ✅ 正确方式：提供gradient参数\ny.backward(gradient=torch.tensor([1., 1.]))  \nprint(x.grad)  # 输出: tensor([2., 2.]) \n               # 计算过程：d(sum(y))/dx = d(2x1 + 2x2)/dx = [2, 2]\n```\n\n##### 数学等价性\n```python\n# 以下两种写法等价\ny.backward(gradient=torch.tensor([1., 1.]))\n# 等价于\ny.sum().backward()\n```\n\n##### 高级用法\n```python\n# 自定义权重计算梯度\ngradient_weights = torch.tensor([0.5, 2.0])\ny.backward(gradient=gradient_weights)\nprint(x.grad)  # 输出: tensor([1., 4.]) \n               # 计算：0.5*2 + 2.0*2 = [1, 4]\n```\n\n##### 注意\n- **默认行为**：当输出是标量时，等价于`gradient=torch.tensor(1.0)`\n- **广播机制**：`gradient`的形状必须与输出张量形状匹配\n- **物理意义**：可理解为对多维输出不同通道的梯度重要性加权\n\n### 梯度计算解牛\n\n#### **1. 梯度计算的本质**\nPyTorch的梯度计算本质上是 **逐元素（element-wise）** 进行的。对于任意形状的张量：\n- 梯度张量与原张量形状 **严格一致**\n- 每个元素的梯度表示 **该元素对最终标量损失值的贡献**\n\n##### 示例代码中的 `w_0`\n```python\ntrue_w = torch.tensor([10.0, 7.0, 5.0, 2.0])  # 4维向量\nw_0 = torch.normal(0, 0.01, true_w.shape, requires_grad=True)  # 形状为 (4,)\n```\n\n当调用 `loss.backward()` 时：\n- 系统会计算损失值对 `w_0` 的 **每个元素** 的偏导数\n- 最终 `w_0.grad` 也会是一个形状为 `(4,)` 的张量\n\n---\n\n#### **2. 计算过程可视化**\n假设当前 `w_0` 和梯度如下：\n```python\nw_0 = tensor([w1, w2, w3, w4], requires_grad=True)  # 实际值可能是 [0.1, -0.2, 0.05, 0.3]\ngrad = tensor([dw1, dw2, dw3, dw4])                # 例如 [1.2, 0.8, -0.5, 2.1]\n```\n\n参数更新操作：\n```python\nw_0 -= lr * w_0.grad\n# 等价于：\nw1_new = w1 - lr * dw1\nw2_new = w2 - lr * dw2\nw3_new = w3 - lr * dw3\nw4_new = w4 - lr * dw4\n```\n\n---\n\n#### **3. 数学推导验证**\n![](Snipaste_2025-03-20_14-56-59.jpg)\n\n---\n\n#### **4. 代码执行细节验证**\n在你的代码中：\n```python\ndef fun(x, w, b):\n    pred_y = torch.matmul(x, w) + b  # x形状(batch_size,4), w形状(4,)\n    return pred_y                    # 输出形状(batch_size,)\n\ndef maeLoss(pred_y, y):\n    return torch.sum(abs(pred_y-y))/len(y)  # 输出是标量\n```\n\n反向传播时：\n1. `loss` 是标量（关键！）\n2. 通过链式法则，PyTorch会自动计算标量损失对每个 `w_j` 的梯度\n3. 所有梯度按原始维度组织，存储在 `w_0.grad`\n\n---\n\n#### **5. 梯度形状验证实验**\n可以添加调试代码验证梯度形状：\n```python\nprint(w_0.shape)        # 输出: torch.Size([4])\nprint(w_0.grad.shape)   # 反向传播后会输出: torch.Size([4])\n```\n\n---\n\n#### **6. 与一维情况的对比**\n| 参数类型 | 参数形状 | 梯度形状 | 更新方式              |\n|----------|----------|----------|-----------------------|\n| 标量     | ()       | ()       | `w -= lr * w.grad`    |\n| 向量     | (4,)     | (4,)     | 逐元素更新            |\n| 矩阵     | (3,4)    | (3,4)    | 每个矩阵元素独立更新  |\n\n---\n\n#### **为什么没有维度冲突？**\n- **关键点**：损失函数最终输出始终是 **标量**（`maeLoss` 返回的是平均值）\n- **数学保证**：标量对任意维度张量的导数自然保持与原张量相同的形状\n- **PyTorch特性**：自动微分系统会处理任意维度的张量梯度计算\n\n---\n\n#### **常见误区澄清**\n误区：*\"多维张量需要特殊处理才能计算梯度\"*\n\n事实：\n- 只要最终损失是标量，PyTorch可以处理任意维度的参数梯度\n- 无论参数是标量、向量、矩阵还是高阶张量，梯度计算规则一致\n- 参数更新时的逐元素操作是自动完成的\n\n# matplotlib.pyplot\n\n## plt.plot()\n在使用 PyTorch 进行深度学习训练时，如果要将张量数据传递给 `matplotlib.pyplot.plot()` 进行可视化，通常需要添加 `.detach().numpy()` 操作。这是由 PyTorch 张量和 Matplotlib 的底层机制差异导致的，具体原因如下：\n\n---\n\n### **1. 核心原因：数据格式转换**\n| 操作步骤          | 作用                                                                                     | 必要性 |\n|--------------------|------------------------------------------------------------------------------------------|--------|\n| `.detach()`        | 将张量从计算图中分离，得到一个不需要梯度追踪的新张量                                      | 必要   |\n| `.numpy()`         | 将 PyTorch 张量转换为 NumPy 数组（Matplotlib 只能处理 NumPy 数组或 Python 原生数据类型） | 必要   |\n\n---\n\n### **2. 分步详解**\n#### **（1）脱离计算图（.detach()）**\n- **问题背景**：PyTorch 张量可能带有梯度信息（`requires_grad=True`）\n- **风险**：如果直接使用带有梯度的张量：\n  - 会增加不必要的内存占用（保持计算图）\n  - 可能引发意外的梯度传播（尽管绘图操作不需要梯度）\n- **示例对比**：\n  ```python\n  # 原始张量（带梯度）\n  tensor_with_grad = torch.tensor([1.0, 2.0], requires_grad=True)\n  \n  # 直接转换会报错\n  try:\n      plt.plot(tensor_with_grad)  # ❌ 报错：Can't call numpy() on Tensor that requires grad\n  except Exception as e:\n      print(e)\n  \n  # 正确做法\n  plt.plot(tensor_with_grad.detach().numpy())  # ✅ 正常工作\n  ```\n\n#### **（2）设备转移（CPU/GPU）**\n- **问题背景**：如果张量在 GPU 上（`device='cuda'`）\n- **风险**：Matplotlib 无法直接处理 GPU 上的张量\n- **完整转换流程**：\n  ```python\n  # GPU 张量处理流程\n  gpu_tensor = torch.tensor([1.0, 2.0], device='cuda')\n  \n  # 错误方式\n  plt.plot(gpu_tensor.cpu().detach().numpy())  # ❌ 顺序错误，应先 detach\n  \n  # 正确方式\n  plt.plot(gpu_tensor.detach().cpu().numpy())  # ✅ 正确顺序：detach → cpu → numpy\n  ```\n\n#### **（3）数据类型转换**\n| 数据类型           | 说明                                                                 |\n|--------------------|----------------------------------------------------------------------|\n| PyTorch Tensor     | 可以是任意形状和数据类型（float32, int64 等）                        |\n| NumPy Array        | Matplotlib 的底层数据容器，与 PyTorch 内存不兼容                     |\n\n---\n\n### **3. 完整转换流程**\n```python\n# 假设有一个需要绘制的 PyTorch 张量\noriginal_tensor = torch.randn(100, requires_grad=True, device='cuda')\n\n# 安全转换步骤\nplot_data = original_tensor.detach()  # 1. 脱离计算图\n                .cpu()                # 2. 转移到 CPU（如果是 GPU 张量）\n                .numpy()              # 3. 转换为 NumPy 数组\n\n# 绘制图形\nplt.plot(plot_data)\nplt.show()\n```\n\n---\n\n### **4. 常见错误场景**\n#### **场景 1：未分离计算图**\n```python\nx = torch.linspace(0, 2*np.pi, 100, requires_grad=True)\ny = torch.sin(x)\n\nplt.plot(x.numpy(), y.numpy())  # ❌ RuntimeError: Can't call numpy() on Tensor that requires grad\n```\n\n#### **场景 2：未处理 GPU 张量**\n```python\ngpu_data = torch.randn(10).cuda()\nplt.plot(gpu_data.detach().numpy())  # ❌ TypeError: can't convert cuda:0 device type tensor to numpy\n```\n\n#### **场景 3：错误操作顺序**\n```python\n# 先转 NumPy 再 detach 会丢失梯度信息\ntemp = y.numpy()         # ❌ 错误开始点\ndetached = temp.detach() # ❌ AttributeError: 'numpy.ndarray' object has no attribute 'detach'\n```\n\n---\n\n### **5. 最佳实践总结**\n| 操作类型          | 推荐写法                                  | 说明                          |\n|-------------------|------------------------------------------|------------------------------|\n| CPU + 无梯度      | `tensor.numpy()`                         | 直接转换                      |\n| CPU + 有梯度      | `tensor.detach().numpy()`                | 必须 detach                   |\n| GPU + 无梯度      | `tensor.cpu().numpy()`                   | 需要转移到 CPU                |\n| GPU + 有梯度      | `tensor.detach().cpu().numpy()`          | 完整流程                      |\n\n---\n\n### **6. 特殊场景处理**\n#### **保留梯度但需要可视化**\n如果需要在可视化后继续梯度计算（罕见需求）：\n```python\n# 使用 with torch.no_grad(): 临时禁用梯度\nwith torch.no_grad():\n    plt.plot(x.cpu().numpy(), y.cpu().numpy())\n```\n\n#### **批量处理张量**\n对于高维张量（如神经网络中间层输出）：\n```python\n# 假设 feature_map 是 4D 张量 (batch, channel, height, width)\nfeature_map = model(inputs)\nplt.imshow(feature_map[0, 0].detach().cpu().numpy())  # 可视化第一个样本的第一个通道\n```\n\n---\n\n### **总结**\n`.detach().numpy()`（对于 GPU 张量还需 `.cpu()`）的组合操作是 PyTorch 与 Matplotlib 协作的 **必要桥梁**，其主要作用包括：\n1. **断开梯度传播**：防止可视化操作影响反向传播\n2. **设备转移**：确保数据位于 CPU 内存\n3. **格式转换**：将张量转换为 Matplotlib 可识别的 NumPy 数组\n\n这种转换虽然增加了代码的复杂度，但能有效避免许多隐蔽的错误，是 PyTorch 可视化过程中必须掌握的关键技巧。\n","slug":"深度学习/深度学习python库","published":1,"date":"2025-03-20T04:02:36.621Z","updated":"2025-03-20T06:57:50.693Z","comments":1,"layout":"post","photos":[],"_id":"cm8h01uzr0008ekeqdc786d5u","content":"<h1 id=\"numpy\"><a href=\"#numpy\" class=\"headerlink\" title=\"numpy\"></a>numpy</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br></pre></td></tr></table></figure>\n<h2 id=\"变量及基本操作\"><a href=\"#变量及基本操作\" class=\"headerlink\" title=\"变量及基本操作\"></a>变量及基本操作</h2><h3 id=\"矩阵\"><a href=\"#矩阵\" class=\"headerlink\" title=\"矩阵\"></a>矩阵</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list1 = [  </span><br><span class=\"line\">    [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>],  </span><br><span class=\"line\">    [<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>],  </span><br><span class=\"line\">    [<span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>],  </span><br><span class=\"line\">    [<span class=\"number\">13</span>, <span class=\"number\">14</span>, <span class=\"number\">15</span>, <span class=\"number\">16</span>]  </span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\">array1 = np.array(list1) <span class=\"comment\"># 将列表转化为矩阵，打印的效果会不一样</span></span><br><span class=\"line\"></span><br><span class=\"line\">array2 = array1</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 矩阵合并，默认axis = 0, 纵向合并。 axis = 1为横向合并，以此类推</span></span><br><span class=\"line\">array3 = np.concatenate((array1, array2), axis = <span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<h4 id=\"矩阵的切片\"><a href=\"#矩阵的切片\" class=\"headerlink\" title=\"矩阵的切片\"></a>矩阵的切片</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(array3[<span class=\"number\">1</span>:<span class=\"number\">3</span>, <span class=\"number\">2</span>:<span class=\"number\">4</span>]) <span class=\"comment\"># 可以在两个维度进行切片</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 也可以跳着切</span></span><br><span class=\"line\">list2 = [<span class=\"number\">1</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(array3[:, list2])</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"torch\"><a href=\"#torch\" class=\"headerlink\" title=\"torch\"></a>torch</h1><h2 id=\"变量及基本操作-1\"><a href=\"#变量及基本操作-1\" class=\"headerlink\" title=\"变量及基本操作\"></a>变量及基本操作</h2><h3 id=\"张量-tensor\"><a href=\"#张量-tensor\" class=\"headerlink\" title=\"张量 tensor\"></a>张量 tensor</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor1 = torch.tensor(list1)  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(array1)  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(tensor1)</span><br></pre></td></tr></table></figure>\n<p>输出如下： tensor与array的区别仅在于tensor将array放在了一张张量网上，可以进行梯度计算<br><img src=\"/2025/03/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0python%E5%BA%93/Pastedimage20250319161350.png\"></p>\n<h3 id=\"梯度计算\"><a href=\"#梯度计算\" class=\"headerlink\" title=\"梯度计算\"></a>梯度计算</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor(<span class=\"number\">3.0</span>)</span><br><span class=\"line\">x.requires_grad_(<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = x**<span class=\"number\">2</span></span><br><span class=\"line\">y.backward() <span class=\"comment\"># 2x = 6</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 每个变量的梯度会在变量中进行记录，如果对一个变量分别求两次梯度，需进行清零。否则会出现梯度累加，导致计算错误，如本例中不进行清零x的梯度将为6 + 6 = 12</span></span><br><span class=\"line\">x.grad = torch.tensor(<span class=\"number\">0.0</span>)</span><br><span class=\"line\">y2 = x**<span class=\"number\">2</span></span><br><span class=\"line\">y2.backward() <span class=\"comment\">#2x = 6</span></span><br><span class=\"line\"></span><br><span class=\"line\">x.detach() <span class=\"comment\"># 将x在张量网上摘下，不再计算梯度</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"创建张量\"><a href=\"#创建张量\" class=\"headerlink\" title=\"创建张量\"></a>创建张量</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor1 = torch.ones((<span class=\"number\">10</span>,<span class=\"number\">1</span>)) <span class=\"comment\"># 创建一个10行1列全是1的张量</span></span><br><span class=\"line\">tensor0 = torch.zeros((<span class=\"number\">10</span>,<span class=\"number\">1</span>)) <span class=\"comment\"># 创建一个10行1列全是0的张量</span></span><br><span class=\"line\">tensor2 = torch.normal(mean, std, shape) <span class=\"comment\"># 创建一个形状为shape，均值为mean，标准差为std的正态分布的张量</span></span><br><span class=\"line\"><span class=\"comment\"># ep</span></span><br><span class=\"line\">tensor2 = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, (<span class=\"number\">3</span>, <span class=\"number\">10</span>, <span class=\"number\">4</span>)) <span class=\"comment\"># 均值为0，标准差为0.01，形状3维10行4列</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"张量求和\"><a href=\"#张量求和\" class=\"headerlink\" title=\"张量求和\"></a>张量求和</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sum1 = torch.<span class=\"built_in\">sum</span>(tensor1, dim = <span class=\"number\">0</span> <span class=\"keyword\">or</span> <span class=\"number\">1</span>, keepdim = <span class=\"literal\">True</span>) <span class=\"comment\"># 0按列求和，1按行求和; keepdim保持原shape打印</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"张量形状\"><a href=\"#张量形状\" class=\"headerlink\" title=\"张量形状\"></a>张量形状</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor.shape</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"tensor-backward\"><a href=\"#tensor-backward\" class=\"headerlink\" title=\"tensor.backward()\"></a>tensor.backward()</h2><p>在PyTorch中，<code>tensor.backward()</code> 是实现自动微分（Autograd）的核心方法。它的主要作用是计算当前张量相对于某个标量值的梯度，并存储在相应张量的 <code>.grad</code> 属性中。以下是关键要点：</p>\n<ol>\n<li><strong>基本用法</strong>：</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor(<span class=\"number\">2.0</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = x ** <span class=\"number\">2</span></span><br><span class=\"line\">y.backward()  <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)  <span class=\"comment\"># 输出：tensor(4.)</span></span><br></pre></td></tr></table></figure>\n\n<ol start=\"2\">\n<li><strong>工作原理</strong>：</li>\n</ol>\n<ul>\n<li>构建计算图：所有涉及<code>requires_grad=True</code>张量的运算都会记录在动态计算图中</li>\n<li>反向传播：从调用<code>backward()</code>的张量开始，反向遍历计算图应用链式法则</li>\n<li>梯度累积：结果梯度会累加到叶子节点的<code>.grad</code>属性中</li>\n</ul>\n<ol start=\"3\">\n<li><strong>参数说明</strong>：</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 当输出是非标量时需指定梯度权重</span></span><br><span class=\"line\">x = torch.randn(<span class=\"number\">3</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = x * <span class=\"number\">2</span></span><br><span class=\"line\">y.backward(gradient=torch.tensor([<span class=\"number\">1.</span>, <span class=\"number\">1.</span>, <span class=\"number\">1.</span>]))  <span class=\"comment\"># 等效于y.sum().backward()</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保留计算图结构（用于多次反向传播）</span></span><br><span class=\"line\">loss.backward(retain_graph=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<ol start=\"4\">\n<li><strong>典型应用场景</strong>：</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 训练循环中的典型用法</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> data, target <span class=\"keyword\">in</span> dataloader:</span><br><span class=\"line\">    optimizer.zero_grad()  <span class=\"comment\"># 清空梯度</span></span><br><span class=\"line\">    output = model(data)</span><br><span class=\"line\">    loss = loss_fn(output, target)</span><br><span class=\"line\">    loss.backward()        <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">    optimizer.step()       <span class=\"comment\"># 更新参数</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"注意事项：\"><a href=\"#注意事项：\" class=\"headerlink\" title=\"注意事项：\"></a>注意事项：</h3><ul>\n<li>梯度会自动累积，需手动调用<code>zero_grad()</code>清除</li>\n<li>默认会释放计算图，二次调用<code>backward()</code>需设置<code>retain_graph=True</code></li>\n<li>只能对标量值直接调用<code>backward()</code>，多维张量需提供<code>gradient</code>参数<br>以下是关于PyTorch中<code>tensor.backward()</code>两个关键机制的深入解释：</li>\n</ul>\n<hr>\n<h4 id=\"1-默认释放计算图与retain-graph-True的作用\"><a href=\"#1-默认释放计算图与retain-graph-True的作用\" class=\"headerlink\" title=\"1. 默认释放计算图与retain_graph=True的作用\"></a>1. <strong>默认释放计算图与<code>retain_graph=True</code>的作用</strong></h4><h5 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h5><p>PyTorch的自动微分系统基于动态计算图（Dynamic Computation Graph）。当调用<code>backward()</code>时：</p>\n<ul>\n<li><strong>默认行为</strong>：反向传播完成后，计算图会被<strong>立即释放</strong>以节省内存。</li>\n<li><strong>问题</strong>：如果尝试再次调用<code>backward()</code>，由于计算图已销毁，会抛出<code>RuntimeError</code>。</li>\n</ul>\n<h5 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor(<span class=\"number\">2.0</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = x ** <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 第一次反向传播（正常执行）</span></span><br><span class=\"line\">y.backward()  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)  <span class=\"comment\"># 输出: tensor(12.)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 第二次反向传播（会报错）</span></span><br><span class=\"line\">y.backward()  <span class=\"comment\"># ❌ RuntimeError: Trying to backward through the graph a second time</span></span><br></pre></td></tr></table></figure>\n\n<h5 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h5><p>通过<code>retain_graph=True</code>保留计算图：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y.backward(retain_graph=<span class=\"literal\">True</span>)  <span class=\"comment\"># 第一次反向传播保留计算图</span></span><br><span class=\"line\">y.backward()                   <span class=\"comment\"># ✅ 可再次执行</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)                  <span class=\"comment\"># 梯度累积为 12 + 12 = 24</span></span><br></pre></td></tr></table></figure>\n\n<h5 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h5><ul>\n<li><strong>内存开销</strong>：保留计算图会增加内存占用，需谨慎使用</li>\n<li><strong>应用场景</strong>：如GAN需要交替训练生成器和判别器时</li>\n<li><strong>替代方案</strong>：使用<code>torch.autograd.grad()</code>直接计算梯度（不修改<code>.grad</code>属性）</li>\n</ul>\n<hr>\n<h4 id=\"2-标量限制与gradient参数的作用\"><a href=\"#2-标量限制与gradient参数的作用\" class=\"headerlink\" title=\"2. 标量限制与gradient参数的作用\"></a>2. <strong>标量限制与<code>gradient</code>参数的作用</strong></h4><h5 id=\"原理-1\"><a href=\"#原理-1\" class=\"headerlink\" title=\"原理\"></a>原理</h5><ul>\n<li><strong>标量限制</strong>：数学上梯度定义为<strong>标量函数对张量的导数</strong>。当输出是非标量时，PyTorch无法确定如何聚合多维输出的梯度。</li>\n<li><strong>gradient参数</strong>：本质是一个权重向量，用于计算<strong>加权和的梯度</strong>（相当于<code>torch.sum(gradient * output)</code>）。</li>\n</ul>\n<h5 id=\"示例-1\"><a href=\"#示例-1\" class=\"headerlink\" title=\"示例\"></a>示例</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = x * <span class=\"number\">2</span>  <span class=\"comment\"># y = [2.0, 4.0]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># ❌ 直接调用会报错</span></span><br><span class=\"line\">y.backward()  <span class=\"comment\"># RuntimeError: grad can be implicitly created only for scalar outputs</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># ✅ 正确方式：提供gradient参数</span></span><br><span class=\"line\">y.backward(gradient=torch.tensor([<span class=\"number\">1.</span>, <span class=\"number\">1.</span>]))  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)  <span class=\"comment\"># 输出: tensor([2., 2.]) </span></span><br><span class=\"line\">               <span class=\"comment\"># 计算过程：d(sum(y))/dx = d(2x1 + 2x2)/dx = [2, 2]</span></span><br></pre></td></tr></table></figure>\n\n<h5 id=\"数学等价性\"><a href=\"#数学等价性\" class=\"headerlink\" title=\"数学等价性\"></a>数学等价性</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 以下两种写法等价</span></span><br><span class=\"line\">y.backward(gradient=torch.tensor([<span class=\"number\">1.</span>, <span class=\"number\">1.</span>]))</span><br><span class=\"line\"><span class=\"comment\"># 等价于</span></span><br><span class=\"line\">y.<span class=\"built_in\">sum</span>().backward()</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"高级用法\"><a href=\"#高级用法\" class=\"headerlink\" title=\"高级用法\"></a>高级用法</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 自定义权重计算梯度</span></span><br><span class=\"line\">gradient_weights = torch.tensor([<span class=\"number\">0.5</span>, <span class=\"number\">2.0</span>])</span><br><span class=\"line\">y.backward(gradient=gradient_weights)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)  <span class=\"comment\"># 输出: tensor([1., 4.]) </span></span><br><span class=\"line\">               <span class=\"comment\"># 计算：0.5*2 + 2.0*2 = [1, 4]</span></span><br></pre></td></tr></table></figure>\n\n<h5 id=\"注意-1\"><a href=\"#注意-1\" class=\"headerlink\" title=\"注意\"></a>注意</h5><ul>\n<li><strong>默认行为</strong>：当输出是标量时，等价于<code>gradient=torch.tensor(1.0)</code></li>\n<li><strong>广播机制</strong>：<code>gradient</code>的形状必须与输出张量形状匹配</li>\n<li><strong>物理意义</strong>：可理解为对多维输出不同通道的梯度重要性加权</li>\n</ul>\n<h3 id=\"梯度计算解牛\"><a href=\"#梯度计算解牛\" class=\"headerlink\" title=\"梯度计算解牛\"></a>梯度计算解牛</h3><h4 id=\"1-梯度计算的本质\"><a href=\"#1-梯度计算的本质\" class=\"headerlink\" title=\"1. 梯度计算的本质\"></a><strong>1. 梯度计算的本质</strong></h4><p>PyTorch的梯度计算本质上是 <strong>逐元素（element-wise）</strong> 进行的。对于任意形状的张量：</p>\n<ul>\n<li>梯度张量与原张量形状 <strong>严格一致</strong></li>\n<li>每个元素的梯度表示 <strong>该元素对最终标量损失值的贡献</strong></li>\n</ul>\n<h5 id=\"示例代码中的-w-0\"><a href=\"#示例代码中的-w-0\" class=\"headerlink\" title=\"示例代码中的 w_0\"></a>示例代码中的 <code>w_0</code></h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">true_w = torch.tensor([<span class=\"number\">10.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">5.0</span>, <span class=\"number\">2.0</span>])  <span class=\"comment\"># 4维向量</span></span><br><span class=\"line\">w_0 = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, true_w.shape, requires_grad=<span class=\"literal\">True</span>)  <span class=\"comment\"># 形状为 (4,)</span></span><br></pre></td></tr></table></figure>\n\n<p>当调用 <code>loss.backward()</code> 时：</p>\n<ul>\n<li>系统会计算损失值对 <code>w_0</code> 的 <strong>每个元素</strong> 的偏导数</li>\n<li>最终 <code>w_0.grad</code> 也会是一个形状为 <code>(4,)</code> 的张量</li>\n</ul>\n<hr>\n<h4 id=\"2-计算过程可视化\"><a href=\"#2-计算过程可视化\" class=\"headerlink\" title=\"2. 计算过程可视化\"></a><strong>2. 计算过程可视化</strong></h4><p>假设当前 <code>w_0</code> 和梯度如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w_0 = tensor([w1, w2, w3, w4], requires_grad=<span class=\"literal\">True</span>)  <span class=\"comment\"># 实际值可能是 [0.1, -0.2, 0.05, 0.3]</span></span><br><span class=\"line\">grad = tensor([dw1, dw2, dw3, dw4])                <span class=\"comment\"># 例如 [1.2, 0.8, -0.5, 2.1]</span></span><br></pre></td></tr></table></figure>\n\n<p>参数更新操作：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w_0 -= lr * w_0.grad</span><br><span class=\"line\"><span class=\"comment\"># 等价于：</span></span><br><span class=\"line\">w1_new = w1 - lr * dw1</span><br><span class=\"line\">w2_new = w2 - lr * dw2</span><br><span class=\"line\">w3_new = w3 - lr * dw3</span><br><span class=\"line\">w4_new = w4 - lr * dw4</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h4 id=\"3-数学推导验证\"><a href=\"#3-数学推导验证\" class=\"headerlink\" title=\"3. 数学推导验证\"></a><strong>3. 数学推导验证</strong></h4><p><img src=\"/2025/03/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0python%E5%BA%93/Snipaste_2025-03-20_14-56-59.jpg\"></p>\n<hr>\n<h4 id=\"4-代码执行细节验证\"><a href=\"#4-代码执行细节验证\" class=\"headerlink\" title=\"4. 代码执行细节验证\"></a><strong>4. 代码执行细节验证</strong></h4><p>在你的代码中：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">fun</span>(<span class=\"params\">x, w, b</span>):</span><br><span class=\"line\">    pred_y = torch.matmul(x, w) + b  <span class=\"comment\"># x形状(batch_size,4), w形状(4,)</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> pred_y                    <span class=\"comment\"># 输出形状(batch_size,)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">maeLoss</span>(<span class=\"params\">pred_y, y</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">sum</span>(<span class=\"built_in\">abs</span>(pred_y-y))/<span class=\"built_in\">len</span>(y)  <span class=\"comment\"># 输出是标量</span></span><br></pre></td></tr></table></figure>\n\n<p>反向传播时：</p>\n<ol>\n<li><code>loss</code> 是标量（关键！）</li>\n<li>通过链式法则，PyTorch会自动计算标量损失对每个 <code>w_j</code> 的梯度</li>\n<li>所有梯度按原始维度组织，存储在 <code>w_0.grad</code></li>\n</ol>\n<hr>\n<h4 id=\"5-梯度形状验证实验\"><a href=\"#5-梯度形状验证实验\" class=\"headerlink\" title=\"5. 梯度形状验证实验\"></a><strong>5. 梯度形状验证实验</strong></h4><p>可以添加调试代码验证梯度形状：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(w_0.shape)        <span class=\"comment\"># 输出: torch.Size([4])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(w_0.grad.shape)   <span class=\"comment\"># 反向传播后会输出: torch.Size([4])</span></span><br></pre></td></tr></table></figure>\n\n<hr>\n<h4 id=\"6-与一维情况的对比\"><a href=\"#6-与一维情况的对比\" class=\"headerlink\" title=\"6. 与一维情况的对比\"></a><strong>6. 与一维情况的对比</strong></h4><table>\n<thead>\n<tr>\n<th>参数类型</th>\n<th>参数形状</th>\n<th>梯度形状</th>\n<th>更新方式</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>标量</td>\n<td>()</td>\n<td>()</td>\n<td><code>w -= lr * w.grad</code></td>\n</tr>\n<tr>\n<td>向量</td>\n<td>(4,)</td>\n<td>(4,)</td>\n<td>逐元素更新</td>\n</tr>\n<tr>\n<td>矩阵</td>\n<td>(3,4)</td>\n<td>(3,4)</td>\n<td>每个矩阵元素独立更新</td>\n</tr>\n</tbody></table>\n<hr>\n<h4 id=\"为什么没有维度冲突？\"><a href=\"#为什么没有维度冲突？\" class=\"headerlink\" title=\"为什么没有维度冲突？\"></a><strong>为什么没有维度冲突？</strong></h4><ul>\n<li><strong>关键点</strong>：损失函数最终输出始终是 <strong>标量</strong>（<code>maeLoss</code> 返回的是平均值）</li>\n<li><strong>数学保证</strong>：标量对任意维度张量的导数自然保持与原张量相同的形状</li>\n<li><strong>PyTorch特性</strong>：自动微分系统会处理任意维度的张量梯度计算</li>\n</ul>\n<hr>\n<h4 id=\"常见误区澄清\"><a href=\"#常见误区澄清\" class=\"headerlink\" title=\"常见误区澄清\"></a><strong>常见误区澄清</strong></h4><p>误区：<em>“多维张量需要特殊处理才能计算梯度”</em></p>\n<p>事实：</p>\n<ul>\n<li>只要最终损失是标量，PyTorch可以处理任意维度的参数梯度</li>\n<li>无论参数是标量、向量、矩阵还是高阶张量，梯度计算规则一致</li>\n<li>参数更新时的逐元素操作是自动完成的</li>\n</ul>\n<h1 id=\"matplotlib-pyplot\"><a href=\"#matplotlib-pyplot\" class=\"headerlink\" title=\"matplotlib.pyplot\"></a>matplotlib.pyplot</h1><h2 id=\"plt-plot\"><a href=\"#plt-plot\" class=\"headerlink\" title=\"plt.plot()\"></a>plt.plot()</h2><p>在使用 PyTorch 进行深度学习训练时，如果要将张量数据传递给 <code>matplotlib.pyplot.plot()</code> 进行可视化，通常需要添加 <code>.detach().numpy()</code> 操作。这是由 PyTorch 张量和 Matplotlib 的底层机制差异导致的，具体原因如下：</p>\n<hr>\n<h3 id=\"1-核心原因：数据格式转换\"><a href=\"#1-核心原因：数据格式转换\" class=\"headerlink\" title=\"1. 核心原因：数据格式转换\"></a><strong>1. 核心原因：数据格式转换</strong></h3><table>\n<thead>\n<tr>\n<th>操作步骤</th>\n<th>作用</th>\n<th>必要性</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>.detach()</code></td>\n<td>将张量从计算图中分离，得到一个不需要梯度追踪的新张量</td>\n<td>必要</td>\n</tr>\n<tr>\n<td><code>.numpy()</code></td>\n<td>将 PyTorch 张量转换为 NumPy 数组（Matplotlib 只能处理 NumPy 数组或 Python 原生数据类型）</td>\n<td>必要</td>\n</tr>\n</tbody></table>\n<hr>\n<h3 id=\"2-分步详解\"><a href=\"#2-分步详解\" class=\"headerlink\" title=\"2. 分步详解\"></a><strong>2. 分步详解</strong></h3><h4 id=\"（1）脱离计算图（-detach-）\"><a href=\"#（1）脱离计算图（-detach-）\" class=\"headerlink\" title=\"（1）脱离计算图（.detach()）\"></a><strong>（1）脱离计算图（.detach()）</strong></h4><ul>\n<li><strong>问题背景</strong>：PyTorch 张量可能带有梯度信息（<code>requires_grad=True</code>）</li>\n<li><strong>风险</strong>：如果直接使用带有梯度的张量：<ul>\n<li>会增加不必要的内存占用（保持计算图）</li>\n<li>可能引发意外的梯度传播（尽管绘图操作不需要梯度）</li>\n</ul>\n</li>\n<li><strong>示例对比</strong>：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 原始张量（带梯度）</span></span><br><span class=\"line\">tensor_with_grad = torch.tensor([<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 直接转换会报错</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    plt.plot(tensor_with_grad)  <span class=\"comment\"># ❌ 报错：Can&#x27;t call numpy() on Tensor that requires grad</span></span><br><span class=\"line\"><span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(e)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 正确做法</span></span><br><span class=\"line\">plt.plot(tensor_with_grad.detach().numpy())  <span class=\"comment\"># ✅ 正常工作</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h4 id=\"（2）设备转移（CPU-GPU）\"><a href=\"#（2）设备转移（CPU-GPU）\" class=\"headerlink\" title=\"（2）设备转移（CPU&#x2F;GPU）\"></a><strong>（2）设备转移（CPU&#x2F;GPU）</strong></h4><ul>\n<li><strong>问题背景</strong>：如果张量在 GPU 上（<code>device=&#39;cuda&#39;</code>）</li>\n<li><strong>风险</strong>：Matplotlib 无法直接处理 GPU 上的张量</li>\n<li><strong>完整转换流程</strong>：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># GPU 张量处理流程</span></span><br><span class=\"line\">gpu_tensor = torch.tensor([<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], device=<span class=\"string\">&#x27;cuda&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 错误方式</span></span><br><span class=\"line\">plt.plot(gpu_tensor.cpu().detach().numpy())  <span class=\"comment\"># ❌ 顺序错误，应先 detach</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 正确方式</span></span><br><span class=\"line\">plt.plot(gpu_tensor.detach().cpu().numpy())  <span class=\"comment\"># ✅ 正确顺序：detach → cpu → numpy</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h4 id=\"（3）数据类型转换\"><a href=\"#（3）数据类型转换\" class=\"headerlink\" title=\"（3）数据类型转换\"></a><strong>（3）数据类型转换</strong></h4><table>\n<thead>\n<tr>\n<th>数据类型</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PyTorch Tensor</td>\n<td>可以是任意形状和数据类型（float32, int64 等）</td>\n</tr>\n<tr>\n<td>NumPy Array</td>\n<td>Matplotlib 的底层数据容器，与 PyTorch 内存不兼容</td>\n</tr>\n</tbody></table>\n<hr>\n<h3 id=\"3-完整转换流程\"><a href=\"#3-完整转换流程\" class=\"headerlink\" title=\"3. 完整转换流程\"></a><strong>3. 完整转换流程</strong></h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 假设有一个需要绘制的 PyTorch 张量</span></span><br><span class=\"line\">original_tensor = torch.randn(<span class=\"number\">100</span>, requires_grad=<span class=\"literal\">True</span>, device=<span class=\"string\">&#x27;cuda&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 安全转换步骤</span></span><br><span class=\"line\">plot_data = original_tensor.detach()  <span class=\"comment\"># 1. 脱离计算图</span></span><br><span class=\"line\">                .cpu()                <span class=\"comment\"># 2. 转移到 CPU（如果是 GPU 张量）</span></span><br><span class=\"line\">                .numpy()              <span class=\"comment\"># 3. 转换为 NumPy 数组</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制图形</span></span><br><span class=\"line\">plt.plot(plot_data)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h3 id=\"4-常见错误场景\"><a href=\"#4-常见错误场景\" class=\"headerlink\" title=\"4. 常见错误场景\"></a><strong>4. 常见错误场景</strong></h3><h4 id=\"场景-1：未分离计算图\"><a href=\"#场景-1：未分离计算图\" class=\"headerlink\" title=\"场景 1：未分离计算图\"></a><strong>场景 1：未分离计算图</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.linspace(<span class=\"number\">0</span>, <span class=\"number\">2</span>*np.pi, <span class=\"number\">100</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = torch.sin(x)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot(x.numpy(), y.numpy())  <span class=\"comment\"># ❌ RuntimeError: Can&#x27;t call numpy() on Tensor that requires grad</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"场景-2：未处理-GPU-张量\"><a href=\"#场景-2：未处理-GPU-张量\" class=\"headerlink\" title=\"场景 2：未处理 GPU 张量\"></a><strong>场景 2：未处理 GPU 张量</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gpu_data = torch.randn(<span class=\"number\">10</span>).cuda()</span><br><span class=\"line\">plt.plot(gpu_data.detach().numpy())  <span class=\"comment\"># ❌ TypeError: can&#x27;t convert cuda:0 device type tensor to numpy</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"场景-3：错误操作顺序\"><a href=\"#场景-3：错误操作顺序\" class=\"headerlink\" title=\"场景 3：错误操作顺序\"></a><strong>场景 3：错误操作顺序</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 先转 NumPy 再 detach 会丢失梯度信息</span></span><br><span class=\"line\">temp = y.numpy()         <span class=\"comment\"># ❌ 错误开始点</span></span><br><span class=\"line\">detached = temp.detach() <span class=\"comment\"># ❌ AttributeError: &#x27;numpy.ndarray&#x27; object has no attribute &#x27;detach&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<hr>\n<h3 id=\"5-最佳实践总结\"><a href=\"#5-最佳实践总结\" class=\"headerlink\" title=\"5. 最佳实践总结\"></a><strong>5. 最佳实践总结</strong></h3><table>\n<thead>\n<tr>\n<th>操作类型</th>\n<th>推荐写法</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU + 无梯度</td>\n<td><code>tensor.numpy()</code></td>\n<td>直接转换</td>\n</tr>\n<tr>\n<td>CPU + 有梯度</td>\n<td><code>tensor.detach().numpy()</code></td>\n<td>必须 detach</td>\n</tr>\n<tr>\n<td>GPU + 无梯度</td>\n<td><code>tensor.cpu().numpy()</code></td>\n<td>需要转移到 CPU</td>\n</tr>\n<tr>\n<td>GPU + 有梯度</td>\n<td><code>tensor.detach().cpu().numpy()</code></td>\n<td>完整流程</td>\n</tr>\n</tbody></table>\n<hr>\n<h3 id=\"6-特殊场景处理\"><a href=\"#6-特殊场景处理\" class=\"headerlink\" title=\"6. 特殊场景处理\"></a><strong>6. 特殊场景处理</strong></h3><h4 id=\"保留梯度但需要可视化\"><a href=\"#保留梯度但需要可视化\" class=\"headerlink\" title=\"保留梯度但需要可视化\"></a><strong>保留梯度但需要可视化</strong></h4><p>如果需要在可视化后继续梯度计算（罕见需求）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用 with torch.no_grad(): 临时禁用梯度</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    plt.plot(x.cpu().numpy(), y.cpu().numpy())</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"批量处理张量\"><a href=\"#批量处理张量\" class=\"headerlink\" title=\"批量处理张量\"></a><strong>批量处理张量</strong></h4><p>对于高维张量（如神经网络中间层输出）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 假设 feature_map 是 4D 张量 (batch, channel, height, width)</span></span><br><span class=\"line\">feature_map = model(inputs)</span><br><span class=\"line\">plt.imshow(feature_map[<span class=\"number\">0</span>, <span class=\"number\">0</span>].detach().cpu().numpy())  <span class=\"comment\"># 可视化第一个样本的第一个通道</span></span><br></pre></td></tr></table></figure>\n\n<hr>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a><strong>总结</strong></h3><p><code>.detach().numpy()</code>（对于 GPU 张量还需 <code>.cpu()</code>）的组合操作是 PyTorch 与 Matplotlib 协作的 <strong>必要桥梁</strong>，其主要作用包括：</p>\n<ol>\n<li><strong>断开梯度传播</strong>：防止可视化操作影响反向传播</li>\n<li><strong>设备转移</strong>：确保数据位于 CPU 内存</li>\n<li><strong>格式转换</strong>：将张量转换为 Matplotlib 可识别的 NumPy 数组</li>\n</ol>\n<p>这种转换虽然增加了代码的复杂度，但能有效避免许多隐蔽的错误，是 PyTorch 可视化过程中必须掌握的关键技巧。</p>\n","excerpt":"","more":"<h1 id=\"numpy\"><a href=\"#numpy\" class=\"headerlink\" title=\"numpy\"></a>numpy</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">import</span> numpy <span class=\"keyword\">as</span> np</span><br></pre></td></tr></table></figure>\n<h2 id=\"变量及基本操作\"><a href=\"#变量及基本操作\" class=\"headerlink\" title=\"变量及基本操作\"></a>变量及基本操作</h2><h3 id=\"矩阵\"><a href=\"#矩阵\" class=\"headerlink\" title=\"矩阵\"></a>矩阵</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">list1 = [  </span><br><span class=\"line\">    [<span class=\"number\">1</span>, <span class=\"number\">2</span>, <span class=\"number\">3</span>, <span class=\"number\">4</span>],  </span><br><span class=\"line\">    [<span class=\"number\">5</span>, <span class=\"number\">6</span>, <span class=\"number\">7</span>, <span class=\"number\">8</span>],  </span><br><span class=\"line\">    [<span class=\"number\">9</span>, <span class=\"number\">10</span>, <span class=\"number\">11</span>, <span class=\"number\">12</span>],  </span><br><span class=\"line\">    [<span class=\"number\">13</span>, <span class=\"number\">14</span>, <span class=\"number\">15</span>, <span class=\"number\">16</span>]  </span><br><span class=\"line\">]</span><br><span class=\"line\"></span><br><span class=\"line\">array1 = np.array(list1) <span class=\"comment\"># 将列表转化为矩阵，打印的效果会不一样</span></span><br><span class=\"line\"></span><br><span class=\"line\">array2 = array1</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 矩阵合并，默认axis = 0, 纵向合并。 axis = 1为横向合并，以此类推</span></span><br><span class=\"line\">array3 = np.concatenate((array1, array2), axis = <span class=\"number\">1</span>)</span><br></pre></td></tr></table></figure>\n<h4 id=\"矩阵的切片\"><a href=\"#矩阵的切片\" class=\"headerlink\" title=\"矩阵的切片\"></a>矩阵的切片</h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(array3[<span class=\"number\">1</span>:<span class=\"number\">3</span>, <span class=\"number\">2</span>:<span class=\"number\">4</span>]) <span class=\"comment\"># 可以在两个维度进行切片</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 也可以跳着切</span></span><br><span class=\"line\">list2 = [<span class=\"number\">1</span>, <span class=\"number\">3</span>]</span><br><span class=\"line\"><span class=\"built_in\">print</span>(array3[:, list2])</span><br></pre></td></tr></table></figure>\n\n<h1 id=\"torch\"><a href=\"#torch\" class=\"headerlink\" title=\"torch\"></a>torch</h1><h2 id=\"变量及基本操作-1\"><a href=\"#变量及基本操作-1\" class=\"headerlink\" title=\"变量及基本操作\"></a>变量及基本操作</h2><h3 id=\"张量-tensor\"><a href=\"#张量-tensor\" class=\"headerlink\" title=\"张量 tensor\"></a>张量 tensor</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor1 = torch.tensor(list1)  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(array1)  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(tensor1)</span><br></pre></td></tr></table></figure>\n<p>输出如下： tensor与array的区别仅在于tensor将array放在了一张张量网上，可以进行梯度计算<br><img src=\"/2025/03/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0python%E5%BA%93/Pastedimage20250319161350.png\"></p>\n<h3 id=\"梯度计算\"><a href=\"#梯度计算\" class=\"headerlink\" title=\"梯度计算\"></a>梯度计算</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor(<span class=\"number\">3.0</span>)</span><br><span class=\"line\">x.requires_grad_(<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = x**<span class=\"number\">2</span></span><br><span class=\"line\">y.backward() <span class=\"comment\"># 2x = 6</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 每个变量的梯度会在变量中进行记录，如果对一个变量分别求两次梯度，需进行清零。否则会出现梯度累加，导致计算错误，如本例中不进行清零x的梯度将为6 + 6 = 12</span></span><br><span class=\"line\">x.grad = torch.tensor(<span class=\"number\">0.0</span>)</span><br><span class=\"line\">y2 = x**<span class=\"number\">2</span></span><br><span class=\"line\">y2.backward() <span class=\"comment\">#2x = 6</span></span><br><span class=\"line\"></span><br><span class=\"line\">x.detach() <span class=\"comment\"># 将x在张量网上摘下，不再计算梯度</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"创建张量\"><a href=\"#创建张量\" class=\"headerlink\" title=\"创建张量\"></a>创建张量</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor1 = torch.ones((<span class=\"number\">10</span>,<span class=\"number\">1</span>)) <span class=\"comment\"># 创建一个10行1列全是1的张量</span></span><br><span class=\"line\">tensor0 = torch.zeros((<span class=\"number\">10</span>,<span class=\"number\">1</span>)) <span class=\"comment\"># 创建一个10行1列全是0的张量</span></span><br><span class=\"line\">tensor2 = torch.normal(mean, std, shape) <span class=\"comment\"># 创建一个形状为shape，均值为mean，标准差为std的正态分布的张量</span></span><br><span class=\"line\"><span class=\"comment\"># ep</span></span><br><span class=\"line\">tensor2 = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, (<span class=\"number\">3</span>, <span class=\"number\">10</span>, <span class=\"number\">4</span>)) <span class=\"comment\"># 均值为0，标准差为0.01，形状3维10行4列</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"张量求和\"><a href=\"#张量求和\" class=\"headerlink\" title=\"张量求和\"></a>张量求和</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">sum1 = torch.<span class=\"built_in\">sum</span>(tensor1, dim = <span class=\"number\">0</span> <span class=\"keyword\">or</span> <span class=\"number\">1</span>, keepdim = <span class=\"literal\">True</span>) <span class=\"comment\"># 0按列求和，1按行求和; keepdim保持原shape打印</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"张量形状\"><a href=\"#张量形状\" class=\"headerlink\" title=\"张量形状\"></a>张量形状</h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">tensor.shape</span><br></pre></td></tr></table></figure>\n\n<h2 id=\"tensor-backward\"><a href=\"#tensor-backward\" class=\"headerlink\" title=\"tensor.backward()\"></a>tensor.backward()</h2><p>在PyTorch中，<code>tensor.backward()</code> 是实现自动微分（Autograd）的核心方法。它的主要作用是计算当前张量相对于某个标量值的梯度，并存储在相应张量的 <code>.grad</code> 属性中。以下是关键要点：</p>\n<ol>\n<li><strong>基本用法</strong>：</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor(<span class=\"number\">2.0</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = x ** <span class=\"number\">2</span></span><br><span class=\"line\">y.backward()  <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)  <span class=\"comment\"># 输出：tensor(4.)</span></span><br></pre></td></tr></table></figure>\n\n<ol start=\"2\">\n<li><strong>工作原理</strong>：</li>\n</ol>\n<ul>\n<li>构建计算图：所有涉及<code>requires_grad=True</code>张量的运算都会记录在动态计算图中</li>\n<li>反向传播：从调用<code>backward()</code>的张量开始，反向遍历计算图应用链式法则</li>\n<li>梯度累积：结果梯度会累加到叶子节点的<code>.grad</code>属性中</li>\n</ul>\n<ol start=\"3\">\n<li><strong>参数说明</strong>：</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 当输出是非标量时需指定梯度权重</span></span><br><span class=\"line\">x = torch.randn(<span class=\"number\">3</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = x * <span class=\"number\">2</span></span><br><span class=\"line\">y.backward(gradient=torch.tensor([<span class=\"number\">1.</span>, <span class=\"number\">1.</span>, <span class=\"number\">1.</span>]))  <span class=\"comment\"># 等效于y.sum().backward()</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 保留计算图结构（用于多次反向传播）</span></span><br><span class=\"line\">loss.backward(retain_graph=<span class=\"literal\">True</span>)</span><br></pre></td></tr></table></figure>\n\n<ol start=\"4\">\n<li><strong>典型应用场景</strong>：</li>\n</ol>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 训练循环中的典型用法</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> data, target <span class=\"keyword\">in</span> dataloader:</span><br><span class=\"line\">    optimizer.zero_grad()  <span class=\"comment\"># 清空梯度</span></span><br><span class=\"line\">    output = model(data)</span><br><span class=\"line\">    loss = loss_fn(output, target)</span><br><span class=\"line\">    loss.backward()        <span class=\"comment\"># 计算梯度</span></span><br><span class=\"line\">    optimizer.step()       <span class=\"comment\"># 更新参数</span></span><br></pre></td></tr></table></figure>\n\n<h3 id=\"注意事项：\"><a href=\"#注意事项：\" class=\"headerlink\" title=\"注意事项：\"></a>注意事项：</h3><ul>\n<li>梯度会自动累积，需手动调用<code>zero_grad()</code>清除</li>\n<li>默认会释放计算图，二次调用<code>backward()</code>需设置<code>retain_graph=True</code></li>\n<li>只能对标量值直接调用<code>backward()</code>，多维张量需提供<code>gradient</code>参数<br>以下是关于PyTorch中<code>tensor.backward()</code>两个关键机制的深入解释：</li>\n</ul>\n<hr>\n<h4 id=\"1-默认释放计算图与retain-graph-True的作用\"><a href=\"#1-默认释放计算图与retain-graph-True的作用\" class=\"headerlink\" title=\"1. 默认释放计算图与retain_graph=True的作用\"></a>1. <strong>默认释放计算图与<code>retain_graph=True</code>的作用</strong></h4><h5 id=\"原理\"><a href=\"#原理\" class=\"headerlink\" title=\"原理\"></a>原理</h5><p>PyTorch的自动微分系统基于动态计算图（Dynamic Computation Graph）。当调用<code>backward()</code>时：</p>\n<ul>\n<li><strong>默认行为</strong>：反向传播完成后，计算图会被<strong>立即释放</strong>以节省内存。</li>\n<li><strong>问题</strong>：如果尝试再次调用<code>backward()</code>，由于计算图已销毁，会抛出<code>RuntimeError</code>。</li>\n</ul>\n<h5 id=\"示例\"><a href=\"#示例\" class=\"headerlink\" title=\"示例\"></a>示例</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor(<span class=\"number\">2.0</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = x ** <span class=\"number\">3</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 第一次反向传播（正常执行）</span></span><br><span class=\"line\">y.backward()  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)  <span class=\"comment\"># 输出: tensor(12.)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 第二次反向传播（会报错）</span></span><br><span class=\"line\">y.backward()  <span class=\"comment\"># ❌ RuntimeError: Trying to backward through the graph a second time</span></span><br></pre></td></tr></table></figure>\n\n<h5 id=\"解决方案\"><a href=\"#解决方案\" class=\"headerlink\" title=\"解决方案\"></a>解决方案</h5><p>通过<code>retain_graph=True</code>保留计算图：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">y.backward(retain_graph=<span class=\"literal\">True</span>)  <span class=\"comment\"># 第一次反向传播保留计算图</span></span><br><span class=\"line\">y.backward()                   <span class=\"comment\"># ✅ 可再次执行</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)                  <span class=\"comment\"># 梯度累积为 12 + 12 = 24</span></span><br></pre></td></tr></table></figure>\n\n<h5 id=\"注意\"><a href=\"#注意\" class=\"headerlink\" title=\"注意\"></a>注意</h5><ul>\n<li><strong>内存开销</strong>：保留计算图会增加内存占用，需谨慎使用</li>\n<li><strong>应用场景</strong>：如GAN需要交替训练生成器和判别器时</li>\n<li><strong>替代方案</strong>：使用<code>torch.autograd.grad()</code>直接计算梯度（不修改<code>.grad</code>属性）</li>\n</ul>\n<hr>\n<h4 id=\"2-标量限制与gradient参数的作用\"><a href=\"#2-标量限制与gradient参数的作用\" class=\"headerlink\" title=\"2. 标量限制与gradient参数的作用\"></a>2. <strong>标量限制与<code>gradient</code>参数的作用</strong></h4><h5 id=\"原理-1\"><a href=\"#原理-1\" class=\"headerlink\" title=\"原理\"></a>原理</h5><ul>\n<li><strong>标量限制</strong>：数学上梯度定义为<strong>标量函数对张量的导数</strong>。当输出是非标量时，PyTorch无法确定如何聚合多维输出的梯度。</li>\n<li><strong>gradient参数</strong>：本质是一个权重向量，用于计算<strong>加权和的梯度</strong>（相当于<code>torch.sum(gradient * output)</code>）。</li>\n</ul>\n<h5 id=\"示例-1\"><a href=\"#示例-1\" class=\"headerlink\" title=\"示例\"></a>示例</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.tensor([<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = x * <span class=\"number\">2</span>  <span class=\"comment\"># y = [2.0, 4.0]</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># ❌ 直接调用会报错</span></span><br><span class=\"line\">y.backward()  <span class=\"comment\"># RuntimeError: grad can be implicitly created only for scalar outputs</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># ✅ 正确方式：提供gradient参数</span></span><br><span class=\"line\">y.backward(gradient=torch.tensor([<span class=\"number\">1.</span>, <span class=\"number\">1.</span>]))  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)  <span class=\"comment\"># 输出: tensor([2., 2.]) </span></span><br><span class=\"line\">               <span class=\"comment\"># 计算过程：d(sum(y))/dx = d(2x1 + 2x2)/dx = [2, 2]</span></span><br></pre></td></tr></table></figure>\n\n<h5 id=\"数学等价性\"><a href=\"#数学等价性\" class=\"headerlink\" title=\"数学等价性\"></a>数学等价性</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 以下两种写法等价</span></span><br><span class=\"line\">y.backward(gradient=torch.tensor([<span class=\"number\">1.</span>, <span class=\"number\">1.</span>]))</span><br><span class=\"line\"><span class=\"comment\"># 等价于</span></span><br><span class=\"line\">y.<span class=\"built_in\">sum</span>().backward()</span><br></pre></td></tr></table></figure>\n\n<h5 id=\"高级用法\"><a href=\"#高级用法\" class=\"headerlink\" title=\"高级用法\"></a>高级用法</h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 自定义权重计算梯度</span></span><br><span class=\"line\">gradient_weights = torch.tensor([<span class=\"number\">0.5</span>, <span class=\"number\">2.0</span>])</span><br><span class=\"line\">y.backward(gradient=gradient_weights)</span><br><span class=\"line\"><span class=\"built_in\">print</span>(x.grad)  <span class=\"comment\"># 输出: tensor([1., 4.]) </span></span><br><span class=\"line\">               <span class=\"comment\"># 计算：0.5*2 + 2.0*2 = [1, 4]</span></span><br></pre></td></tr></table></figure>\n\n<h5 id=\"注意-1\"><a href=\"#注意-1\" class=\"headerlink\" title=\"注意\"></a>注意</h5><ul>\n<li><strong>默认行为</strong>：当输出是标量时，等价于<code>gradient=torch.tensor(1.0)</code></li>\n<li><strong>广播机制</strong>：<code>gradient</code>的形状必须与输出张量形状匹配</li>\n<li><strong>物理意义</strong>：可理解为对多维输出不同通道的梯度重要性加权</li>\n</ul>\n<h3 id=\"梯度计算解牛\"><a href=\"#梯度计算解牛\" class=\"headerlink\" title=\"梯度计算解牛\"></a>梯度计算解牛</h3><h4 id=\"1-梯度计算的本质\"><a href=\"#1-梯度计算的本质\" class=\"headerlink\" title=\"1. 梯度计算的本质\"></a><strong>1. 梯度计算的本质</strong></h4><p>PyTorch的梯度计算本质上是 <strong>逐元素（element-wise）</strong> 进行的。对于任意形状的张量：</p>\n<ul>\n<li>梯度张量与原张量形状 <strong>严格一致</strong></li>\n<li>每个元素的梯度表示 <strong>该元素对最终标量损失值的贡献</strong></li>\n</ul>\n<h5 id=\"示例代码中的-w-0\"><a href=\"#示例代码中的-w-0\" class=\"headerlink\" title=\"示例代码中的 w_0\"></a>示例代码中的 <code>w_0</code></h5><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">true_w = torch.tensor([<span class=\"number\">10.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">5.0</span>, <span class=\"number\">2.0</span>])  <span class=\"comment\"># 4维向量</span></span><br><span class=\"line\">w_0 = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, true_w.shape, requires_grad=<span class=\"literal\">True</span>)  <span class=\"comment\"># 形状为 (4,)</span></span><br></pre></td></tr></table></figure>\n\n<p>当调用 <code>loss.backward()</code> 时：</p>\n<ul>\n<li>系统会计算损失值对 <code>w_0</code> 的 <strong>每个元素</strong> 的偏导数</li>\n<li>最终 <code>w_0.grad</code> 也会是一个形状为 <code>(4,)</code> 的张量</li>\n</ul>\n<hr>\n<h4 id=\"2-计算过程可视化\"><a href=\"#2-计算过程可视化\" class=\"headerlink\" title=\"2. 计算过程可视化\"></a><strong>2. 计算过程可视化</strong></h4><p>假设当前 <code>w_0</code> 和梯度如下：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w_0 = tensor([w1, w2, w3, w4], requires_grad=<span class=\"literal\">True</span>)  <span class=\"comment\"># 实际值可能是 [0.1, -0.2, 0.05, 0.3]</span></span><br><span class=\"line\">grad = tensor([dw1, dw2, dw3, dw4])                <span class=\"comment\"># 例如 [1.2, 0.8, -0.5, 2.1]</span></span><br></pre></td></tr></table></figure>\n\n<p>参数更新操作：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">w_0 -= lr * w_0.grad</span><br><span class=\"line\"><span class=\"comment\"># 等价于：</span></span><br><span class=\"line\">w1_new = w1 - lr * dw1</span><br><span class=\"line\">w2_new = w2 - lr * dw2</span><br><span class=\"line\">w3_new = w3 - lr * dw3</span><br><span class=\"line\">w4_new = w4 - lr * dw4</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h4 id=\"3-数学推导验证\"><a href=\"#3-数学推导验证\" class=\"headerlink\" title=\"3. 数学推导验证\"></a><strong>3. 数学推导验证</strong></h4><p><img src=\"/2025/03/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0python%E5%BA%93/Snipaste_2025-03-20_14-56-59.jpg\"></p>\n<hr>\n<h4 id=\"4-代码执行细节验证\"><a href=\"#4-代码执行细节验证\" class=\"headerlink\" title=\"4. 代码执行细节验证\"></a><strong>4. 代码执行细节验证</strong></h4><p>在你的代码中：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">fun</span>(<span class=\"params\">x, w, b</span>):</span><br><span class=\"line\">    pred_y = torch.matmul(x, w) + b  <span class=\"comment\"># x形状(batch_size,4), w形状(4,)</span></span><br><span class=\"line\">    <span class=\"keyword\">return</span> pred_y                    <span class=\"comment\"># 输出形状(batch_size,)</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">maeLoss</span>(<span class=\"params\">pred_y, y</span>):</span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">sum</span>(<span class=\"built_in\">abs</span>(pred_y-y))/<span class=\"built_in\">len</span>(y)  <span class=\"comment\"># 输出是标量</span></span><br></pre></td></tr></table></figure>\n\n<p>反向传播时：</p>\n<ol>\n<li><code>loss</code> 是标量（关键！）</li>\n<li>通过链式法则，PyTorch会自动计算标量损失对每个 <code>w_j</code> 的梯度</li>\n<li>所有梯度按原始维度组织，存储在 <code>w_0.grad</code></li>\n</ol>\n<hr>\n<h4 id=\"5-梯度形状验证实验\"><a href=\"#5-梯度形状验证实验\" class=\"headerlink\" title=\"5. 梯度形状验证实验\"></a><strong>5. 梯度形状验证实验</strong></h4><p>可以添加调试代码验证梯度形状：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"built_in\">print</span>(w_0.shape)        <span class=\"comment\"># 输出: torch.Size([4])</span></span><br><span class=\"line\"><span class=\"built_in\">print</span>(w_0.grad.shape)   <span class=\"comment\"># 反向传播后会输出: torch.Size([4])</span></span><br></pre></td></tr></table></figure>\n\n<hr>\n<h4 id=\"6-与一维情况的对比\"><a href=\"#6-与一维情况的对比\" class=\"headerlink\" title=\"6. 与一维情况的对比\"></a><strong>6. 与一维情况的对比</strong></h4><table>\n<thead>\n<tr>\n<th>参数类型</th>\n<th>参数形状</th>\n<th>梯度形状</th>\n<th>更新方式</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>标量</td>\n<td>()</td>\n<td>()</td>\n<td><code>w -= lr * w.grad</code></td>\n</tr>\n<tr>\n<td>向量</td>\n<td>(4,)</td>\n<td>(4,)</td>\n<td>逐元素更新</td>\n</tr>\n<tr>\n<td>矩阵</td>\n<td>(3,4)</td>\n<td>(3,4)</td>\n<td>每个矩阵元素独立更新</td>\n</tr>\n</tbody></table>\n<hr>\n<h4 id=\"为什么没有维度冲突？\"><a href=\"#为什么没有维度冲突？\" class=\"headerlink\" title=\"为什么没有维度冲突？\"></a><strong>为什么没有维度冲突？</strong></h4><ul>\n<li><strong>关键点</strong>：损失函数最终输出始终是 <strong>标量</strong>（<code>maeLoss</code> 返回的是平均值）</li>\n<li><strong>数学保证</strong>：标量对任意维度张量的导数自然保持与原张量相同的形状</li>\n<li><strong>PyTorch特性</strong>：自动微分系统会处理任意维度的张量梯度计算</li>\n</ul>\n<hr>\n<h4 id=\"常见误区澄清\"><a href=\"#常见误区澄清\" class=\"headerlink\" title=\"常见误区澄清\"></a><strong>常见误区澄清</strong></h4><p>误区：<em>“多维张量需要特殊处理才能计算梯度”</em></p>\n<p>事实：</p>\n<ul>\n<li>只要最终损失是标量，PyTorch可以处理任意维度的参数梯度</li>\n<li>无论参数是标量、向量、矩阵还是高阶张量，梯度计算规则一致</li>\n<li>参数更新时的逐元素操作是自动完成的</li>\n</ul>\n<h1 id=\"matplotlib-pyplot\"><a href=\"#matplotlib-pyplot\" class=\"headerlink\" title=\"matplotlib.pyplot\"></a>matplotlib.pyplot</h1><h2 id=\"plt-plot\"><a href=\"#plt-plot\" class=\"headerlink\" title=\"plt.plot()\"></a>plt.plot()</h2><p>在使用 PyTorch 进行深度学习训练时，如果要将张量数据传递给 <code>matplotlib.pyplot.plot()</code> 进行可视化，通常需要添加 <code>.detach().numpy()</code> 操作。这是由 PyTorch 张量和 Matplotlib 的底层机制差异导致的，具体原因如下：</p>\n<hr>\n<h3 id=\"1-核心原因：数据格式转换\"><a href=\"#1-核心原因：数据格式转换\" class=\"headerlink\" title=\"1. 核心原因：数据格式转换\"></a><strong>1. 核心原因：数据格式转换</strong></h3><table>\n<thead>\n<tr>\n<th>操作步骤</th>\n<th>作用</th>\n<th>必要性</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>.detach()</code></td>\n<td>将张量从计算图中分离，得到一个不需要梯度追踪的新张量</td>\n<td>必要</td>\n</tr>\n<tr>\n<td><code>.numpy()</code></td>\n<td>将 PyTorch 张量转换为 NumPy 数组（Matplotlib 只能处理 NumPy 数组或 Python 原生数据类型）</td>\n<td>必要</td>\n</tr>\n</tbody></table>\n<hr>\n<h3 id=\"2-分步详解\"><a href=\"#2-分步详解\" class=\"headerlink\" title=\"2. 分步详解\"></a><strong>2. 分步详解</strong></h3><h4 id=\"（1）脱离计算图（-detach-）\"><a href=\"#（1）脱离计算图（-detach-）\" class=\"headerlink\" title=\"（1）脱离计算图（.detach()）\"></a><strong>（1）脱离计算图（.detach()）</strong></h4><ul>\n<li><strong>问题背景</strong>：PyTorch 张量可能带有梯度信息（<code>requires_grad=True</code>）</li>\n<li><strong>风险</strong>：如果直接使用带有梯度的张量：<ul>\n<li>会增加不必要的内存占用（保持计算图）</li>\n<li>可能引发意外的梯度传播（尽管绘图操作不需要梯度）</li>\n</ul>\n</li>\n<li><strong>示例对比</strong>：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 原始张量（带梯度）</span></span><br><span class=\"line\">tensor_with_grad = torch.tensor([<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 直接转换会报错</span></span><br><span class=\"line\"><span class=\"keyword\">try</span>:</span><br><span class=\"line\">    plt.plot(tensor_with_grad)  <span class=\"comment\"># ❌ 报错：Can&#x27;t call numpy() on Tensor that requires grad</span></span><br><span class=\"line\"><span class=\"keyword\">except</span> Exception <span class=\"keyword\">as</span> e:</span><br><span class=\"line\">    <span class=\"built_in\">print</span>(e)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 正确做法</span></span><br><span class=\"line\">plt.plot(tensor_with_grad.detach().numpy())  <span class=\"comment\"># ✅ 正常工作</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h4 id=\"（2）设备转移（CPU-GPU）\"><a href=\"#（2）设备转移（CPU-GPU）\" class=\"headerlink\" title=\"（2）设备转移（CPU&#x2F;GPU）\"></a><strong>（2）设备转移（CPU&#x2F;GPU）</strong></h4><ul>\n<li><strong>问题背景</strong>：如果张量在 GPU 上（<code>device=&#39;cuda&#39;</code>）</li>\n<li><strong>风险</strong>：Matplotlib 无法直接处理 GPU 上的张量</li>\n<li><strong>完整转换流程</strong>：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># GPU 张量处理流程</span></span><br><span class=\"line\">gpu_tensor = torch.tensor([<span class=\"number\">1.0</span>, <span class=\"number\">2.0</span>], device=<span class=\"string\">&#x27;cuda&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 错误方式</span></span><br><span class=\"line\">plt.plot(gpu_tensor.cpu().detach().numpy())  <span class=\"comment\"># ❌ 顺序错误，应先 detach</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 正确方式</span></span><br><span class=\"line\">plt.plot(gpu_tensor.detach().cpu().numpy())  <span class=\"comment\"># ✅ 正确顺序：detach → cpu → numpy</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n<h4 id=\"（3）数据类型转换\"><a href=\"#（3）数据类型转换\" class=\"headerlink\" title=\"（3）数据类型转换\"></a><strong>（3）数据类型转换</strong></h4><table>\n<thead>\n<tr>\n<th>数据类型</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>PyTorch Tensor</td>\n<td>可以是任意形状和数据类型（float32, int64 等）</td>\n</tr>\n<tr>\n<td>NumPy Array</td>\n<td>Matplotlib 的底层数据容器，与 PyTorch 内存不兼容</td>\n</tr>\n</tbody></table>\n<hr>\n<h3 id=\"3-完整转换流程\"><a href=\"#3-完整转换流程\" class=\"headerlink\" title=\"3. 完整转换流程\"></a><strong>3. 完整转换流程</strong></h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 假设有一个需要绘制的 PyTorch 张量</span></span><br><span class=\"line\">original_tensor = torch.randn(<span class=\"number\">100</span>, requires_grad=<span class=\"literal\">True</span>, device=<span class=\"string\">&#x27;cuda&#x27;</span>)</span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 安全转换步骤</span></span><br><span class=\"line\">plot_data = original_tensor.detach()  <span class=\"comment\"># 1. 脱离计算图</span></span><br><span class=\"line\">                .cpu()                <span class=\"comment\"># 2. 转移到 CPU（如果是 GPU 张量）</span></span><br><span class=\"line\">                .numpy()              <span class=\"comment\"># 3. 转换为 NumPy 数组</span></span><br><span class=\"line\"></span><br><span class=\"line\"><span class=\"comment\"># 绘制图形</span></span><br><span class=\"line\">plt.plot(plot_data)</span><br><span class=\"line\">plt.show()</span><br></pre></td></tr></table></figure>\n\n<hr>\n<h3 id=\"4-常见错误场景\"><a href=\"#4-常见错误场景\" class=\"headerlink\" title=\"4. 常见错误场景\"></a><strong>4. 常见错误场景</strong></h3><h4 id=\"场景-1：未分离计算图\"><a href=\"#场景-1：未分离计算图\" class=\"headerlink\" title=\"场景 1：未分离计算图\"></a><strong>场景 1：未分离计算图</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">x = torch.linspace(<span class=\"number\">0</span>, <span class=\"number\">2</span>*np.pi, <span class=\"number\">100</span>, requires_grad=<span class=\"literal\">True</span>)</span><br><span class=\"line\">y = torch.sin(x)</span><br><span class=\"line\"></span><br><span class=\"line\">plt.plot(x.numpy(), y.numpy())  <span class=\"comment\"># ❌ RuntimeError: Can&#x27;t call numpy() on Tensor that requires grad</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"场景-2：未处理-GPU-张量\"><a href=\"#场景-2：未处理-GPU-张量\" class=\"headerlink\" title=\"场景 2：未处理 GPU 张量\"></a><strong>场景 2：未处理 GPU 张量</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">gpu_data = torch.randn(<span class=\"number\">10</span>).cuda()</span><br><span class=\"line\">plt.plot(gpu_data.detach().numpy())  <span class=\"comment\"># ❌ TypeError: can&#x27;t convert cuda:0 device type tensor to numpy</span></span><br></pre></td></tr></table></figure>\n\n<h4 id=\"场景-3：错误操作顺序\"><a href=\"#场景-3：错误操作顺序\" class=\"headerlink\" title=\"场景 3：错误操作顺序\"></a><strong>场景 3：错误操作顺序</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 先转 NumPy 再 detach 会丢失梯度信息</span></span><br><span class=\"line\">temp = y.numpy()         <span class=\"comment\"># ❌ 错误开始点</span></span><br><span class=\"line\">detached = temp.detach() <span class=\"comment\"># ❌ AttributeError: &#x27;numpy.ndarray&#x27; object has no attribute &#x27;detach&#x27;</span></span><br></pre></td></tr></table></figure>\n\n<hr>\n<h3 id=\"5-最佳实践总结\"><a href=\"#5-最佳实践总结\" class=\"headerlink\" title=\"5. 最佳实践总结\"></a><strong>5. 最佳实践总结</strong></h3><table>\n<thead>\n<tr>\n<th>操作类型</th>\n<th>推荐写法</th>\n<th>说明</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>CPU + 无梯度</td>\n<td><code>tensor.numpy()</code></td>\n<td>直接转换</td>\n</tr>\n<tr>\n<td>CPU + 有梯度</td>\n<td><code>tensor.detach().numpy()</code></td>\n<td>必须 detach</td>\n</tr>\n<tr>\n<td>GPU + 无梯度</td>\n<td><code>tensor.cpu().numpy()</code></td>\n<td>需要转移到 CPU</td>\n</tr>\n<tr>\n<td>GPU + 有梯度</td>\n<td><code>tensor.detach().cpu().numpy()</code></td>\n<td>完整流程</td>\n</tr>\n</tbody></table>\n<hr>\n<h3 id=\"6-特殊场景处理\"><a href=\"#6-特殊场景处理\" class=\"headerlink\" title=\"6. 特殊场景处理\"></a><strong>6. 特殊场景处理</strong></h3><h4 id=\"保留梯度但需要可视化\"><a href=\"#保留梯度但需要可视化\" class=\"headerlink\" title=\"保留梯度但需要可视化\"></a><strong>保留梯度但需要可视化</strong></h4><p>如果需要在可视化后继续梯度计算（罕见需求）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 使用 with torch.no_grad(): 临时禁用梯度</span></span><br><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br><span class=\"line\">    plt.plot(x.cpu().numpy(), y.cpu().numpy())</span><br></pre></td></tr></table></figure>\n\n<h4 id=\"批量处理张量\"><a href=\"#批量处理张量\" class=\"headerlink\" title=\"批量处理张量\"></a><strong>批量处理张量</strong></h4><p>对于高维张量（如神经网络中间层输出）：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 假设 feature_map 是 4D 张量 (batch, channel, height, width)</span></span><br><span class=\"line\">feature_map = model(inputs)</span><br><span class=\"line\">plt.imshow(feature_map[<span class=\"number\">0</span>, <span class=\"number\">0</span>].detach().cpu().numpy())  <span class=\"comment\"># 可视化第一个样本的第一个通道</span></span><br></pre></td></tr></table></figure>\n\n<hr>\n<h3 id=\"总结\"><a href=\"#总结\" class=\"headerlink\" title=\"总结\"></a><strong>总结</strong></h3><p><code>.detach().numpy()</code>（对于 GPU 张量还需 <code>.cpu()</code>）的组合操作是 PyTorch 与 Matplotlib 协作的 <strong>必要桥梁</strong>，其主要作用包括：</p>\n<ol>\n<li><strong>断开梯度传播</strong>：防止可视化操作影响反向传播</li>\n<li><strong>设备转移</strong>：确保数据位于 CPU 内存</li>\n<li><strong>格式转换</strong>：将张量转换为 Matplotlib 可识别的 NumPy 数组</li>\n</ol>\n<p>这种转换虽然增加了代码的复杂度，但能有效避免许多隐蔽的错误，是 PyTorch 可视化过程中必须掌握的关键技巧。</p>\n"},{"title":"线性回归练习代码解读","mathjax":true,"_content":"# part.1 函数定义 create_data()\n```python\ndef create_data(w, b, data_num):\n    x = torch.normal(0, 1, (data_num, len(w)))  \n    y = torch.matmul(x, w) + b  \n  \n    noise = torch.normal(0, 0.01, y.shape)  \n    y += noise  \n  \n    return x, y\n```\n\n定义了一个create_data函数，用于生成数据。\n\ntorch.normal: [docs](https://pytorch.org/docs/stable/generated/torch.normal.html)\n\ntorch.matmul: Matrix product of two tensors.\n\nx : 以0为均值，1为标准差，【(data_num)\\*(w长度)】的tensor数据\ny : x\\*w + b 并在此基础上附加了noise扰动\n\n![](Pastedimage20250319232552.png)\n# part.2 变量赋值\n```python\nnum = 500  \n  \ntrue_w = torch.tensor([10.0, 7.0, 5.0, 2.0])  \ntrue_b = torch.tensor(1.1)\n\nX, Y = create_data(true_w, true_b, num)\n```\n\n定义了本实验的真实值和实验规模，并调用create_data函数生成了实验数据。\ntrue_w: 4\\*1 true_b: 1\nX: 500\\*4 Y：500\\*1\n![](Pastedimage20250319234616.png)\n# part.3 main函数\n跳过函数定义看代码主体部分：\n```python\nlr = 0.05  \n  \nw_0 = torch.normal(0, 0.01, true_w.shape, requires_grad=True)  \nb_0 = torch.tensor(0.01, requires_grad=True)   \n  \nepochs = 50\n```\n\nlr: 超参数，学习率\nw_0, b_0：初始值 \n\trequires_grad = True：A tensor can be created with requires_grad=True so that torch.autograd records operations on them for automatic differentiation.\n\tEach tensor has an associated torch.Storage, which holds its data.\nepochs: 定义了执行梯度下降算法的轮数\n\n```python\nfor epoch in range(epochs):  \n    data_loss = 0  \n    for batch_x, batch_y in data_provider(X, Y, batchsize):  \n        pred_y = fun(batch_x, w_0, b_0)  \n        loss = maeLoss(pred_y, batch_y)  \n        loss.backward()  \n        sgd([w_0, b_0], lr)  \n        data_loss += loss  \n  \n    print(\"epoch %03d: loss: %.6f\"%(epoch, data_loss))  \n  \nprint(\"真实的函数值是\", true_w, true_b)  \nprint(\"深度学习得到的函数值是\", w_0, b_0)\n```\nfrom 66 to 78\ndata_loss变量：统计每一轮深度学习的效果\ntorch.backward():{% post_link 深度学习python库 %}\n当调用 `loss.backward()` 时：\n- 系统会计算损失值对 `w_0` 的 **每个元素** 的偏导数\n- 最终 `w_0.grad` 也会是一个形状为 `(4,)` 的张量\n# part.4 函数定义\n\n## data_provider()函数\n```python\ndef data_provider(data, label, batchsize):  \n    length = len(label)  \n    indices = list(range(length))  \n    random.shuffle(indices)  \n  \n    for each in range(0, length, batchsize):  \n        get_indices = indices[each: each + batchsize]  \n        get_data = data[get_indices]  \n        get_label = label[get_indices]  \n  \n        yield get_data, get_label  \n  \n  \nbatchsize = 16\n```\nfrom 26 to 36\n以下是 `data_provider()` 函数的逐行代码解读：\n\n---\n\n### **函数定义**\n```python\ndef data_provider(data, label, batchsize):\n```\n- **输入参数**：\n  - `data`: 特征数据张量（形状通常为 `[样本数, 特征维度]`）\n  - `label`: 标签数据张量（形状为 `[样本数]`）\n  - `batchsize`: 每个批次的样本数量\n- **功能**：生成随机小批量（mini-batch）数据\n\n---\n\n### **步骤分解**\n\n#### **1. 获取数据集长度**\n```python\n    length = len(label)  # 获取总样本数（假设数据与标签一一对应）\n```\n- 关键作用：确定需要处理的总样本数量\n- 潜在风险：如果 `data` 和 `label` 长度不一致会引发错误，但代码未做检查\n\n---\n\n#### **2. 创建索引列表**\n```python\n    indices = list(range(length))  # 生成顺序索引 [0, 1, 2, ..., length-1]\n```\n- 示例：当 `length=500` 时，生成 `[0,1,2,...,499]`\n- 目的：为后续随机采样做准备\n\n---\n\n#### **3. 随机打乱索引**\n```python\n    random.shuffle(indices)  # 原地打乱索引顺序\n```\n- 重要性：\n  - 破坏数据原始顺序，防止模型学习到顺序特征\n  - 每个epoch会生成不同的批次组合\n- 示例：可能变为 `[253, 12, 487, ..., 76]`\n\n---\n\n#### **4. 批次循环生成**\n```python\n    for each in range(0, length, batchsize):\n```\n- **循环机制**：\n  - `each` 从0开始，以 `batchsize` 为步长递增\n  - 最后一个批次可能小于 `batchsize`（例如总样本500，batchsize=16时，最后一批次有4个样本）\n- **可视化**：\n  ```\n  batch1: 0-15\n  batch2: 16-31\n  ...\n  batch31: 496-500\n  ```\n\n---\n\n#### **5. 获取当前批次索引**\n```python\n        get_indices = indices[each: each + batchsize]  # 切片获取当前批次索引\n```\n- 示例：当 `each=32`, `batchsize=16` 时，获取索引 `indices[32:48]`\n- 注意：对列表进行切片时，超出范围不会报错（自动取到列表末尾）\n\n---\n\n#### **6. 提取批次数据**\n```python\n        get_data = data[get_indices]    # 按索引提取特征数据\n        get_label = label[get_indices]  # 按索引提取对应标签\n```\n- **张量索引特性**：\n  - 支持通过索引列表进行高级索引（advanced indexing）\n  - 要求 `data` 和 `label` 的第一个维度必须与 `length` 一致\n- **输出形状**：\n  - `get_data`: `[当前批次大小, 特征维度]`\n  - `get_label`: `[当前批次大小]`\n\n---\n\n#### **7. 生成数据批次**\n```python\n        yield get_data, get_label  # 返回生成器对象\n```\n- **生成器优势**：\n  - 惰性加载：不会一次性将所有批次加载到内存\n  - 内存效率：适合处理大型数据集\n- **使用场景**：\n  ```python\n  # 在训练循环中使用\n  for batch_data, batch_label in data_provider(X, Y, 16):\n      # 执行训练步骤\n  ```\n\n---\n\n### **关键特性总结**\n| 特性                | 说明                          | 重要性                  |\n|---------------------|-----------------------------|-------------------------|\n| 随机打乱            | 每个epoch重新洗牌数据顺序       | 防止模型记住样本顺序       |\n| 动态批次生成         | 使用生成器逐批产生数据          | 节省内存，支持大数据集     |\n| 不完整批次处理       | 自动处理末尾不完整批次           | 保证数据利用率100%        |\n| 通用索引机制         | 适用于任何支持高级索引的数据结构   | 兼容NumPy/PyTorch等张量   |\n\n---\n\n### **执行流程示意图**\n```\n原始数据\n  │\n  ├─ 创建顺序索引 [0,1,2,...]\n  │\n  └─ 随机洗牌 → [253,12,487,...]\n            │\n            ├─ 切片[0:16] → batch1\n            ├─ 切片[16:32] → batch2\n            │\n            └─ ... → 直到遍历所有数据\n```\n\n该函数实现了深度学习训练中最基础且重要的 **随机小批量采样** 功能，是确保模型有效训练的关键组件。\n## fun()函数\n```python\ndef fun(x, w, b):  \n    pred_y = torch.matmul(x, w) + b  \n    return pred_y\n```\n根据给定的参数w, b对数据x生成预测值y并返回预测值pred_y\n## maeLoss()函数\n```python\ndef maeLoss(pred_y, y):  \n    return torch.sum(abs(pred_y-y))/len(y)\n```\n![](06ec95ff3c17534440c17f2c5081b49.png)\n## sgd()函数\n```python\ndef sgd(paras, lr):  \n    with torch.no_grad():  \n        for para in paras:  \n            para -= para.grad * lr  \n            para.grad.zero_()\n```\n以下是 `sgd` 函数的详细执行过程分析：\n\n---\n\n### **`sgd` 函数执行步骤详解**\n\n#### **1. 进入无梯度计算模式**\n```python\nwith torch.no_grad():\n```\n- **作用**：禁用梯度跟踪，确保参数更新操作不会记录到计算图中。\n- **必要性**：参数更新是纯粹的数值操作，不需要梯度信息。禁用梯度跟踪可以：\n  - 避免不必要的内存占用（计算图不会被扩展）\n  - 防止参数更新操作被错误地加入反向传播流程\n\n---\n\n#### **2. 遍历所有参数**\n```python\nfor para in paras:  # paras = [w_0, b_0]\n```\n- **参数类型**：`para` 是 `requires_grad=True` 的叶子张量（如 `w_0` 和 `b_0`）\n- **关键属性**：每个 `para` 的梯度存储在 `para.grad` 中，由之前的 `loss.backward()` 计算得到\n\n---\n\n#### **3. 参数值更新**\n```python\npara -= para.grad * lr  # 等价于 para = para - lr * para.grad\n```\n- **数学意义**：执行梯度下降更新  \n  $$ \\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\cdot \\nabla_{\\theta}\\mathcal{L} $$\n- **实现细节**：\n  - 原地修改张量的值 (`para` 是直接操作对象)\n  - 由于在 `no_grad()` 上下文中，此操作不会影响后续反向传播的计算图\n\n---\n\n#### **4. 梯度清零**\n```python\npara.grad.zero_()\n```\n- **必要性**：\n  - PyTorch 默认会累积梯度（梯度 += 新梯度）\n  - 必须显式清零，否则下一个 batch 的梯度会与当前梯度错误叠加\n- **方法**：调用 `zero_()` 原地清零梯度张量\n\n---\n\n### **执行时序示例**\n假设当前 batch 的梯度已计算完成（`loss.backward()` 后）：\n\n| 步骤 | 参数值 `w_0` | 梯度 `w_0.grad` | 学习率 `lr` |\n|------|-------------|----------------|-----------|\n| 初始 | 0.5         | 2.0            | 0.05      |\n| 更新 | 0.5 - 0.05*2.0 = 0.4 | 2.0 → 0.0 (清零后) | - |\n\n---\n\n### **与标准实现的差异**\n1. **手动更新 vs 优化器**：\n   ```python\n   # 标准 PyTorch 方式（等效实现）\n   optimizer = torch.optim.SGD([w_0, b_0], lr=lr)\n   optimizer.step()\n   optimizer.zero_grad()\n   ```\n   - 用户代码手动实现了优化器的核心逻辑\n\n2. **梯度清零时机**：\n   - 用户代码在每个 **batch** 更新后立即清零梯度（正确）\n   - 错误做法：在 epoch 结束后才清零（会导致梯度跨 batch 累积）\n\n---\n\n### **潜在问题与改进**\n1. **梯度爆炸风险**：\n   - 如果学习率 (`lr`) 过大，可能导致参数更新幅度过大\n   - 改进方案：添加梯度裁剪 (`torch.nn.utils.clip_grad_norm_`)\n\n2. **更复杂的优化器**：\n   ```python\n   # 添加动量（需修改 sgd 函数）\n   velocity = 0\n   def sgd_momentum(para, lr, momentum=0.9):\n       velocity = momentum * velocity - lr * para.grad\n       para += velocity\n   ```\n\n---\n\n### **关键总结**\n| 操作              | 作用                           | 必要性等级 |\n|-------------------|------------------------------|-----------|\n| `torch.no_grad()` | 防止参数更新污染计算图          | 必要      |\n| `para -= grad*lr` | 执行梯度下降参数更新            | 核心操作  |\n| `grad.zero_()`    | 防止梯度跨 batch 累积           | 必要      |\n\n通过这种手动实现的 SGD，开发者可以更直观地理解优化器底层的工作原理，但在实际项目中建议使用 PyTorch 内置优化器以获得更好的性能和稳定性。\n### 图示\n![](5a6c5073a5cd2f4a2e6f7ca2e39dc5f.png)\n![](0b0fcf44d3b47e8a5e9e12faccd42bc.png)\n# 深度学习的训练过程\n![](607dc97725be3eecbb191ed4887eadb.png)\n\n","source":"_posts/李哥考研复试项目/线性回归练习代码解读.md","raw":"---\ntitle: 线性回归练习代码解读\ntags: \n- 深度学习\n- 计算机考研复试\ncategories: \n- [深度学习]\n- [李哥考研复试项目]\nmathjax: true\n---\n# part.1 函数定义 create_data()\n```python\ndef create_data(w, b, data_num):\n    x = torch.normal(0, 1, (data_num, len(w)))  \n    y = torch.matmul(x, w) + b  \n  \n    noise = torch.normal(0, 0.01, y.shape)  \n    y += noise  \n  \n    return x, y\n```\n\n定义了一个create_data函数，用于生成数据。\n\ntorch.normal: [docs](https://pytorch.org/docs/stable/generated/torch.normal.html)\n\ntorch.matmul: Matrix product of two tensors.\n\nx : 以0为均值，1为标准差，【(data_num)\\*(w长度)】的tensor数据\ny : x\\*w + b 并在此基础上附加了noise扰动\n\n![](Pastedimage20250319232552.png)\n# part.2 变量赋值\n```python\nnum = 500  \n  \ntrue_w = torch.tensor([10.0, 7.0, 5.0, 2.0])  \ntrue_b = torch.tensor(1.1)\n\nX, Y = create_data(true_w, true_b, num)\n```\n\n定义了本实验的真实值和实验规模，并调用create_data函数生成了实验数据。\ntrue_w: 4\\*1 true_b: 1\nX: 500\\*4 Y：500\\*1\n![](Pastedimage20250319234616.png)\n# part.3 main函数\n跳过函数定义看代码主体部分：\n```python\nlr = 0.05  \n  \nw_0 = torch.normal(0, 0.01, true_w.shape, requires_grad=True)  \nb_0 = torch.tensor(0.01, requires_grad=True)   \n  \nepochs = 50\n```\n\nlr: 超参数，学习率\nw_0, b_0：初始值 \n\trequires_grad = True：A tensor can be created with requires_grad=True so that torch.autograd records operations on them for automatic differentiation.\n\tEach tensor has an associated torch.Storage, which holds its data.\nepochs: 定义了执行梯度下降算法的轮数\n\n```python\nfor epoch in range(epochs):  \n    data_loss = 0  \n    for batch_x, batch_y in data_provider(X, Y, batchsize):  \n        pred_y = fun(batch_x, w_0, b_0)  \n        loss = maeLoss(pred_y, batch_y)  \n        loss.backward()  \n        sgd([w_0, b_0], lr)  \n        data_loss += loss  \n  \n    print(\"epoch %03d: loss: %.6f\"%(epoch, data_loss))  \n  \nprint(\"真实的函数值是\", true_w, true_b)  \nprint(\"深度学习得到的函数值是\", w_0, b_0)\n```\nfrom 66 to 78\ndata_loss变量：统计每一轮深度学习的效果\ntorch.backward():{% post_link 深度学习python库 %}\n当调用 `loss.backward()` 时：\n- 系统会计算损失值对 `w_0` 的 **每个元素** 的偏导数\n- 最终 `w_0.grad` 也会是一个形状为 `(4,)` 的张量\n# part.4 函数定义\n\n## data_provider()函数\n```python\ndef data_provider(data, label, batchsize):  \n    length = len(label)  \n    indices = list(range(length))  \n    random.shuffle(indices)  \n  \n    for each in range(0, length, batchsize):  \n        get_indices = indices[each: each + batchsize]  \n        get_data = data[get_indices]  \n        get_label = label[get_indices]  \n  \n        yield get_data, get_label  \n  \n  \nbatchsize = 16\n```\nfrom 26 to 36\n以下是 `data_provider()` 函数的逐行代码解读：\n\n---\n\n### **函数定义**\n```python\ndef data_provider(data, label, batchsize):\n```\n- **输入参数**：\n  - `data`: 特征数据张量（形状通常为 `[样本数, 特征维度]`）\n  - `label`: 标签数据张量（形状为 `[样本数]`）\n  - `batchsize`: 每个批次的样本数量\n- **功能**：生成随机小批量（mini-batch）数据\n\n---\n\n### **步骤分解**\n\n#### **1. 获取数据集长度**\n```python\n    length = len(label)  # 获取总样本数（假设数据与标签一一对应）\n```\n- 关键作用：确定需要处理的总样本数量\n- 潜在风险：如果 `data` 和 `label` 长度不一致会引发错误，但代码未做检查\n\n---\n\n#### **2. 创建索引列表**\n```python\n    indices = list(range(length))  # 生成顺序索引 [0, 1, 2, ..., length-1]\n```\n- 示例：当 `length=500` 时，生成 `[0,1,2,...,499]`\n- 目的：为后续随机采样做准备\n\n---\n\n#### **3. 随机打乱索引**\n```python\n    random.shuffle(indices)  # 原地打乱索引顺序\n```\n- 重要性：\n  - 破坏数据原始顺序，防止模型学习到顺序特征\n  - 每个epoch会生成不同的批次组合\n- 示例：可能变为 `[253, 12, 487, ..., 76]`\n\n---\n\n#### **4. 批次循环生成**\n```python\n    for each in range(0, length, batchsize):\n```\n- **循环机制**：\n  - `each` 从0开始，以 `batchsize` 为步长递增\n  - 最后一个批次可能小于 `batchsize`（例如总样本500，batchsize=16时，最后一批次有4个样本）\n- **可视化**：\n  ```\n  batch1: 0-15\n  batch2: 16-31\n  ...\n  batch31: 496-500\n  ```\n\n---\n\n#### **5. 获取当前批次索引**\n```python\n        get_indices = indices[each: each + batchsize]  # 切片获取当前批次索引\n```\n- 示例：当 `each=32`, `batchsize=16` 时，获取索引 `indices[32:48]`\n- 注意：对列表进行切片时，超出范围不会报错（自动取到列表末尾）\n\n---\n\n#### **6. 提取批次数据**\n```python\n        get_data = data[get_indices]    # 按索引提取特征数据\n        get_label = label[get_indices]  # 按索引提取对应标签\n```\n- **张量索引特性**：\n  - 支持通过索引列表进行高级索引（advanced indexing）\n  - 要求 `data` 和 `label` 的第一个维度必须与 `length` 一致\n- **输出形状**：\n  - `get_data`: `[当前批次大小, 特征维度]`\n  - `get_label`: `[当前批次大小]`\n\n---\n\n#### **7. 生成数据批次**\n```python\n        yield get_data, get_label  # 返回生成器对象\n```\n- **生成器优势**：\n  - 惰性加载：不会一次性将所有批次加载到内存\n  - 内存效率：适合处理大型数据集\n- **使用场景**：\n  ```python\n  # 在训练循环中使用\n  for batch_data, batch_label in data_provider(X, Y, 16):\n      # 执行训练步骤\n  ```\n\n---\n\n### **关键特性总结**\n| 特性                | 说明                          | 重要性                  |\n|---------------------|-----------------------------|-------------------------|\n| 随机打乱            | 每个epoch重新洗牌数据顺序       | 防止模型记住样本顺序       |\n| 动态批次生成         | 使用生成器逐批产生数据          | 节省内存，支持大数据集     |\n| 不完整批次处理       | 自动处理末尾不完整批次           | 保证数据利用率100%        |\n| 通用索引机制         | 适用于任何支持高级索引的数据结构   | 兼容NumPy/PyTorch等张量   |\n\n---\n\n### **执行流程示意图**\n```\n原始数据\n  │\n  ├─ 创建顺序索引 [0,1,2,...]\n  │\n  └─ 随机洗牌 → [253,12,487,...]\n            │\n            ├─ 切片[0:16] → batch1\n            ├─ 切片[16:32] → batch2\n            │\n            └─ ... → 直到遍历所有数据\n```\n\n该函数实现了深度学习训练中最基础且重要的 **随机小批量采样** 功能，是确保模型有效训练的关键组件。\n## fun()函数\n```python\ndef fun(x, w, b):  \n    pred_y = torch.matmul(x, w) + b  \n    return pred_y\n```\n根据给定的参数w, b对数据x生成预测值y并返回预测值pred_y\n## maeLoss()函数\n```python\ndef maeLoss(pred_y, y):  \n    return torch.sum(abs(pred_y-y))/len(y)\n```\n![](06ec95ff3c17534440c17f2c5081b49.png)\n## sgd()函数\n```python\ndef sgd(paras, lr):  \n    with torch.no_grad():  \n        for para in paras:  \n            para -= para.grad * lr  \n            para.grad.zero_()\n```\n以下是 `sgd` 函数的详细执行过程分析：\n\n---\n\n### **`sgd` 函数执行步骤详解**\n\n#### **1. 进入无梯度计算模式**\n```python\nwith torch.no_grad():\n```\n- **作用**：禁用梯度跟踪，确保参数更新操作不会记录到计算图中。\n- **必要性**：参数更新是纯粹的数值操作，不需要梯度信息。禁用梯度跟踪可以：\n  - 避免不必要的内存占用（计算图不会被扩展）\n  - 防止参数更新操作被错误地加入反向传播流程\n\n---\n\n#### **2. 遍历所有参数**\n```python\nfor para in paras:  # paras = [w_0, b_0]\n```\n- **参数类型**：`para` 是 `requires_grad=True` 的叶子张量（如 `w_0` 和 `b_0`）\n- **关键属性**：每个 `para` 的梯度存储在 `para.grad` 中，由之前的 `loss.backward()` 计算得到\n\n---\n\n#### **3. 参数值更新**\n```python\npara -= para.grad * lr  # 等价于 para = para - lr * para.grad\n```\n- **数学意义**：执行梯度下降更新  \n  $$ \\theta_{\\text{new}} = \\theta_{\\text{old}} - \\eta \\cdot \\nabla_{\\theta}\\mathcal{L} $$\n- **实现细节**：\n  - 原地修改张量的值 (`para` 是直接操作对象)\n  - 由于在 `no_grad()` 上下文中，此操作不会影响后续反向传播的计算图\n\n---\n\n#### **4. 梯度清零**\n```python\npara.grad.zero_()\n```\n- **必要性**：\n  - PyTorch 默认会累积梯度（梯度 += 新梯度）\n  - 必须显式清零，否则下一个 batch 的梯度会与当前梯度错误叠加\n- **方法**：调用 `zero_()` 原地清零梯度张量\n\n---\n\n### **执行时序示例**\n假设当前 batch 的梯度已计算完成（`loss.backward()` 后）：\n\n| 步骤 | 参数值 `w_0` | 梯度 `w_0.grad` | 学习率 `lr` |\n|------|-------------|----------------|-----------|\n| 初始 | 0.5         | 2.0            | 0.05      |\n| 更新 | 0.5 - 0.05*2.0 = 0.4 | 2.0 → 0.0 (清零后) | - |\n\n---\n\n### **与标准实现的差异**\n1. **手动更新 vs 优化器**：\n   ```python\n   # 标准 PyTorch 方式（等效实现）\n   optimizer = torch.optim.SGD([w_0, b_0], lr=lr)\n   optimizer.step()\n   optimizer.zero_grad()\n   ```\n   - 用户代码手动实现了优化器的核心逻辑\n\n2. **梯度清零时机**：\n   - 用户代码在每个 **batch** 更新后立即清零梯度（正确）\n   - 错误做法：在 epoch 结束后才清零（会导致梯度跨 batch 累积）\n\n---\n\n### **潜在问题与改进**\n1. **梯度爆炸风险**：\n   - 如果学习率 (`lr`) 过大，可能导致参数更新幅度过大\n   - 改进方案：添加梯度裁剪 (`torch.nn.utils.clip_grad_norm_`)\n\n2. **更复杂的优化器**：\n   ```python\n   # 添加动量（需修改 sgd 函数）\n   velocity = 0\n   def sgd_momentum(para, lr, momentum=0.9):\n       velocity = momentum * velocity - lr * para.grad\n       para += velocity\n   ```\n\n---\n\n### **关键总结**\n| 操作              | 作用                           | 必要性等级 |\n|-------------------|------------------------------|-----------|\n| `torch.no_grad()` | 防止参数更新污染计算图          | 必要      |\n| `para -= grad*lr` | 执行梯度下降参数更新            | 核心操作  |\n| `grad.zero_()`    | 防止梯度跨 batch 累积           | 必要      |\n\n通过这种手动实现的 SGD，开发者可以更直观地理解优化器底层的工作原理，但在实际项目中建议使用 PyTorch 内置优化器以获得更好的性能和稳定性。\n### 图示\n![](5a6c5073a5cd2f4a2e6f7ca2e39dc5f.png)\n![](0b0fcf44d3b47e8a5e9e12faccd42bc.png)\n# 深度学习的训练过程\n![](607dc97725be3eecbb191ed4887eadb.png)\n\n","slug":"李哥考研复试项目/线性回归练习代码解读","published":1,"date":"2025-03-20T04:02:36.625Z","updated":"2025-03-20T06:47:57.330Z","comments":1,"layout":"post","photos":[],"_id":"cm8h01v0m000gekeqdrfj7vkq","content":"<h1 id=\"part-1-函数定义-create-data\"><a href=\"#part-1-函数定义-create-data\" class=\"headerlink\" title=\"part.1 函数定义 create_data()\"></a>part.1 函数定义 create_data()</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">create_data</span>(<span class=\"params\">w, b, data_num</span>):</span><br><span class=\"line\">    x = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (data_num, <span class=\"built_in\">len</span>(w)))  </span><br><span class=\"line\">    y = torch.matmul(x, w) + b  </span><br><span class=\"line\">  </span><br><span class=\"line\">    noise = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, y.shape)  </span><br><span class=\"line\">    y += noise  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> x, y</span><br></pre></td></tr></table></figure>\n\n<p>定义了一个create_data函数，用于生成数据。</p>\n<p>torch.normal: <a href=\"https://pytorch.org/docs/stable/generated/torch.normal.html\">docs</a></p>\n<p>torch.matmul: Matrix product of two tensors.</p>\n<p>x : 以0为均值，1为标准差，【(data_num)*(w长度)】的tensor数据<br>y : x*w + b 并在此基础上附加了noise扰动</p>\n<p><img src=\"/2025/03/20/%E6%9D%8E%E5%93%A5%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Pastedimage20250319232552.png\"></p>\n<h1 id=\"part-2-变量赋值\"><a href=\"#part-2-变量赋值\" class=\"headerlink\" title=\"part.2 变量赋值\"></a>part.2 变量赋值</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num = <span class=\"number\">500</span>  </span><br><span class=\"line\">  </span><br><span class=\"line\">true_w = torch.tensor([<span class=\"number\">10.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">5.0</span>, <span class=\"number\">2.0</span>])  </span><br><span class=\"line\">true_b = torch.tensor(<span class=\"number\">1.1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">X, Y = create_data(true_w, true_b, num)</span><br></pre></td></tr></table></figure>\n\n<p>定义了本实验的真实值和实验规模，并调用create_data函数生成了实验数据。<br>true_w: 4*1 true_b: 1<br>X: 500*4 Y：500*1<br><img src=\"/2025/03/20/%E6%9D%8E%E5%93%A5%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Pastedimage20250319234616.png\"></p>\n<h1 id=\"part-3-main函数\"><a href=\"#part-3-main函数\" class=\"headerlink\" title=\"part.3 main函数\"></a>part.3 main函数</h1><p>跳过函数定义看代码主体部分：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">lr = <span class=\"number\">0.05</span>  </span><br><span class=\"line\">  </span><br><span class=\"line\">w_0 = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, true_w.shape, requires_grad=<span class=\"literal\">True</span>)  </span><br><span class=\"line\">b_0 = torch.tensor(<span class=\"number\">0.01</span>, requires_grad=<span class=\"literal\">True</span>)   </span><br><span class=\"line\">  </span><br><span class=\"line\">epochs = <span class=\"number\">50</span></span><br></pre></td></tr></table></figure>\n\n<p>lr: 超参数，学习率<br>w_0, b_0：初始值<br>\trequires_grad &#x3D; True：A tensor can be created with requires_grad&#x3D;True so that torch.autograd records operations on them for automatic differentiation.<br>\tEach tensor has an associated torch.Storage, which holds its data.<br>epochs: 定义了执行梯度下降算法的轮数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):  </span><br><span class=\"line\">    data_loss = <span class=\"number\">0</span>  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> batch_x, batch_y <span class=\"keyword\">in</span> data_provider(X, Y, batchsize):  </span><br><span class=\"line\">        pred_y = fun(batch_x, w_0, b_0)  </span><br><span class=\"line\">        loss = maeLoss(pred_y, batch_y)  </span><br><span class=\"line\">        loss.backward()  </span><br><span class=\"line\">        sgd([w_0, b_0], lr)  </span><br><span class=\"line\">        data_loss += loss  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;epoch %03d: loss: %.6f&quot;</span>%(epoch, data_loss))  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;真实的函数值是&quot;</span>, true_w, true_b)  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;深度学习得到的函数值是&quot;</span>, w_0, b_0)</span><br></pre></td></tr></table></figure>\n<p>from 66 to 78<br>data_loss变量：统计每一轮深度学习的效果<br>torch.backward():<a href=\"/2025/03/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0python%E5%BA%93/\" title=\"深度学习python库\">深度学习python库</a><br>当调用 <code>loss.backward()</code> 时：</p>\n<ul>\n<li>系统会计算损失值对 <code>w_0</code> 的 <strong>每个元素</strong> 的偏导数</li>\n<li>最终 <code>w_0.grad</code> 也会是一个形状为 <code>(4,)</code> 的张量</li>\n</ul>\n<h1 id=\"part-4-函数定义\"><a href=\"#part-4-函数定义\" class=\"headerlink\" title=\"part.4 函数定义\"></a>part.4 函数定义</h1><h2 id=\"data-provider-函数\"><a href=\"#data-provider-函数\" class=\"headerlink\" title=\"data_provider()函数\"></a>data_provider()函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">data_provider</span>(<span class=\"params\">data, label, batchsize</span>):  </span><br><span class=\"line\">    length = <span class=\"built_in\">len</span>(label)  </span><br><span class=\"line\">    indices = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(length))  </span><br><span class=\"line\">    random.shuffle(indices)  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> each <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, length, batchsize):  </span><br><span class=\"line\">        get_indices = indices[each: each + batchsize]  </span><br><span class=\"line\">        get_data = data[get_indices]  </span><br><span class=\"line\">        get_label = label[get_indices]  </span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"keyword\">yield</span> get_data, get_label  </span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\">batchsize = <span class=\"number\">16</span></span><br></pre></td></tr></table></figure>\n<p>from 26 to 36<br>以下是 <code>data_provider()</code> 函数的逐行代码解读：</p>\n<hr>\n<h3 id=\"函数定义\"><a href=\"#函数定义\" class=\"headerlink\" title=\"函数定义\"></a><strong>函数定义</strong></h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">data_provider</span>(<span class=\"params\">data, label, batchsize</span>):</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>输入参数</strong>：<ul>\n<li><code>data</code>: 特征数据张量（形状通常为 <code>[样本数, 特征维度]</code>）</li>\n<li><code>label</code>: 标签数据张量（形状为 <code>[样本数]</code>）</li>\n<li><code>batchsize</code>: 每个批次的样本数量</li>\n</ul>\n</li>\n<li><strong>功能</strong>：生成随机小批量（mini-batch）数据</li>\n</ul>\n<hr>\n<h3 id=\"步骤分解\"><a href=\"#步骤分解\" class=\"headerlink\" title=\"步骤分解\"></a><strong>步骤分解</strong></h3><h4 id=\"1-获取数据集长度\"><a href=\"#1-获取数据集长度\" class=\"headerlink\" title=\"1. 获取数据集长度\"></a><strong>1. 获取数据集长度</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">length = <span class=\"built_in\">len</span>(label)  <span class=\"comment\"># 获取总样本数（假设数据与标签一一对应）</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>关键作用：确定需要处理的总样本数量</li>\n<li>潜在风险：如果 <code>data</code> 和 <code>label</code> 长度不一致会引发错误，但代码未做检查</li>\n</ul>\n<hr>\n<h4 id=\"2-创建索引列表\"><a href=\"#2-创建索引列表\" class=\"headerlink\" title=\"2. 创建索引列表\"></a><strong>2. 创建索引列表</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">indices = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(length))  <span class=\"comment\"># 生成顺序索引 [0, 1, 2, ..., length-1]</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>示例：当 <code>length=500</code> 时，生成 <code>[0,1,2,...,499]</code></li>\n<li>目的：为后续随机采样做准备</li>\n</ul>\n<hr>\n<h4 id=\"3-随机打乱索引\"><a href=\"#3-随机打乱索引\" class=\"headerlink\" title=\"3. 随机打乱索引\"></a><strong>3. 随机打乱索引</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random.shuffle(indices)  <span class=\"comment\"># 原地打乱索引顺序</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>重要性：<ul>\n<li>破坏数据原始顺序，防止模型学习到顺序特征</li>\n<li>每个epoch会生成不同的批次组合</li>\n</ul>\n</li>\n<li>示例：可能变为 <code>[253, 12, 487, ..., 76]</code></li>\n</ul>\n<hr>\n<h4 id=\"4-批次循环生成\"><a href=\"#4-批次循环生成\" class=\"headerlink\" title=\"4. 批次循环生成\"></a><strong>4. 批次循环生成</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> each <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, length, batchsize):</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>循环机制</strong>：<ul>\n<li><code>each</code> 从0开始，以 <code>batchsize</code> 为步长递增</li>\n<li>最后一个批次可能小于 <code>batchsize</code>（例如总样本500，batchsize&#x3D;16时，最后一批次有4个样本）</li>\n</ul>\n</li>\n<li><strong>可视化</strong>：<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch1: 0-15</span><br><span class=\"line\">batch2: 16-31</span><br><span class=\"line\">...</span><br><span class=\"line\">batch31: 496-500</span><br></pre></td></tr></table></figure></li>\n</ul>\n<hr>\n<h4 id=\"5-获取当前批次索引\"><a href=\"#5-获取当前批次索引\" class=\"headerlink\" title=\"5. 获取当前批次索引\"></a><strong>5. 获取当前批次索引</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">get_indices = indices[each: each + batchsize]  <span class=\"comment\"># 切片获取当前批次索引</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>示例：当 <code>each=32</code>, <code>batchsize=16</code> 时，获取索引 <code>indices[32:48]</code></li>\n<li>注意：对列表进行切片时，超出范围不会报错（自动取到列表末尾）</li>\n</ul>\n<hr>\n<h4 id=\"6-提取批次数据\"><a href=\"#6-提取批次数据\" class=\"headerlink\" title=\"6. 提取批次数据\"></a><strong>6. 提取批次数据</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">get_data = data[get_indices]    <span class=\"comment\"># 按索引提取特征数据</span></span><br><span class=\"line\">get_label = label[get_indices]  <span class=\"comment\"># 按索引提取对应标签</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>张量索引特性</strong>：<ul>\n<li>支持通过索引列表进行高级索引（advanced indexing）</li>\n<li>要求 <code>data</code> 和 <code>label</code> 的第一个维度必须与 <code>length</code> 一致</li>\n</ul>\n</li>\n<li><strong>输出形状</strong>：<ul>\n<li><code>get_data</code>: <code>[当前批次大小, 特征维度]</code></li>\n<li><code>get_label</code>: <code>[当前批次大小]</code></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h4 id=\"7-生成数据批次\"><a href=\"#7-生成数据批次\" class=\"headerlink\" title=\"7. 生成数据批次\"></a><strong>7. 生成数据批次</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">yield</span> get_data, get_label  <span class=\"comment\"># 返回生成器对象</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>生成器优势</strong>：<ul>\n<li>惰性加载：不会一次性将所有批次加载到内存</li>\n<li>内存效率：适合处理大型数据集</li>\n</ul>\n</li>\n<li><strong>使用场景</strong>：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 在训练循环中使用</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> batch_data, batch_label <span class=\"keyword\">in</span> data_provider(X, Y, <span class=\"number\">16</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 执行训练步骤</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n<hr>\n<h3 id=\"关键特性总结\"><a href=\"#关键特性总结\" class=\"headerlink\" title=\"关键特性总结\"></a><strong>关键特性总结</strong></h3><table>\n<thead>\n<tr>\n<th>特性</th>\n<th>说明</th>\n<th>重要性</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>随机打乱</td>\n<td>每个epoch重新洗牌数据顺序</td>\n<td>防止模型记住样本顺序</td>\n</tr>\n<tr>\n<td>动态批次生成</td>\n<td>使用生成器逐批产生数据</td>\n<td>节省内存，支持大数据集</td>\n</tr>\n<tr>\n<td>不完整批次处理</td>\n<td>自动处理末尾不完整批次</td>\n<td>保证数据利用率100%</td>\n</tr>\n<tr>\n<td>通用索引机制</td>\n<td>适用于任何支持高级索引的数据结构</td>\n<td>兼容NumPy&#x2F;PyTorch等张量</td>\n</tr>\n</tbody></table>\n<hr>\n<h3 id=\"执行流程示意图\"><a href=\"#执行流程示意图\" class=\"headerlink\" title=\"执行流程示意图\"></a><strong>执行流程示意图</strong></h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">原始数据</span><br><span class=\"line\">  │</span><br><span class=\"line\">  ├─ 创建顺序索引 [0,1,2,...]</span><br><span class=\"line\">  │</span><br><span class=\"line\">  └─ 随机洗牌 → [253,12,487,...]</span><br><span class=\"line\">            │</span><br><span class=\"line\">            ├─ 切片[0:16] → batch1</span><br><span class=\"line\">            ├─ 切片[16:32] → batch2</span><br><span class=\"line\">            │</span><br><span class=\"line\">            └─ ... → 直到遍历所有数据</span><br></pre></td></tr></table></figure>\n\n<p>该函数实现了深度学习训练中最基础且重要的 <strong>随机小批量采样</strong> 功能，是确保模型有效训练的关键组件。</p>\n<h2 id=\"fun-函数\"><a href=\"#fun-函数\" class=\"headerlink\" title=\"fun()函数\"></a>fun()函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">fun</span>(<span class=\"params\">x, w, b</span>):  </span><br><span class=\"line\">    pred_y = torch.matmul(x, w) + b  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> pred_y</span><br></pre></td></tr></table></figure>\n<p>根据给定的参数w, b对数据x生成预测值y并返回预测值pred_y</p>\n<h2 id=\"maeLoss-函数\"><a href=\"#maeLoss-函数\" class=\"headerlink\" title=\"maeLoss()函数\"></a>maeLoss()函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">maeLoss</span>(<span class=\"params\">pred_y, y</span>):  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">sum</span>(<span class=\"built_in\">abs</span>(pred_y-y))/<span class=\"built_in\">len</span>(y)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2025/03/20/%E6%9D%8E%E5%93%A5%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/06ec95ff3c17534440c17f2c5081b49.png\"></p>\n<h2 id=\"sgd-函数\"><a href=\"#sgd-函数\" class=\"headerlink\" title=\"sgd()函数\"></a>sgd()函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sgd</span>(<span class=\"params\">paras, lr</span>):  </span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> para <span class=\"keyword\">in</span> paras:  </span><br><span class=\"line\">            para -= para.grad * lr  </span><br><span class=\"line\">            para.grad.zero_()</span><br></pre></td></tr></table></figure>\n<p>以下是 <code>sgd</code> 函数的详细执行过程分析：</p>\n<hr>\n<h3 id=\"sgd-函数执行步骤详解\"><a href=\"#sgd-函数执行步骤详解\" class=\"headerlink\" title=\"sgd 函数执行步骤详解\"></a><strong><code>sgd</code> 函数执行步骤详解</strong></h3><h4 id=\"1-进入无梯度计算模式\"><a href=\"#1-进入无梯度计算模式\" class=\"headerlink\" title=\"1. 进入无梯度计算模式\"></a><strong>1. 进入无梯度计算模式</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>作用</strong>：禁用梯度跟踪，确保参数更新操作不会记录到计算图中。</li>\n<li><strong>必要性</strong>：参数更新是纯粹的数值操作，不需要梯度信息。禁用梯度跟踪可以：<ul>\n<li>避免不必要的内存占用（计算图不会被扩展）</li>\n<li>防止参数更新操作被错误地加入反向传播流程</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h4 id=\"2-遍历所有参数\"><a href=\"#2-遍历所有参数\" class=\"headerlink\" title=\"2. 遍历所有参数\"></a><strong>2. 遍历所有参数</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> para <span class=\"keyword\">in</span> paras:  <span class=\"comment\"># paras = [w_0, b_0]</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>参数类型</strong>：<code>para</code> 是 <code>requires_grad=True</code> 的叶子张量（如 <code>w_0</code> 和 <code>b_0</code>）</li>\n<li><strong>关键属性</strong>：每个 <code>para</code> 的梯度存储在 <code>para.grad</code> 中，由之前的 <code>loss.backward()</code> 计算得到</li>\n</ul>\n<hr>\n<h4 id=\"3-参数值更新\"><a href=\"#3-参数值更新\" class=\"headerlink\" title=\"3. 参数值更新\"></a><strong>3. 参数值更新</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">para -= para.grad * lr  <span class=\"comment\"># 等价于 para = para - lr * para.grad</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>数学意义</strong>：执行梯度下降更新<br>$$ \\theta_{\\text{new}} &#x3D; \\theta_{\\text{old}} - \\eta \\cdot \\nabla_{\\theta}\\mathcal{L} $$</li>\n<li><strong>实现细节</strong>：<ul>\n<li>原地修改张量的值 (<code>para</code> 是直接操作对象)</li>\n<li>由于在 <code>no_grad()</code> 上下文中，此操作不会影响后续反向传播的计算图</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h4 id=\"4-梯度清零\"><a href=\"#4-梯度清零\" class=\"headerlink\" title=\"4. 梯度清零\"></a><strong>4. 梯度清零</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">para.grad.zero_()</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>必要性</strong>：<ul>\n<li>PyTorch 默认会累积梯度（梯度 +&#x3D; 新梯度）</li>\n<li>必须显式清零，否则下一个 batch 的梯度会与当前梯度错误叠加</li>\n</ul>\n</li>\n<li><strong>方法</strong>：调用 <code>zero_()</code> 原地清零梯度张量</li>\n</ul>\n<hr>\n<h3 id=\"执行时序示例\"><a href=\"#执行时序示例\" class=\"headerlink\" title=\"执行时序示例\"></a><strong>执行时序示例</strong></h3><p>假设当前 batch 的梯度已计算完成（<code>loss.backward()</code> 后）：</p>\n<table>\n<thead>\n<tr>\n<th>步骤</th>\n<th>参数值 <code>w_0</code></th>\n<th>梯度 <code>w_0.grad</code></th>\n<th>学习率 <code>lr</code></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>初始</td>\n<td>0.5</td>\n<td>2.0</td>\n<td>0.05</td>\n</tr>\n<tr>\n<td>更新</td>\n<td>0.5 - 0.05*2.0 &#x3D; 0.4</td>\n<td>2.0 → 0.0 (清零后)</td>\n<td>-</td>\n</tr>\n</tbody></table>\n<hr>\n<h3 id=\"与标准实现的差异\"><a href=\"#与标准实现的差异\" class=\"headerlink\" title=\"与标准实现的差异\"></a><strong>与标准实现的差异</strong></h3><ol>\n<li><p><strong>手动更新 vs 优化器</strong>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 标准 PyTorch 方式（等效实现）</span></span><br><span class=\"line\">optimizer = torch.optim.SGD([w_0, b_0], lr=lr)</span><br><span class=\"line\">optimizer.step()</span><br><span class=\"line\">optimizer.zero_grad()</span><br></pre></td></tr></table></figure>\n<ul>\n<li>用户代码手动实现了优化器的核心逻辑</li>\n</ul>\n</li>\n<li><p><strong>梯度清零时机</strong>：</p>\n<ul>\n<li>用户代码在每个 <strong>batch</strong> 更新后立即清零梯度（正确）</li>\n<li>错误做法：在 epoch 结束后才清零（会导致梯度跨 batch 累积）</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"潜在问题与改进\"><a href=\"#潜在问题与改进\" class=\"headerlink\" title=\"潜在问题与改进\"></a><strong>潜在问题与改进</strong></h3><ol>\n<li><p><strong>梯度爆炸风险</strong>：</p>\n<ul>\n<li>如果学习率 (<code>lr</code>) 过大，可能导致参数更新幅度过大</li>\n<li>改进方案：添加梯度裁剪 (<code>torch.nn.utils.clip_grad_norm_</code>)</li>\n</ul>\n</li>\n<li><p><strong>更复杂的优化器</strong>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 添加动量（需修改 sgd 函数）</span></span><br><span class=\"line\">velocity = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sgd_momentum</span>(<span class=\"params\">para, lr, momentum=<span class=\"number\">0.9</span></span>):</span><br><span class=\"line\">    velocity = momentum * velocity - lr * para.grad</span><br><span class=\"line\">    para += velocity</span><br></pre></td></tr></table></figure></li>\n</ol>\n<hr>\n<h3 id=\"关键总结\"><a href=\"#关键总结\" class=\"headerlink\" title=\"关键总结\"></a><strong>关键总结</strong></h3><table>\n<thead>\n<tr>\n<th>操作</th>\n<th>作用</th>\n<th>必要性等级</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>torch.no_grad()</code></td>\n<td>防止参数更新污染计算图</td>\n<td>必要</td>\n</tr>\n<tr>\n<td><code>para -= grad*lr</code></td>\n<td>执行梯度下降参数更新</td>\n<td>核心操作</td>\n</tr>\n<tr>\n<td><code>grad.zero_()</code></td>\n<td>防止梯度跨 batch 累积</td>\n<td>必要</td>\n</tr>\n</tbody></table>\n<p>通过这种手动实现的 SGD，开发者可以更直观地理解优化器底层的工作原理，但在实际项目中建议使用 PyTorch 内置优化器以获得更好的性能和稳定性。</p>\n<h3 id=\"图示\"><a href=\"#图示\" class=\"headerlink\" title=\"图示\"></a>图示</h3><p><img src=\"/2025/03/20/%E6%9D%8E%E5%93%A5%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/5a6c5073a5cd2f4a2e6f7ca2e39dc5f.png\"><br><img src=\"/2025/03/20/%E6%9D%8E%E5%93%A5%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/0b0fcf44d3b47e8a5e9e12faccd42bc.png\"></p>\n<h1 id=\"深度学习的训练过程\"><a href=\"#深度学习的训练过程\" class=\"headerlink\" title=\"深度学习的训练过程\"></a>深度学习的训练过程</h1><p><img src=\"/2025/03/20/%E6%9D%8E%E5%93%A5%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/607dc97725be3eecbb191ed4887eadb.png\"></p>\n","excerpt":"","more":"<h1 id=\"part-1-函数定义-create-data\"><a href=\"#part-1-函数定义-create-data\" class=\"headerlink\" title=\"part.1 函数定义 create_data()\"></a>part.1 函数定义 create_data()</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">create_data</span>(<span class=\"params\">w, b, data_num</span>):</span><br><span class=\"line\">    x = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">1</span>, (data_num, <span class=\"built_in\">len</span>(w)))  </span><br><span class=\"line\">    y = torch.matmul(x, w) + b  </span><br><span class=\"line\">  </span><br><span class=\"line\">    noise = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, y.shape)  </span><br><span class=\"line\">    y += noise  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> x, y</span><br></pre></td></tr></table></figure>\n\n<p>定义了一个create_data函数，用于生成数据。</p>\n<p>torch.normal: <a href=\"https://pytorch.org/docs/stable/generated/torch.normal.html\">docs</a></p>\n<p>torch.matmul: Matrix product of two tensors.</p>\n<p>x : 以0为均值，1为标准差，【(data_num)*(w长度)】的tensor数据<br>y : x*w + b 并在此基础上附加了noise扰动</p>\n<p><img src=\"/2025/03/20/%E6%9D%8E%E5%93%A5%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Pastedimage20250319232552.png\"></p>\n<h1 id=\"part-2-变量赋值\"><a href=\"#part-2-变量赋值\" class=\"headerlink\" title=\"part.2 变量赋值\"></a>part.2 变量赋值</h1><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">num = <span class=\"number\">500</span>  </span><br><span class=\"line\">  </span><br><span class=\"line\">true_w = torch.tensor([<span class=\"number\">10.0</span>, <span class=\"number\">7.0</span>, <span class=\"number\">5.0</span>, <span class=\"number\">2.0</span>])  </span><br><span class=\"line\">true_b = torch.tensor(<span class=\"number\">1.1</span>)</span><br><span class=\"line\"></span><br><span class=\"line\">X, Y = create_data(true_w, true_b, num)</span><br></pre></td></tr></table></figure>\n\n<p>定义了本实验的真实值和实验规模，并调用create_data函数生成了实验数据。<br>true_w: 4*1 true_b: 1<br>X: 500*4 Y：500*1<br><img src=\"/2025/03/20/%E6%9D%8E%E5%93%A5%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/Pastedimage20250319234616.png\"></p>\n<h1 id=\"part-3-main函数\"><a href=\"#part-3-main函数\" class=\"headerlink\" title=\"part.3 main函数\"></a>part.3 main函数</h1><p>跳过函数定义看代码主体部分：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">lr = <span class=\"number\">0.05</span>  </span><br><span class=\"line\">  </span><br><span class=\"line\">w_0 = torch.normal(<span class=\"number\">0</span>, <span class=\"number\">0.01</span>, true_w.shape, requires_grad=<span class=\"literal\">True</span>)  </span><br><span class=\"line\">b_0 = torch.tensor(<span class=\"number\">0.01</span>, requires_grad=<span class=\"literal\">True</span>)   </span><br><span class=\"line\">  </span><br><span class=\"line\">epochs = <span class=\"number\">50</span></span><br></pre></td></tr></table></figure>\n\n<p>lr: 超参数，学习率<br>w_0, b_0：初始值<br>\trequires_grad &#x3D; True：A tensor can be created with requires_grad&#x3D;True so that torch.autograd records operations on them for automatic differentiation.<br>\tEach tensor has an associated torch.Storage, which holds its data.<br>epochs: 定义了执行梯度下降算法的轮数</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> epoch <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(epochs):  </span><br><span class=\"line\">    data_loss = <span class=\"number\">0</span>  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> batch_x, batch_y <span class=\"keyword\">in</span> data_provider(X, Y, batchsize):  </span><br><span class=\"line\">        pred_y = fun(batch_x, w_0, b_0)  </span><br><span class=\"line\">        loss = maeLoss(pred_y, batch_y)  </span><br><span class=\"line\">        loss.backward()  </span><br><span class=\"line\">        sgd([w_0, b_0], lr)  </span><br><span class=\"line\">        data_loss += loss  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"built_in\">print</span>(<span class=\"string\">&quot;epoch %03d: loss: %.6f&quot;</span>%(epoch, data_loss))  </span><br><span class=\"line\">  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;真实的函数值是&quot;</span>, true_w, true_b)  </span><br><span class=\"line\"><span class=\"built_in\">print</span>(<span class=\"string\">&quot;深度学习得到的函数值是&quot;</span>, w_0, b_0)</span><br></pre></td></tr></table></figure>\n<p>from 66 to 78<br>data_loss变量：统计每一轮深度学习的效果<br>torch.backward():<a href=\"/2025/03/20/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0python%E5%BA%93/\" title=\"深度学习python库\">深度学习python库</a><br>当调用 <code>loss.backward()</code> 时：</p>\n<ul>\n<li>系统会计算损失值对 <code>w_0</code> 的 <strong>每个元素</strong> 的偏导数</li>\n<li>最终 <code>w_0.grad</code> 也会是一个形状为 <code>(4,)</code> 的张量</li>\n</ul>\n<h1 id=\"part-4-函数定义\"><a href=\"#part-4-函数定义\" class=\"headerlink\" title=\"part.4 函数定义\"></a>part.4 函数定义</h1><h2 id=\"data-provider-函数\"><a href=\"#data-provider-函数\" class=\"headerlink\" title=\"data_provider()函数\"></a>data_provider()函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">data_provider</span>(<span class=\"params\">data, label, batchsize</span>):  </span><br><span class=\"line\">    length = <span class=\"built_in\">len</span>(label)  </span><br><span class=\"line\">    indices = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(length))  </span><br><span class=\"line\">    random.shuffle(indices)  </span><br><span class=\"line\">  </span><br><span class=\"line\">    <span class=\"keyword\">for</span> each <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, length, batchsize):  </span><br><span class=\"line\">        get_indices = indices[each: each + batchsize]  </span><br><span class=\"line\">        get_data = data[get_indices]  </span><br><span class=\"line\">        get_label = label[get_indices]  </span><br><span class=\"line\">  </span><br><span class=\"line\">        <span class=\"keyword\">yield</span> get_data, get_label  </span><br><span class=\"line\">  </span><br><span class=\"line\">  </span><br><span class=\"line\">batchsize = <span class=\"number\">16</span></span><br></pre></td></tr></table></figure>\n<p>from 26 to 36<br>以下是 <code>data_provider()</code> 函数的逐行代码解读：</p>\n<hr>\n<h3 id=\"函数定义\"><a href=\"#函数定义\" class=\"headerlink\" title=\"函数定义\"></a><strong>函数定义</strong></h3><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">data_provider</span>(<span class=\"params\">data, label, batchsize</span>):</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>输入参数</strong>：<ul>\n<li><code>data</code>: 特征数据张量（形状通常为 <code>[样本数, 特征维度]</code>）</li>\n<li><code>label</code>: 标签数据张量（形状为 <code>[样本数]</code>）</li>\n<li><code>batchsize</code>: 每个批次的样本数量</li>\n</ul>\n</li>\n<li><strong>功能</strong>：生成随机小批量（mini-batch）数据</li>\n</ul>\n<hr>\n<h3 id=\"步骤分解\"><a href=\"#步骤分解\" class=\"headerlink\" title=\"步骤分解\"></a><strong>步骤分解</strong></h3><h4 id=\"1-获取数据集长度\"><a href=\"#1-获取数据集长度\" class=\"headerlink\" title=\"1. 获取数据集长度\"></a><strong>1. 获取数据集长度</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">length = <span class=\"built_in\">len</span>(label)  <span class=\"comment\"># 获取总样本数（假设数据与标签一一对应）</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>关键作用：确定需要处理的总样本数量</li>\n<li>潜在风险：如果 <code>data</code> 和 <code>label</code> 长度不一致会引发错误，但代码未做检查</li>\n</ul>\n<hr>\n<h4 id=\"2-创建索引列表\"><a href=\"#2-创建索引列表\" class=\"headerlink\" title=\"2. 创建索引列表\"></a><strong>2. 创建索引列表</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">indices = <span class=\"built_in\">list</span>(<span class=\"built_in\">range</span>(length))  <span class=\"comment\"># 生成顺序索引 [0, 1, 2, ..., length-1]</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>示例：当 <code>length=500</code> 时，生成 <code>[0,1,2,...,499]</code></li>\n<li>目的：为后续随机采样做准备</li>\n</ul>\n<hr>\n<h4 id=\"3-随机打乱索引\"><a href=\"#3-随机打乱索引\" class=\"headerlink\" title=\"3. 随机打乱索引\"></a><strong>3. 随机打乱索引</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">random.shuffle(indices)  <span class=\"comment\"># 原地打乱索引顺序</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>重要性：<ul>\n<li>破坏数据原始顺序，防止模型学习到顺序特征</li>\n<li>每个epoch会生成不同的批次组合</li>\n</ul>\n</li>\n<li>示例：可能变为 <code>[253, 12, 487, ..., 76]</code></li>\n</ul>\n<hr>\n<h4 id=\"4-批次循环生成\"><a href=\"#4-批次循环生成\" class=\"headerlink\" title=\"4. 批次循环生成\"></a><strong>4. 批次循环生成</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> each <span class=\"keyword\">in</span> <span class=\"built_in\">range</span>(<span class=\"number\">0</span>, length, batchsize):</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>循环机制</strong>：<ul>\n<li><code>each</code> 从0开始，以 <code>batchsize</code> 为步长递增</li>\n<li>最后一个批次可能小于 <code>batchsize</code>（例如总样本500，batchsize&#x3D;16时，最后一批次有4个样本）</li>\n</ul>\n</li>\n<li><strong>可视化</strong>：<figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">batch1: 0-15</span><br><span class=\"line\">batch2: 16-31</span><br><span class=\"line\">...</span><br><span class=\"line\">batch31: 496-500</span><br></pre></td></tr></table></figure></li>\n</ul>\n<hr>\n<h4 id=\"5-获取当前批次索引\"><a href=\"#5-获取当前批次索引\" class=\"headerlink\" title=\"5. 获取当前批次索引\"></a><strong>5. 获取当前批次索引</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">get_indices = indices[each: each + batchsize]  <span class=\"comment\"># 切片获取当前批次索引</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li>示例：当 <code>each=32</code>, <code>batchsize=16</code> 时，获取索引 <code>indices[32:48]</code></li>\n<li>注意：对列表进行切片时，超出范围不会报错（自动取到列表末尾）</li>\n</ul>\n<hr>\n<h4 id=\"6-提取批次数据\"><a href=\"#6-提取批次数据\" class=\"headerlink\" title=\"6. 提取批次数据\"></a><strong>6. 提取批次数据</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">get_data = data[get_indices]    <span class=\"comment\"># 按索引提取特征数据</span></span><br><span class=\"line\">get_label = label[get_indices]  <span class=\"comment\"># 按索引提取对应标签</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>张量索引特性</strong>：<ul>\n<li>支持通过索引列表进行高级索引（advanced indexing）</li>\n<li>要求 <code>data</code> 和 <code>label</code> 的第一个维度必须与 <code>length</code> 一致</li>\n</ul>\n</li>\n<li><strong>输出形状</strong>：<ul>\n<li><code>get_data</code>: <code>[当前批次大小, 特征维度]</code></li>\n<li><code>get_label</code>: <code>[当前批次大小]</code></li>\n</ul>\n</li>\n</ul>\n<hr>\n<h4 id=\"7-生成数据批次\"><a href=\"#7-生成数据批次\" class=\"headerlink\" title=\"7. 生成数据批次\"></a><strong>7. 生成数据批次</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">yield</span> get_data, get_label  <span class=\"comment\"># 返回生成器对象</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>生成器优势</strong>：<ul>\n<li>惰性加载：不会一次性将所有批次加载到内存</li>\n<li>内存效率：适合处理大型数据集</li>\n</ul>\n</li>\n<li><strong>使用场景</strong>：<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 在训练循环中使用</span></span><br><span class=\"line\"><span class=\"keyword\">for</span> batch_data, batch_label <span class=\"keyword\">in</span> data_provider(X, Y, <span class=\"number\">16</span>):</span><br><span class=\"line\">    <span class=\"comment\"># 执行训练步骤</span></span><br></pre></td></tr></table></figure></li>\n</ul>\n<hr>\n<h3 id=\"关键特性总结\"><a href=\"#关键特性总结\" class=\"headerlink\" title=\"关键特性总结\"></a><strong>关键特性总结</strong></h3><table>\n<thead>\n<tr>\n<th>特性</th>\n<th>说明</th>\n<th>重要性</th>\n</tr>\n</thead>\n<tbody><tr>\n<td>随机打乱</td>\n<td>每个epoch重新洗牌数据顺序</td>\n<td>防止模型记住样本顺序</td>\n</tr>\n<tr>\n<td>动态批次生成</td>\n<td>使用生成器逐批产生数据</td>\n<td>节省内存，支持大数据集</td>\n</tr>\n<tr>\n<td>不完整批次处理</td>\n<td>自动处理末尾不完整批次</td>\n<td>保证数据利用率100%</td>\n</tr>\n<tr>\n<td>通用索引机制</td>\n<td>适用于任何支持高级索引的数据结构</td>\n<td>兼容NumPy&#x2F;PyTorch等张量</td>\n</tr>\n</tbody></table>\n<hr>\n<h3 id=\"执行流程示意图\"><a href=\"#执行流程示意图\" class=\"headerlink\" title=\"执行流程示意图\"></a><strong>执行流程示意图</strong></h3><figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">原始数据</span><br><span class=\"line\">  │</span><br><span class=\"line\">  ├─ 创建顺序索引 [0,1,2,...]</span><br><span class=\"line\">  │</span><br><span class=\"line\">  └─ 随机洗牌 → [253,12,487,...]</span><br><span class=\"line\">            │</span><br><span class=\"line\">            ├─ 切片[0:16] → batch1</span><br><span class=\"line\">            ├─ 切片[16:32] → batch2</span><br><span class=\"line\">            │</span><br><span class=\"line\">            └─ ... → 直到遍历所有数据</span><br></pre></td></tr></table></figure>\n\n<p>该函数实现了深度学习训练中最基础且重要的 <strong>随机小批量采样</strong> 功能，是确保模型有效训练的关键组件。</p>\n<h2 id=\"fun-函数\"><a href=\"#fun-函数\" class=\"headerlink\" title=\"fun()函数\"></a>fun()函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">fun</span>(<span class=\"params\">x, w, b</span>):  </span><br><span class=\"line\">    pred_y = torch.matmul(x, w) + b  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> pred_y</span><br></pre></td></tr></table></figure>\n<p>根据给定的参数w, b对数据x生成预测值y并返回预测值pred_y</p>\n<h2 id=\"maeLoss-函数\"><a href=\"#maeLoss-函数\" class=\"headerlink\" title=\"maeLoss()函数\"></a>maeLoss()函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">maeLoss</span>(<span class=\"params\">pred_y, y</span>):  </span><br><span class=\"line\">    <span class=\"keyword\">return</span> torch.<span class=\"built_in\">sum</span>(<span class=\"built_in\">abs</span>(pred_y-y))/<span class=\"built_in\">len</span>(y)</span><br></pre></td></tr></table></figure>\n<p><img src=\"/2025/03/20/%E6%9D%8E%E5%93%A5%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/06ec95ff3c17534440c17f2c5081b49.png\"></p>\n<h2 id=\"sgd-函数\"><a href=\"#sgd-函数\" class=\"headerlink\" title=\"sgd()函数\"></a>sgd()函数</h2><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sgd</span>(<span class=\"params\">paras, lr</span>):  </span><br><span class=\"line\">    <span class=\"keyword\">with</span> torch.no_grad():  </span><br><span class=\"line\">        <span class=\"keyword\">for</span> para <span class=\"keyword\">in</span> paras:  </span><br><span class=\"line\">            para -= para.grad * lr  </span><br><span class=\"line\">            para.grad.zero_()</span><br></pre></td></tr></table></figure>\n<p>以下是 <code>sgd</code> 函数的详细执行过程分析：</p>\n<hr>\n<h3 id=\"sgd-函数执行步骤详解\"><a href=\"#sgd-函数执行步骤详解\" class=\"headerlink\" title=\"sgd 函数执行步骤详解\"></a><strong><code>sgd</code> 函数执行步骤详解</strong></h3><h4 id=\"1-进入无梯度计算模式\"><a href=\"#1-进入无梯度计算模式\" class=\"headerlink\" title=\"1. 进入无梯度计算模式\"></a><strong>1. 进入无梯度计算模式</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">with</span> torch.no_grad():</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>作用</strong>：禁用梯度跟踪，确保参数更新操作不会记录到计算图中。</li>\n<li><strong>必要性</strong>：参数更新是纯粹的数值操作，不需要梯度信息。禁用梯度跟踪可以：<ul>\n<li>避免不必要的内存占用（计算图不会被扩展）</li>\n<li>防止参数更新操作被错误地加入反向传播流程</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h4 id=\"2-遍历所有参数\"><a href=\"#2-遍历所有参数\" class=\"headerlink\" title=\"2. 遍历所有参数\"></a><strong>2. 遍历所有参数</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"keyword\">for</span> para <span class=\"keyword\">in</span> paras:  <span class=\"comment\"># paras = [w_0, b_0]</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>参数类型</strong>：<code>para</code> 是 <code>requires_grad=True</code> 的叶子张量（如 <code>w_0</code> 和 <code>b_0</code>）</li>\n<li><strong>关键属性</strong>：每个 <code>para</code> 的梯度存储在 <code>para.grad</code> 中，由之前的 <code>loss.backward()</code> 计算得到</li>\n</ul>\n<hr>\n<h4 id=\"3-参数值更新\"><a href=\"#3-参数值更新\" class=\"headerlink\" title=\"3. 参数值更新\"></a><strong>3. 参数值更新</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">para -= para.grad * lr  <span class=\"comment\"># 等价于 para = para - lr * para.grad</span></span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>数学意义</strong>：执行梯度下降更新<br>$$ \\theta_{\\text{new}} &#x3D; \\theta_{\\text{old}} - \\eta \\cdot \\nabla_{\\theta}\\mathcal{L} $$</li>\n<li><strong>实现细节</strong>：<ul>\n<li>原地修改张量的值 (<code>para</code> 是直接操作对象)</li>\n<li>由于在 <code>no_grad()</code> 上下文中，此操作不会影响后续反向传播的计算图</li>\n</ul>\n</li>\n</ul>\n<hr>\n<h4 id=\"4-梯度清零\"><a href=\"#4-梯度清零\" class=\"headerlink\" title=\"4. 梯度清零\"></a><strong>4. 梯度清零</strong></h4><figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">para.grad.zero_()</span><br></pre></td></tr></table></figure>\n<ul>\n<li><strong>必要性</strong>：<ul>\n<li>PyTorch 默认会累积梯度（梯度 +&#x3D; 新梯度）</li>\n<li>必须显式清零，否则下一个 batch 的梯度会与当前梯度错误叠加</li>\n</ul>\n</li>\n<li><strong>方法</strong>：调用 <code>zero_()</code> 原地清零梯度张量</li>\n</ul>\n<hr>\n<h3 id=\"执行时序示例\"><a href=\"#执行时序示例\" class=\"headerlink\" title=\"执行时序示例\"></a><strong>执行时序示例</strong></h3><p>假设当前 batch 的梯度已计算完成（<code>loss.backward()</code> 后）：</p>\n<table>\n<thead>\n<tr>\n<th>步骤</th>\n<th>参数值 <code>w_0</code></th>\n<th>梯度 <code>w_0.grad</code></th>\n<th>学习率 <code>lr</code></th>\n</tr>\n</thead>\n<tbody><tr>\n<td>初始</td>\n<td>0.5</td>\n<td>2.0</td>\n<td>0.05</td>\n</tr>\n<tr>\n<td>更新</td>\n<td>0.5 - 0.05*2.0 &#x3D; 0.4</td>\n<td>2.0 → 0.0 (清零后)</td>\n<td>-</td>\n</tr>\n</tbody></table>\n<hr>\n<h3 id=\"与标准实现的差异\"><a href=\"#与标准实现的差异\" class=\"headerlink\" title=\"与标准实现的差异\"></a><strong>与标准实现的差异</strong></h3><ol>\n<li><p><strong>手动更新 vs 优化器</strong>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 标准 PyTorch 方式（等效实现）</span></span><br><span class=\"line\">optimizer = torch.optim.SGD([w_0, b_0], lr=lr)</span><br><span class=\"line\">optimizer.step()</span><br><span class=\"line\">optimizer.zero_grad()</span><br></pre></td></tr></table></figure>\n<ul>\n<li>用户代码手动实现了优化器的核心逻辑</li>\n</ul>\n</li>\n<li><p><strong>梯度清零时机</strong>：</p>\n<ul>\n<li>用户代码在每个 <strong>batch</strong> 更新后立即清零梯度（正确）</li>\n<li>错误做法：在 epoch 结束后才清零（会导致梯度跨 batch 累积）</li>\n</ul>\n</li>\n</ol>\n<hr>\n<h3 id=\"潜在问题与改进\"><a href=\"#潜在问题与改进\" class=\"headerlink\" title=\"潜在问题与改进\"></a><strong>潜在问题与改进</strong></h3><ol>\n<li><p><strong>梯度爆炸风险</strong>：</p>\n<ul>\n<li>如果学习率 (<code>lr</code>) 过大，可能导致参数更新幅度过大</li>\n<li>改进方案：添加梯度裁剪 (<code>torch.nn.utils.clip_grad_norm_</code>)</li>\n</ul>\n</li>\n<li><p><strong>更复杂的优化器</strong>：</p>\n<figure class=\"highlight python\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br></pre></td><td class=\"code\"><pre><span class=\"line\"><span class=\"comment\"># 添加动量（需修改 sgd 函数）</span></span><br><span class=\"line\">velocity = <span class=\"number\">0</span></span><br><span class=\"line\"><span class=\"keyword\">def</span> <span class=\"title function_\">sgd_momentum</span>(<span class=\"params\">para, lr, momentum=<span class=\"number\">0.9</span></span>):</span><br><span class=\"line\">    velocity = momentum * velocity - lr * para.grad</span><br><span class=\"line\">    para += velocity</span><br></pre></td></tr></table></figure></li>\n</ol>\n<hr>\n<h3 id=\"关键总结\"><a href=\"#关键总结\" class=\"headerlink\" title=\"关键总结\"></a><strong>关键总结</strong></h3><table>\n<thead>\n<tr>\n<th>操作</th>\n<th>作用</th>\n<th>必要性等级</th>\n</tr>\n</thead>\n<tbody><tr>\n<td><code>torch.no_grad()</code></td>\n<td>防止参数更新污染计算图</td>\n<td>必要</td>\n</tr>\n<tr>\n<td><code>para -= grad*lr</code></td>\n<td>执行梯度下降参数更新</td>\n<td>核心操作</td>\n</tr>\n<tr>\n<td><code>grad.zero_()</code></td>\n<td>防止梯度跨 batch 累积</td>\n<td>必要</td>\n</tr>\n</tbody></table>\n<p>通过这种手动实现的 SGD，开发者可以更直观地理解优化器底层的工作原理，但在实际项目中建议使用 PyTorch 内置优化器以获得更好的性能和稳定性。</p>\n<h3 id=\"图示\"><a href=\"#图示\" class=\"headerlink\" title=\"图示\"></a>图示</h3><p><img src=\"/2025/03/20/%E6%9D%8E%E5%93%A5%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/5a6c5073a5cd2f4a2e6f7ca2e39dc5f.png\"><br><img src=\"/2025/03/20/%E6%9D%8E%E5%93%A5%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/0b0fcf44d3b47e8a5e9e12faccd42bc.png\"></p>\n<h1 id=\"深度学习的训练过程\"><a href=\"#深度学习的训练过程\" class=\"headerlink\" title=\"深度学习的训练过程\"></a>深度学习的训练过程</h1><p><img src=\"/2025/03/20/%E6%9D%8E%E5%93%A5%E8%80%83%E7%A0%94%E5%A4%8D%E8%AF%95%E9%A1%B9%E7%9B%AE/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E7%BB%83%E4%B9%A0%E4%BB%A3%E7%A0%81%E8%A7%A3%E8%AF%BB/607dc97725be3eecbb191ed4887eadb.png\"></p>\n"}],"PostAsset":[{"_id":"source/_posts/深度学习/深度学习python库/Pastedimage20250319161350.png","slug":"Pastedimage20250319161350.png","post":"cm8h01uzr0008ekeqdc786d5u","modified":0,"renderable":0},{"_id":"source/_posts/深度学习/深度学习python库/Snipaste_2025-03-20_14-56-59.jpg","slug":"Snipaste_2025-03-20_14-56-59.jpg","post":"cm8h01uzr0008ekeqdc786d5u","modified":0,"renderable":0},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读/06ec95ff3c17534440c17f2c5081b49.png","slug":"06ec95ff3c17534440c17f2c5081b49.png","post":"cm8h01v0m000gekeqdrfj7vkq","modified":0,"renderable":0},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读/0b0fcf44d3b47e8a5e9e12faccd42bc.png","slug":"0b0fcf44d3b47e8a5e9e12faccd42bc.png","post":"cm8h01v0m000gekeqdrfj7vkq","modified":0,"renderable":0},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读/5a6c5073a5cd2f4a2e6f7ca2e39dc5f.png","slug":"5a6c5073a5cd2f4a2e6f7ca2e39dc5f.png","post":"cm8h01v0m000gekeqdrfj7vkq","modified":0,"renderable":0},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读/607dc97725be3eecbb191ed4887eadb.png","slug":"607dc97725be3eecbb191ed4887eadb.png","post":"cm8h01v0m000gekeqdrfj7vkq","modified":0,"renderable":0},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读/Pastedimage20250319232552.png","slug":"Pastedimage20250319232552.png","post":"cm8h01v0m000gekeqdrfj7vkq","modified":0,"renderable":0},{"_id":"source/_posts/李哥考研复试项目/线性回归练习代码解读/Pastedimage20250319234616.png","slug":"Pastedimage20250319234616.png","post":"cm8h01v0m000gekeqdrfj7vkq","modified":0,"renderable":0}],"PostCategory":[{"post_id":"cm8h01uzh0001ekeq1hri3nza","category_id":"cm8h01uzl0005ekeqfwzr2leq","_id":"cm8h01uzl0007ekeqgv6l6r5a"},{"post_id":"cm8h01uzr0008ekeqdc786d5u","category_id":"cm8h01uzs000aekeq7lgdb25u","_id":"cm8h01uzt000dekeqgbx04etd"},{"post_id":"cm8h01uzr0008ekeqdc786d5u","category_id":"cm8h01uzl0005ekeqfwzr2leq","_id":"cm8h01uzt000eekeq7f5a6s5k"},{"post_id":"cm8h01v0m000gekeqdrfj7vkq","category_id":"cm8h01uzs000aekeq7lgdb25u","_id":"cm8h01v0n000lekeqacdjg6m2"},{"post_id":"cm8h01v0m000gekeqdrfj7vkq","category_id":"cm8h01v0n000iekeq1ecrgxd0","_id":"cm8h01v0n000mekeq25t2d32l"}],"PostTag":[{"post_id":"cm8h01uzh0001ekeq1hri3nza","tag_id":"cm8h01uzk0003ekeq0bwyebr8","_id":"cm8h01uzl0006ekeq26lh45iv"},{"post_id":"cm8h01uzr0008ekeqdc786d5u","tag_id":"cm8h01uzs0009ekeq39grf29z","_id":"cm8h01uzs000bekeq5rdh06uf"},{"post_id":"cm8h01uzr0008ekeqdc786d5u","tag_id":"cm8h01uzk0003ekeq0bwyebr8","_id":"cm8h01uzt000cekeq9cz26tko"},{"post_id":"cm8h01v0m000gekeqdrfj7vkq","tag_id":"cm8h01uzs0009ekeq39grf29z","_id":"cm8h01v0n000jekeqb3tm9p04"},{"post_id":"cm8h01v0m000gekeqdrfj7vkq","tag_id":"cm8h01v0m000hekeqb4g76y80","_id":"cm8h01v0n000kekeqezezb03n"}],"Tag":[{"name":"python","_id":"cm8h01uzk0003ekeq0bwyebr8"},{"name":"深度学习","_id":"cm8h01uzs0009ekeq39grf29z"},{"name":"计算机考研复试","_id":"cm8h01v0m000hekeqb4g76y80"}]}}